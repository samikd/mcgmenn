{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eb4febc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Found GPU: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# Append root path \n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../lmmnn\")\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"]=\"true\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "\n",
    "if tf.test.gpu_device_name() != '/device:GPU:0':\n",
    "    print('WARNING: GPU device not found.')\n",
    "else:\n",
    "    print('SUCCESS: Found GPU: {}'.format(tf.test.gpu_device_name()))\n",
    "\n",
    "from model.mixed_effects import *\n",
    "from utils.fe_models import get_model\n",
    "from utils.evaluation import *\n",
    "from utils.utils import *\n",
    "from data.preprocessing import dataset_preprocessing\n",
    "\n",
    "# from vis.utils.utils import apply_modifications\n",
    "# helper function\n",
    "def update_layer_activation(model, activation, index=-1):\n",
    "    model.layers[index].activation = activation\n",
    "    return model\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Reshape, Embedding, Concatenate\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "\n",
    "from sklearn.metrics import accuracy_score as acc\n",
    "from sklearn.metrics import roc_auc_score as auroc\n",
    "from sklearn.metrics import f1_score as f1\n",
    "from sklearn.model_selection import train_test_split\n",
    "from category_encoders import TargetEncoder\n",
    "from tensorflow_addons.metrics import F1Score\n",
    "\n",
    "from scipy import stats\n",
    "import pickle\n",
    "import yaml\n",
    "import time\n",
    "import gc\n",
    "\n",
    "RS = 555"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f4d14a",
   "metadata": {},
   "source": [
    "#### Download and save data from Pargent et al. by running \"data/download_pargent2022_datasets.py before running this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65a5c35f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training procedure for eucalyptus\n",
      "Fold no. 0\n",
      "Load results for dataset eucalyptus, iteration=0\n",
      "Load results for dataset eucalyptus, iteration=0\n",
      "Load results for dataset eucalyptus, iteration=0\n",
      "Load results for dataset eucalyptus, iteration=0\n",
      "Fold no. 1\n",
      "Load results for dataset eucalyptus, iteration=1\n",
      "Load results for dataset eucalyptus, iteration=1\n",
      "Load results for dataset eucalyptus, iteration=1\n",
      "Load results for dataset eucalyptus, iteration=1\n",
      "Fold no. 2\n",
      "Load results for dataset eucalyptus, iteration=2\n",
      "Load results for dataset eucalyptus, iteration=2\n",
      "Load results for dataset eucalyptus, iteration=2\n",
      "Load results for dataset eucalyptus, iteration=2\n",
      "Fold no. 3\n",
      "Load results for dataset eucalyptus, iteration=3\n",
      "Load results for dataset eucalyptus, iteration=3\n",
      "Load results for dataset eucalyptus, iteration=3\n",
      "Load results for dataset eucalyptus, iteration=3\n",
      "Fold no. 4\n",
      "Load results for dataset eucalyptus, iteration=4\n",
      "Load results for dataset eucalyptus, iteration=4\n",
      "Load results for dataset eucalyptus, iteration=4\n",
      "Load results for dataset eucalyptus, iteration=4\n",
      "Start training procedure for Midwest_survey\n",
      "Fold no. 0\n",
      "Load results for dataset Midwest_survey, iteration=0\n",
      "Load results for dataset Midwest_survey, iteration=0\n",
      "Load results for dataset Midwest_survey, iteration=0\n",
      "Load results for dataset Midwest_survey, iteration=0\n",
      "Fold no. 1\n",
      "Load results for dataset Midwest_survey, iteration=1\n",
      "Load results for dataset Midwest_survey, iteration=1\n",
      "Load results for dataset Midwest_survey, iteration=1\n",
      "Load results for dataset Midwest_survey, iteration=1\n",
      "Fold no. 2\n",
      "Load results for dataset Midwest_survey, iteration=2\n",
      "Load results for dataset Midwest_survey, iteration=2\n",
      "Load results for dataset Midwest_survey, iteration=2\n",
      "Load results for dataset Midwest_survey, iteration=2\n",
      "Fold no. 3\n",
      "Load results for dataset Midwest_survey, iteration=3\n",
      "Load results for dataset Midwest_survey, iteration=3\n",
      "Load results for dataset Midwest_survey, iteration=3\n",
      "Load results for dataset Midwest_survey, iteration=3\n",
      "Fold no. 4\n",
      "Load results for dataset Midwest_survey, iteration=4\n",
      "Load results for dataset Midwest_survey, iteration=4\n",
      "Load results for dataset Midwest_survey, iteration=4\n",
      "Load results for dataset Midwest_survey, iteration=4\n",
      "Start training procedure for hpc-job-scheduling\n",
      "Fold no. 0\n",
      "Load results for dataset hpc-job-scheduling, iteration=0\n",
      "Load results for dataset hpc-job-scheduling, iteration=0\n",
      "Load results for dataset hpc-job-scheduling, iteration=0\n",
      "Load results for dataset hpc-job-scheduling, iteration=0\n",
      "Fold no. 1\n",
      "Load results for dataset hpc-job-scheduling, iteration=1\n",
      "Load results for dataset hpc-job-scheduling, iteration=1\n",
      "Load results for dataset hpc-job-scheduling, iteration=1\n",
      "Load results for dataset hpc-job-scheduling, iteration=1\n",
      "Fold no. 2\n",
      "Load results for dataset hpc-job-scheduling, iteration=2\n",
      "Load results for dataset hpc-job-scheduling, iteration=2\n",
      "Load results for dataset hpc-job-scheduling, iteration=2\n",
      "Load results for dataset hpc-job-scheduling, iteration=2\n",
      "Fold no. 3\n",
      "Load results for dataset hpc-job-scheduling, iteration=3\n",
      "Load results for dataset hpc-job-scheduling, iteration=3\n",
      "Load results for dataset hpc-job-scheduling, iteration=3\n",
      "Load results for dataset hpc-job-scheduling, iteration=3\n",
      "Fold no. 4\n",
      "Load results for dataset hpc-job-scheduling, iteration=4\n",
      "Load results for dataset hpc-job-scheduling, iteration=4\n",
      "Load results for dataset hpc-job-scheduling, iteration=4\n",
      "Load results for dataset hpc-job-scheduling, iteration=4\n",
      "Start training procedure for video-game-sales\n",
      "Fold no. 0\n",
      "Load results for dataset video-game-sales, iteration=0\n",
      "Load results for dataset video-game-sales, iteration=0\n",
      "Load results for dataset video-game-sales, iteration=0\n",
      "Load results for dataset video-game-sales, iteration=0\n",
      "Fold no. 1\n",
      "Load results for dataset video-game-sales, iteration=1\n",
      "Load results for dataset video-game-sales, iteration=1\n",
      "Load results for dataset video-game-sales, iteration=1\n",
      "Load results for dataset video-game-sales, iteration=1\n",
      "Fold no. 2\n",
      "Load results for dataset video-game-sales, iteration=2\n",
      "Load results for dataset video-game-sales, iteration=2\n",
      "Load results for dataset video-game-sales, iteration=2\n",
      "Load results for dataset video-game-sales, iteration=2\n",
      "Fold no. 3\n",
      "Load results for dataset video-game-sales, iteration=3\n",
      "Load results for dataset video-game-sales, iteration=3\n",
      "Load results for dataset video-game-sales, iteration=3\n",
      "Load results for dataset video-game-sales, iteration=3\n",
      "Fold no. 4\n",
      "Load results for dataset video-game-sales, iteration=4\n",
      "Load results for dataset video-game-sales, iteration=4\n",
      "Load results for dataset video-game-sales, iteration=4\n",
      "Load results for dataset video-game-sales, iteration=4\n",
      "Start training procedure for okcupid-stem\n",
      "Fold no. 0\n",
      "Load results for dataset okcupid-stem, iteration=0\n",
      "Load results for dataset okcupid-stem, iteration=0\n",
      "Load results for dataset okcupid-stem, iteration=0\n",
      "Load results for dataset okcupid-stem, iteration=0\n",
      "Fold no. 1\n",
      "Load results for dataset okcupid-stem, iteration=1\n",
      "Load results for dataset okcupid-stem, iteration=1\n",
      "Load results for dataset okcupid-stem, iteration=1\n",
      "Load results for dataset okcupid-stem, iteration=1\n",
      "Fold no. 2\n",
      "Load results for dataset okcupid-stem, iteration=2\n",
      "Load results for dataset okcupid-stem, iteration=2\n",
      "Load results for dataset okcupid-stem, iteration=2\n",
      "Load results for dataset okcupid-stem, iteration=2\n",
      "Fold no. 3\n",
      "Load results for dataset okcupid-stem, iteration=3\n",
      "Load results for dataset okcupid-stem, iteration=3\n",
      "Load results for dataset okcupid-stem, iteration=3\n",
      "Load results for dataset okcupid-stem, iteration=3\n",
      "Fold no. 4\n",
      "Load results for dataset okcupid-stem, iteration=4\n",
      "Load results for dataset okcupid-stem, iteration=4\n",
      "Load results for dataset okcupid-stem, iteration=4\n",
      "Load results for dataset okcupid-stem, iteration=4\n",
      "Start training procedure for Diabetes130US\n",
      "Fold no. 0\n",
      "Load results for dataset Diabetes130US, iteration=0\n",
      "Load results for dataset Diabetes130US, iteration=0\n",
      "Load results for dataset Diabetes130US, iteration=0\n",
      "Load results for dataset Diabetes130US, iteration=0\n",
      "Fold no. 1\n",
      "Load results for dataset Diabetes130US, iteration=1\n",
      "Load results for dataset Diabetes130US, iteration=1\n",
      "Load results for dataset Diabetes130US, iteration=1\n",
      "Load results for dataset Diabetes130US, iteration=1\n",
      "Fold no. 2\n",
      "Load results for dataset Diabetes130US, iteration=2\n",
      "Load results for dataset Diabetes130US, iteration=2\n",
      "Load results for dataset Diabetes130US, iteration=2\n",
      "Load results for dataset Diabetes130US, iteration=2\n",
      "Fold no. 3\n",
      "Load results for dataset Diabetes130US, iteration=3\n",
      "Load results for dataset Diabetes130US, iteration=3\n",
      "Load results for dataset Diabetes130US, iteration=3\n",
      "Load results for dataset Diabetes130US, iteration=3\n",
      "Fold no. 4\n",
      "Load results for dataset Diabetes130US, iteration=4\n",
      "Load results for dataset Diabetes130US, iteration=4\n",
      "Load results for dataset Diabetes130US, iteration=4\n",
      "Load results for dataset Diabetes130US, iteration=4\n"
     ]
    }
   ],
   "source": [
    "mode=\"cv\"\n",
    "hct=10\n",
    "test_ratio=None\n",
    "val_ratio=None\n",
    "folds=5\n",
    "results = {}\n",
    "dataset_names = [\"eucalyptus\", \"Midwest_survey\", \"hpc-job-scheduling\", \"video-game-sales\", \"okcupid-stem\", \"Diabetes130US\"]\n",
    "\n",
    "\n",
    "loss_use = lambda: tf.keras.losses.CategoricalCrossentropy\n",
    "target= \"categorical\"\n",
    "batch_size=512\n",
    "epochs = 500\n",
    "early_stopping = 20\n",
    "model_name = \"AutoGluon\"\n",
    "embed_dims_method = \"AutoGluon\"\n",
    "\n",
    "results = {}\n",
    "\n",
    "#######################################\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    print(f\"Start training procedure for {dataset_name}\")\n",
    "    data_path = f\"{mode}_RS{RS}_hct{hct}\"\n",
    "    if mode == \"cv\":\n",
    "        data_path += f\"_{folds}folds\"\n",
    "    elif mode == \"train_test\":\n",
    "        data_path += f\"_split{1-test_ratio*100}-{test_ratio*100}\"\n",
    "    elif mode == \"train_val_test\":\n",
    "        data_path += f\"_split{round(100-(test_ratio+val_ratio)*100)}-{round(test_ratio*100)}-{round(val_ratio*100)}\"\n",
    "\n",
    "    # If no data_dict exists, run preprocessing, else load data_dict\n",
    "    if not os.path.exists(f\"../data/prepared/{dataset_name}/\"+data_path+\"/data_dict.pickle\"):\n",
    "        dataset_preprocessing.process_dataset(dataset_name, target, mode, RS, hct, test_ratio, val_ratio, folds)\n",
    "    with open(f\"../data/prepared/{dataset_name}/{data_path}/data_dict.pickle\", 'rb') as handle:\n",
    "            data_dict = pickle.load(handle)\n",
    "\n",
    "    z_cols = data_dict[\"z_cols\"]\n",
    "    \n",
    "    results[dataset_name] = {}\n",
    "    for fold_num in range(folds):\n",
    "        results[dataset_name][fold_num] = {}\n",
    "\n",
    "        print(f\"Fold no. {fold_num}\")\n",
    "        results[dataset_name][fold_num][\"histories\"] = {}\n",
    "        results[dataset_name][fold_num][\"predictions\"] = {}\n",
    "        results[dataset_name][fold_num][\"times\"] = {}\n",
    "        results[dataset_name][fold_num][\"other_info\"] = {}\n",
    "        for lambda_ in [0.,1.,5.,10.]:\n",
    "            save_path = f\"../results/{dataset_name}/{data_path}/fold_{fold_num}/lambda__\"+str(int(lambda_))\n",
    "            if not os.path.exists(save_path):\n",
    "                os.makedirs(save_path)\n",
    "\n",
    "        \n",
    "            z_ohe_encoded_train = data_dict[f\"z_ohe_encoded_train_{fold_num}\"] \n",
    "            z_ohe_encoded_val = data_dict[f\"z_ohe_encoded_val_{fold_num}\"] \n",
    "            z_ohe_encoded_test = data_dict[f\"z_ohe_encoded_test_{fold_num}\"] \n",
    "\n",
    "            z_target_encoded_train = data_dict[f\"z_target_encoded_train_{fold_num}\"] \n",
    "            z_target_encoded_val = data_dict[f\"z_target_encoded_val_{fold_num}\"] \n",
    "            z_target_encoded_test = data_dict[f\"z_target_encoded_test_{fold_num}\"] \n",
    "\n",
    "            target_encoding_time = data_dict[f\"target_encoding_time_{fold_num}\"]\n",
    "            ohe_encoding_time = data_dict[f\"ohe_encoding_time_{fold_num}\"]\n",
    "\n",
    "            x_cols = data_dict[f\"X_train_{fold_num}\"].columns\n",
    "            X_train = data_dict[f\"X_train_{fold_num}\"]\n",
    "            Z_train = data_dict[f\"Z_train_{fold_num}\"]\n",
    "            y_train = data_dict[f\"y_train_{fold_num}\"]\n",
    "\n",
    "            X_val = data_dict[f\"X_val_{fold_num}\"]\n",
    "            Z_val = data_dict[f\"Z_val_{fold_num}\"]\n",
    "            y_val = data_dict[f\"y_val_{fold_num}\"]\n",
    "\n",
    "            X_test = data_dict[f\"X_test_{fold_num}\"]\n",
    "            Z_test = data_dict[f\"Z_test_{fold_num}\"]\n",
    "            y_test = data_dict[f\"y_test_{fold_num}\"]\n",
    "\n",
    "            if not os.path.exists(f\"{save_path}/results_RS{RS}_{dataset_name}_iter{fold_num}.pickle\"):\n",
    "\n",
    "                tf.random.set_seed(RS+fold_num)\n",
    "                np.random.seed(RS+fold_num)\n",
    "\n",
    "                qs = np.max([tf.reduce_max(Z_train, axis=0),tf.reduce_max(Z_val, axis=0),tf.reduce_max(Z_test, axis=0)],axis=0)+1\n",
    "\n",
    "                X_train = tf.convert_to_tensor(X_train)\n",
    "                Z_train = tf.convert_to_tensor(Z_train,dtype=tf.int32)\n",
    "                y_train = tf.convert_to_tensor(y_train)\n",
    "\n",
    "                X_val = tf.convert_to_tensor(X_val)\n",
    "                Z_val = tf.convert_to_tensor(Z_val,dtype=tf.int32)\n",
    "                y_val = tf.convert_to_tensor(y_val)\n",
    "\n",
    "                X_test = tf.convert_to_tensor(X_test)\n",
    "                Z_test = tf.convert_to_tensor(Z_test,dtype=tf.int32)\n",
    "                y_test = tf.convert_to_tensor(y_test)\n",
    "\n",
    "                if target == \"categorical\":\n",
    "                    n_classes = np.unique(y_train).shape[0]\n",
    "                elif target==\"binary\":\n",
    "                    n_classes = 1\n",
    "\n",
    "                y_train = tf.one_hot(tf.cast(y_train,tf.int32),n_classes)\n",
    "                y_val = tf.one_hot(tf.cast(y_val,tf.int32),n_classes)\n",
    "                y_test = tf.one_hot(tf.cast(y_test,tf.int32),n_classes)\n",
    "\n",
    "                ##### GMENN #####\n",
    "                d = X_train.shape[1] # columns\n",
    "                n = X_train.shape[0] # rows\n",
    "                num_outputs = n_classes\n",
    "                perc_numeric = d/(d+Z_train.shape[1])\n",
    "\n",
    "    #             qs = np.max([tf.reduce_max(Z_train, axis=0),tf.reduce_max(Z_val, axis=0),tf.reduce_max(Z_test, axis=0)],axis=0)+1\n",
    "\n",
    "                set_seed(RS)\n",
    "\n",
    "                fe_model, optimizer = get_model(model_name=model_name, input_size=X_train.shape[1], \n",
    "                                                  output_size=num_outputs, \n",
    "                                                  target=target, \n",
    "                                                  perc_numeric=perc_numeric, RS=RS)\n",
    "                \n",
    "                if dataset_name==\"eucalyptus\":\n",
    "                    optimizer.learning_rate.assign(optimizer.learning_rate*10)\n",
    "\n",
    "                initial_stds = np.ones([len(qs),num_outputs]).astype(float).tolist()\n",
    "\n",
    "                me_model = MixedEffectsNetwork(X_train, Z_train, y_train, fe_model, \n",
    "                                               target=target, qs=qs,\n",
    "                                               initial_stds=initial_stds,\n",
    "                                              fe_loss_weight=lambda_,\n",
    "                                               mode=\"intercepts\",\n",
    "                                               early_stopping_fe=early_stopping,\n",
    "                                              )    \n",
    "\n",
    "                me_model.compile(\n",
    "                    loss_class_me = loss_use()(),\n",
    "                    loss_class_fe = loss_use()(),\n",
    "                #     metric_class_me = tf.keras.metrics.AUC(multi_label=True, name=\"auc_me\"),\n",
    "                #     metric_class_fe = tf.keras.metrics.AUC(multi_label=True, name=\"auc_fe\"),\n",
    "                    optimizer=optimizer\n",
    "                )\n",
    "\n",
    "                mcmc = MCMCSamplingCallback(num_mcmc_samples=1,\n",
    "                                            perc_burnin=0.7,\n",
    "                                            warm_restart=None,\n",
    "                                            num_burnin_steps=1,\n",
    "                                            step_size = 0.1#initial_step_size,\n",
    "                                       )\n",
    "\n",
    "\n",
    "                print_metric = PrintMetrics(X_train, Z_train, y_train, X_val, Z_val, y_val)\n",
    "\n",
    "                start = time.time()\n",
    "                history = me_model.fit([X_train,Z_train], y_train,\n",
    "                             callbacks=[mcmc,\n",
    "                                        print_metric,\n",
    "                                        tf.keras.callbacks.EarlyStopping(monitor=\"me_auc_val\", patience=early_stopping, mode=\"max\")],\n",
    "                             epochs=epochs,\n",
    "                             validation_data=[[X_val,Z_val],y_val],\n",
    "                            batch_size=batch_size)\n",
    "\n",
    "                end = time.time()\n",
    "                fit_time_gmenn = round(end-start,2)\n",
    "\n",
    "                y_train_pred_gmenn, y_train_pred_gmenn_fe = me_model([X_train,Z_train])\n",
    "                y_val_pred_gmenn, y_val_pred_gmenn_fe = me_model([X_val,Z_val])\n",
    "                y_test_pred_gmenn, y_test_pred_gmenn_fe = me_model([X_test,Z_test])    \n",
    "\n",
    "\n",
    "                ###### Prepare NN Training ######\n",
    "\n",
    "\n",
    "\n",
    "                ##### Document Results #####\n",
    "\n",
    "                results[dataset_name][fold_num][\"histories\"][\"GMENN\"+str(int(lambda_))] = history.history\n",
    "\n",
    "                results[dataset_name][fold_num][\"predictions\"][\"GMENN\"+str(int(lambda_))] = [y_train_pred_gmenn, y_val_pred_gmenn, y_test_pred_gmenn]\n",
    "                results[dataset_name][fold_num][\"predictions\"][\"GMENN_FE\"+str(int(lambda_))] = [y_train_pred_gmenn_fe, y_val_pred_gmenn_fe, y_test_pred_gmenn_fe]\n",
    "                \n",
    "                results[dataset_name][fold_num][\"times\"][\"GMENN\"+str(int(lambda_))] = fit_time_gmenn\n",
    "\n",
    "                results[dataset_name][fold_num][\"other_info\"][\"GMENN\"+str(int(lambda_))] = {\n",
    "                        \"_stddev_z\": np.array([i.numpy() for i in me_model.data_model._stddev_z]),\n",
    "                        \"acceptance_rates\": np.array(me_model.acceptance_rates),\n",
    "                        \"random_effects\": me_model.mean_samples,\n",
    "                        \"all_samples\": me_model.all_samples,\n",
    "                        \"stds\": me_model.stds\n",
    "                    }\n",
    "\n",
    "                with open(f\"{save_path}//results_RS{RS}_{dataset_name}_iter{fold_num}.pickle\", 'wb') as handle:\n",
    "                    pickle.dump(results[dataset_name][fold_num], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "                del X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "                gc.collect()\n",
    "            else:\n",
    "                print(f\"Load results for dataset {dataset_name}, iteration={fold_num}\")\n",
    "                with open(f\"{save_path}/results_RS{RS}_{dataset_name}_iter{fold_num}.pickle\", 'rb') as handle:\n",
    "                    res = pickle.load(handle)\n",
    "                results[dataset_name][fold_num][\"histories\"][\"GMENN\"+str(int(lambda_))] = res[\"histories\"][\"GMENN\"+str(int(lambda_))]\n",
    "                results[dataset_name][fold_num][\"predictions\"][\"GMENN\"+str(int(lambda_))] = res[\"predictions\"][\"GMENN\"+str(int(lambda_))]\n",
    "                results[dataset_name][fold_num][\"predictions\"][\"GMENN_FE\"+str(int(lambda_))] = res[\"predictions\"][\"GMENN_FE\"+str(int(lambda_))]\n",
    "                results[dataset_name][fold_num][\"times\"][\"GMENN\"+str(int(lambda_))] = res[\"times\"][\"GMENN\"+str(int(lambda_))]\n",
    "                results[dataset_name][fold_num][\"other_info\"][\"GMENN\"+str(int(lambda_))] = res[\"other_info\"][\"GMENN\"+str(int(lambda_))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219c02f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63d93847",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dcfb167",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-22 12:56:18.927017: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "models = [\"GMENN\"+str(int(lambda_)) for lambda_ in [0,1,5,10]]\n",
    "\n",
    "results_perf = {dataset_name: {num: {model: {}  for model in models} for num in range(folds)} for dataset_name in dataset_names}\n",
    "for dataset_name in dataset_names:\n",
    "    try:\n",
    "        with open(f\"../data/prepared/{dataset_name}/{data_path}/data_dict.pickle\", 'rb') as handle:\n",
    "            data_dict = pickle.load(handle)        \n",
    "    except:\n",
    "        print(f\"dataset {dataset_name} not found\") \n",
    "    for num in range(folds):\n",
    "        y_test = data_dict[f\"y_test_{num}\"]\n",
    "        n_classes = np.unique(y_test).shape[0]\n",
    "        y_test = tf.one_hot(data_dict[f\"y_test_{num}\"],n_classes)\n",
    "        for model in models:\n",
    "            try:\n",
    "                y_pred = results[dataset_name][num][\"predictions\"][model][2]\n",
    "\n",
    "                results_perf[dataset_name][num][model] = get_metrics(y_test,y_pred,target)\n",
    "                results_perf[dataset_name][num][model][\"Time\"] = results[dataset_name][num][\"times\"][model]\n",
    "#                 results_perf[dataset_name][num][model][\"FE_AUC\"] = results[dataset_name][num][\"histories\"][model][\"fe_auc_val\"][-1]\n",
    "\n",
    "            except:\n",
    "                print(f\"Set nan for {dataset_name}, {num}\")\n",
    "                results_perf[dataset_name][num][model] = {\"Accuracy\": np.nan,\n",
    "                                                          \"AUROC\": np.nan,\n",
    "                                                          \"F1\": np.nan,\n",
    "                                                          \"Time\": np.nan,\n",
    "                                                          \"FE_AUC\": np.nan}\n",
    "#                 print(f\"Didnt work for {dataset_name}, {num}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3246b11d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GMENN0</th>\n",
       "      <th>GMENN1</th>\n",
       "      <th>GMENN5</th>\n",
       "      <th>GMENN10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eucalyptus</th>\n",
       "      <td>0.89 (0.024)</td>\n",
       "      <td>0.9 (0.021)</td>\n",
       "      <td>0.9 (0.023)</td>\n",
       "      <td>0.89 (0.025)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Midwest_survey</th>\n",
       "      <td>0.87 (0.021)</td>\n",
       "      <td>0.88 (0.025)</td>\n",
       "      <td>0.88 (0.026)</td>\n",
       "      <td>0.88 (0.026)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hpc-job-scheduling</th>\n",
       "      <td>0.91 (0.008)</td>\n",
       "      <td>0.91 (0.008)</td>\n",
       "      <td>0.9 (0.006)</td>\n",
       "      <td>0.88 (0.041)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video-game-sales</th>\n",
       "      <td>0.78 (0.01)</td>\n",
       "      <td>0.79 (0.01)</td>\n",
       "      <td>0.79 (0.009)</td>\n",
       "      <td>0.79 (0.01)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>okcupid-stem</th>\n",
       "      <td>0.79 (0.003)</td>\n",
       "      <td>0.8 (0.004)</td>\n",
       "      <td>0.81 (0.003)</td>\n",
       "      <td>0.81 (0.003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Diabetes130US</th>\n",
       "      <td>0.63 (0.002)</td>\n",
       "      <td>0.64 (0.003)</td>\n",
       "      <td>0.65 (0.005)</td>\n",
       "      <td>0.65 (0.005)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          GMENN0        GMENN1        GMENN5       GMENN10\n",
       "eucalyptus          0.89 (0.024)   0.9 (0.021)   0.9 (0.023)  0.89 (0.025)\n",
       "Midwest_survey      0.87 (0.021)  0.88 (0.025)  0.88 (0.026)  0.88 (0.026)\n",
       "hpc-job-scheduling  0.91 (0.008)  0.91 (0.008)   0.9 (0.006)  0.88 (0.041)\n",
       "video-game-sales     0.78 (0.01)   0.79 (0.01)  0.79 (0.009)   0.79 (0.01)\n",
       "okcupid-stem        0.79 (0.003)   0.8 (0.004)  0.81 (0.003)  0.81 (0.003)\n",
       "Diabetes130US       0.63 (0.002)  0.64 (0.003)  0.65 (0.005)  0.65 (0.005)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [\"GMENN\"+str(int(lambda_)) for lambda_ in [0,1,5,10]]\n",
    "\n",
    "metric = \"AUROC\"\n",
    "\n",
    "#####\n",
    "dataset_res_dict = {}\n",
    "best_models = {}\n",
    "t_test_results = {}\n",
    "\n",
    "round_mean_at = 2\n",
    "round_std_at = 3\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    dataset_models = list(results_perf[dataset_name][0].keys())\n",
    "    use_df = pd.DataFrame([pd.DataFrame(results_perf[dataset_name][fold_num]).loc[metric,models] for fold_num in results_perf[dataset_name].keys()],index=results_perf[dataset_name].keys())\n",
    "    \n",
    "    df_mean = pd.DataFrame(use_df.mean(axis=0).round(round_mean_at).astype(str) + \" (\" + use_df.std(axis=0).round(round_std_at).astype(str) + \")\").transpose()\n",
    "    model_dict = {i: df_mean[i].values[0] for i in df_mean.columns}\n",
    "    dataset_res_dict[dataset_name] = model_dict\n",
    "    \n",
    "    best_models[dataset_name] = use_df.columns[use_df.mean(axis=0).argmax()]\n",
    "\n",
    "    t_test_res = np.array([stats.ttest_rel(use_df[best_models[dataset_name]].values, use_df[model].values)[1] if model in dataset_models else 0 for model in models]).round(3)\n",
    "    t_test_res[np.isnan(t_test_res)] = 1.\n",
    "    t_test_results[dataset_name] = t_test_res\n",
    "    \n",
    "res_df = pd.DataFrame(dataset_res_dict).transpose()\n",
    "    \n",
    "def negative_bold(val):\n",
    "    i = np.where(val.name==np.array(models))[0][0]\n",
    "    return [\"font-weight: bold\"  if t_test_results[dataset_name][i]>=0.05 else \"\" for dataset_name in val.keys()]\n",
    "    # Case without transpose:\n",
    "#     return [\"font-weight: bold\"  if t_test_results[val.name][i]>=0.05 else \"\" for i in range(len(val))]\n",
    "\n",
    "# res_df.style.apply(negative_bold)\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2c72e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.89, 0.87, 0.91, 0.78, 0.79, 0.63]),\n",
       " array([0.9 , 0.88, 0.91, 0.79, 0.8 , 0.64]),\n",
       " array([0.9 , 0.88, 0.9 , 0.79, 0.81, 0.65]),\n",
       " array([0.89, 0.88, 0.88, 0.79, 0.81, 0.65]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df[\"GMENN0\"].apply(lambda x: float(x.split(\" \")[0])).values,res_df[\"GMENN1\"].apply(lambda x: float(x.split(\" \")[0])).values,res_df[\"GMENN5\"].apply(lambda x: float(x.split(\" \")[0])).values,res_df[\"GMENN10\"].apply(lambda x: float(x.split(\" \")[0])).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f572754c",
   "metadata": {},
   "source": [
    "### Fixed Effects Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdc65ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"GMENN_FE\"+str(int(lambda_)) for lambda_ in [0,1,5,10]]\n",
    "\n",
    "results_perf = {dataset_name: {num: {model: {}  for model in models} for num in range(folds)} for dataset_name in dataset_names}\n",
    "for dataset_name in dataset_names:\n",
    "    try:\n",
    "        with open(f\"../data/prepared/{dataset_name}/{data_path}/data_dict.pickle\", 'rb') as handle:\n",
    "            data_dict = pickle.load(handle)        \n",
    "    except:\n",
    "        print(f\"dataset {dataset_name} not found\") \n",
    "    for num in range(folds):\n",
    "        y_test = data_dict[f\"y_test_{num}\"]\n",
    "        n_classes = np.unique(y_test).shape[0]\n",
    "        y_test = tf.one_hot(data_dict[f\"y_test_{num}\"],n_classes)\n",
    "        for model in models:\n",
    "            try:\n",
    "                y_pred = results[dataset_name][num][\"predictions\"][model][2]\n",
    "\n",
    "                results_perf[dataset_name][num][model] = get_metrics(y_test,y_pred,target)\n",
    "#                 results_perf[dataset_name][num][model][\"Time\"] = results[dataset_name][num][\"times\"][model]\n",
    "#                 results_perf[dataset_name][num][model][\"FE_AUC\"] = results[dataset_name][num][\"histories\"][model][\"fe_auc_val\"][-1]\n",
    "\n",
    "            except:\n",
    "                print(f\"Set nan for {dataset_name}, {num}\")\n",
    "                results_perf[dataset_name][num][model] = {\"Accuracy\": np.nan,\n",
    "                                                          \"AUROC\": np.nan,\n",
    "                                                          \"F1\": np.nan,\n",
    "                                                          \"Time\": np.nan,\n",
    "                                                          \"FE_AUC\": np.nan}\n",
    "#                 print(f\"Didnt work for {dataset_name}, {num}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c04a616d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GMENN_FE0</th>\n",
       "      <th>GMENN_FE1</th>\n",
       "      <th>GMENN_FE5</th>\n",
       "      <th>GMENN_FE10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eucalyptus</th>\n",
       "      <td>0.86 (0.045)</td>\n",
       "      <td>0.9 (0.022)</td>\n",
       "      <td>0.89 (0.023)</td>\n",
       "      <td>0.89 (0.024)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Midwest_survey</th>\n",
       "      <td>0.73 (0.021)</td>\n",
       "      <td>0.75 (0.021)</td>\n",
       "      <td>0.75 (0.021)</td>\n",
       "      <td>0.75 (0.021)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hpc-job-scheduling</th>\n",
       "      <td>0.83 (0.009)</td>\n",
       "      <td>0.85 (0.005)</td>\n",
       "      <td>0.85 (0.004)</td>\n",
       "      <td>0.83 (0.05)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video-game-sales</th>\n",
       "      <td>0.63 (0.025)</td>\n",
       "      <td>0.69 (0.008)</td>\n",
       "      <td>0.7 (0.009)</td>\n",
       "      <td>0.7 (0.01)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>okcupid-stem</th>\n",
       "      <td>0.71 (0.004)</td>\n",
       "      <td>0.72 (0.004)</td>\n",
       "      <td>0.73 (0.006)</td>\n",
       "      <td>0.73 (0.006)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Diabetes130US</th>\n",
       "      <td>0.6 (0.005)</td>\n",
       "      <td>0.61 (0.003)</td>\n",
       "      <td>0.62 (0.009)</td>\n",
       "      <td>0.62 (0.007)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       GMENN_FE0     GMENN_FE1     GMENN_FE5    GMENN_FE10\n",
       "eucalyptus          0.86 (0.045)   0.9 (0.022)  0.89 (0.023)  0.89 (0.024)\n",
       "Midwest_survey      0.73 (0.021)  0.75 (0.021)  0.75 (0.021)  0.75 (0.021)\n",
       "hpc-job-scheduling  0.83 (0.009)  0.85 (0.005)  0.85 (0.004)   0.83 (0.05)\n",
       "video-game-sales    0.63 (0.025)  0.69 (0.008)   0.7 (0.009)    0.7 (0.01)\n",
       "okcupid-stem        0.71 (0.004)  0.72 (0.004)  0.73 (0.006)  0.73 (0.006)\n",
       "Diabetes130US        0.6 (0.005)  0.61 (0.003)  0.62 (0.009)  0.62 (0.007)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [\"GMENN_FE\"+str(int(lambda_)) for lambda_ in [0.,1.,5.,10.]]\n",
    "metric = \"AUROC\"\n",
    "\n",
    "#####\n",
    "dataset_res_dict = {}\n",
    "best_models = {}\n",
    "t_test_results = {}\n",
    "\n",
    "round_mean_at = 2\n",
    "round_std_at = 3\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    dataset_models = list(results_perf[dataset_name][0].keys())\n",
    "    use_df = pd.DataFrame([pd.DataFrame(results_perf[dataset_name][fold_num]).loc[metric,models] for fold_num in results_perf[dataset_name].keys()],index=results_perf[dataset_name].keys())\n",
    "    \n",
    "    df_mean = pd.DataFrame(use_df.mean(axis=0).round(round_mean_at).astype(str) + \" (\" + use_df.std(axis=0).round(round_std_at).astype(str) + \")\").transpose()\n",
    "    model_dict = {i: df_mean[i].values[0] for i in df_mean.columns}\n",
    "    dataset_res_dict[dataset_name] = model_dict\n",
    "    \n",
    "    best_models[dataset_name] = use_df.columns[use_df.mean(axis=0).argmax()]\n",
    "\n",
    "    t_test_res = np.array([stats.ttest_rel(use_df[best_models[dataset_name]].values, use_df[model].values)[1] if model in dataset_models else 0 for model in models]).round(3)\n",
    "    t_test_res[np.isnan(t_test_res)] = 1.\n",
    "    t_test_results[dataset_name] = t_test_res\n",
    "    \n",
    "res_df = pd.DataFrame(dataset_res_dict).transpose()\n",
    "    \n",
    "def negative_bold(val):\n",
    "    i = np.where(val.name==np.array(models))[0][0]\n",
    "    return [\"font-weight: bold\"  if t_test_results[dataset_name][i]>=0.05 else \"\" for dataset_name in val.keys()]\n",
    "    # Case without transpose:\n",
    "#     return [\"font-weight: bold\"  if t_test_results[val.name][i]>=0.05 else \"\" for i in range(len(val))]\n",
    "\n",
    "# res_df.style.apply(negative_bold)\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99a0692d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.86, 0.73, 0.83, 0.63, 0.71, 0.6 ]),\n",
       " array([0.9 , 0.75, 0.85, 0.69, 0.72, 0.61]),\n",
       " array([0.89, 0.75, 0.85, 0.7 , 0.73, 0.62]),\n",
       " array([0.89, 0.75, 0.83, 0.7 , 0.73, 0.62]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df[\"GMENN_FE0\"].apply(lambda x: float(x.split(\" \")[0])).values,res_df[\"GMENN_FE1\"].apply(lambda x: float(x.split(\" \")[0])).values,res_df[\"GMENN_FE5\"].apply(lambda x: float(x.split(\" \")[0])).values,res_df[\"GMENN_FE10\"].apply(lambda x: float(x.split(\" \")[0])).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fed8bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb0d4d2b",
   "metadata": {},
   "source": [
    "### Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9c819dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"GMENN\"+str(int(lambda_)) for lambda_ in [0,1,5,10]]\n",
    "\n",
    "results_perf = {dataset_name: {num: {model: {}  for model in models} for num in range(folds)} for dataset_name in dataset_names}\n",
    "for dataset_name in dataset_names:\n",
    "    try:\n",
    "        with open(f\"../data/prepared/{dataset_name}/{data_path}/data_dict.pickle\", 'rb') as handle:\n",
    "            data_dict = pickle.load(handle)        \n",
    "    except:\n",
    "        print(f\"dataset {dataset_name} not found\") \n",
    "    for num in range(folds):\n",
    "        y_test = data_dict[f\"y_test_{num}\"]\n",
    "        n_classes = np.unique(y_test).shape[0]\n",
    "        y_test = tf.one_hot(data_dict[f\"y_test_{num}\"],n_classes)\n",
    "        for model in models:\n",
    "            try:\n",
    "                y_pred = results[dataset_name][num][\"predictions\"][model][2]\n",
    "\n",
    "                results_perf[dataset_name][num][model] = get_metrics(y_test,y_pred,target)\n",
    "                results_perf[dataset_name][num][model][\"Time\"] = results[dataset_name][num][\"times\"][model]\n",
    "#                 results_perf[dataset_name][num][model][\"FE_AUC\"] = results[dataset_name][num][\"histories\"][model][\"fe_auc_val\"][-1]\n",
    "\n",
    "            except:\n",
    "                print(f\"Set nan for {dataset_name}, {num}\")\n",
    "                results_perf[dataset_name][num][model] = {\"Accuracy\": np.nan,\n",
    "                                                          \"AUROC\": np.nan,\n",
    "                                                          \"F1\": np.nan,\n",
    "                                                          \"Time\": np.nan,\n",
    "                                                          \"FE_AUC\": np.nan}\n",
    "#                 print(f\"Didnt work for {dataset_name}, {num}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd49a259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GMENN0</th>\n",
       "      <th>GMENN1</th>\n",
       "      <th>GMENN5</th>\n",
       "      <th>GMENN10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eucalyptus</th>\n",
       "      <td>5.95 (4.682)</td>\n",
       "      <td>3.58 (3.31)</td>\n",
       "      <td>2.35 (1.637)</td>\n",
       "      <td>2.41 (1.512)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Midwest_survey</th>\n",
       "      <td>1.54 (0.559)</td>\n",
       "      <td>1.26 (0.494)</td>\n",
       "      <td>1.41 (0.491)</td>\n",
       "      <td>1.41 (0.491)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hpc-job-scheduling</th>\n",
       "      <td>4.5 (1.473)</td>\n",
       "      <td>3.79 (1.322)</td>\n",
       "      <td>6.69 (4.127)</td>\n",
       "      <td>2.76 (1.76)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video-game-sales</th>\n",
       "      <td>1.36 (0.211)</td>\n",
       "      <td>1.4 (0.341)</td>\n",
       "      <td>1.47 (0.445)</td>\n",
       "      <td>1.55 (0.553)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>okcupid-stem</th>\n",
       "      <td>2.42 (0.303)</td>\n",
       "      <td>2.33 (0.268)</td>\n",
       "      <td>2.95 (0.528)</td>\n",
       "      <td>2.97 (0.811)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Diabetes130US</th>\n",
       "      <td>2.54 (0.268)</td>\n",
       "      <td>2.66 (0.277)</td>\n",
       "      <td>2.5 (0.159)</td>\n",
       "      <td>2.39 (0.161)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          GMENN0        GMENN1        GMENN5       GMENN10\n",
       "eucalyptus          5.95 (4.682)   3.58 (3.31)  2.35 (1.637)  2.41 (1.512)\n",
       "Midwest_survey      1.54 (0.559)  1.26 (0.494)  1.41 (0.491)  1.41 (0.491)\n",
       "hpc-job-scheduling   4.5 (1.473)  3.79 (1.322)  6.69 (4.127)   2.76 (1.76)\n",
       "video-game-sales    1.36 (0.211)   1.4 (0.341)  1.47 (0.445)  1.55 (0.553)\n",
       "okcupid-stem        2.42 (0.303)  2.33 (0.268)  2.95 (0.528)  2.97 (0.811)\n",
       "Diabetes130US       2.54 (0.268)  2.66 (0.277)   2.5 (0.159)  2.39 (0.161)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = \"Time\"\n",
    "\n",
    "#####\n",
    "dataset_res_dict = {}\n",
    "best_models = {}\n",
    "t_test_results = {}\n",
    "\n",
    "round_mean_at = 2\n",
    "round_std_at = 3\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    dataset_models = list(results_perf[dataset_name][0].keys())\n",
    "    use_df = pd.DataFrame([pd.DataFrame(results_perf[dataset_name][fold_num]).loc[metric,models] for fold_num in results_perf[dataset_name].keys()],index=results_perf[dataset_name].keys())/60\n",
    "    \n",
    "    df_mean = pd.DataFrame(use_df.mean(axis=0).round(round_mean_at).astype(str) + \" (\" + use_df.std(axis=0).round(round_std_at).astype(str) + \")\").transpose()\n",
    "    model_dict = {i: df_mean[i].values[0] for i in df_mean.columns}\n",
    "    dataset_res_dict[dataset_name] = model_dict\n",
    "    \n",
    "    best_models[dataset_name] = use_df.columns[use_df.mean(axis=0).argmin()]\n",
    "\n",
    "    t_test_res = np.array([stats.ttest_rel(use_df[best_models[dataset_name]].values, use_df[model].values)[1] if model in dataset_models else 0 for model in models]).round(3)\n",
    "    t_test_res[np.isnan(t_test_res)] = 1.\n",
    "    t_test_results[dataset_name] = t_test_res\n",
    "    \n",
    "res_df = pd.DataFrame(dataset_res_dict).transpose()\n",
    "    \n",
    "def negative_bold(val):\n",
    "    i = np.where(val.name==np.array(models))[0][0]\n",
    "    return [\"font-weight: bold\"  if t_test_results[dataset_name][i]>=0.05 else \"\" for dataset_name in val.keys()]\n",
    "    # Case without transpose:\n",
    "#     return [\"font-weight: bold\"  if t_test_results[val.name][i]>=0.05 else \"\" for i in range(len(val))]\n",
    "\n",
    "# res_df.style.apply(negative_bold)\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d068c049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5.95, 1.54, 4.5 , 1.36, 2.42, 2.54]),\n",
       " array([3.58, 1.26, 3.79, 1.4 , 2.33, 2.66]),\n",
       " array([2.35, 1.41, 6.69, 1.47, 2.95, 2.5 ]),\n",
       " array([2.41, 1.41, 2.76, 1.55, 2.97, 2.39]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df[\"GMENN0\"].apply(lambda x: float(x.split(\" \")[0])).values,res_df[\"GMENN1\"].apply(lambda x: float(x.split(\" \")[0])).values,res_df[\"GMENN5\"].apply(lambda x: float(x.split(\" \")[0])).values,res_df[\"GMENN10\"].apply(lambda x: float(x.split(\" \")[0])).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3f1100c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gmenn",
   "language": "python",
   "name": "gmenn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc07112e7ae1e8e28a0232207620ff002934c05692de8df42430404c766a0a8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
