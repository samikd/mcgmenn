{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eb4febc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Found GPU: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# Append root path \n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../lmmnn\")\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"]=\"true\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "if tf.test.gpu_device_name() != '/device:GPU:0':\n",
    "    print('WARNING: GPU device not found.')\n",
    "else:\n",
    "    print('SUCCESS: Found GPU: {}'.format(tf.test.gpu_device_name()))\n",
    "\n",
    "from model.mixed_effects import *\n",
    "from utils.fe_models import get_model\n",
    "from utils.evaluation import *\n",
    "from utils.utils import *\n",
    "from data.preprocessing import dataset_preprocessing\n",
    "\n",
    "# from vis.utils.utils import apply_modifications\n",
    "# helper function\n",
    "def update_layer_activation(model, activation, index=-1):\n",
    "    model.layers[index].activation = activation\n",
    "    return model\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Reshape, Embedding, Concatenate\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "\n",
    "from sklearn.metrics import accuracy_score as acc\n",
    "from sklearn.metrics import roc_auc_score as auroc\n",
    "from sklearn.metrics import f1_score as f1\n",
    "from sklearn.model_selection import train_test_split\n",
    "from category_encoders import TargetEncoder\n",
    "from tensorflow_addons.metrics import F1Score\n",
    "\n",
    "from scipy import stats\n",
    "import pickle\n",
    "import yaml\n",
    "import time\n",
    "import gc\n",
    "\n",
    "RS = 555"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2591484f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCMCSamplingCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self,\n",
    "                 num_mcmc_samples=1,\n",
    "                 step_size=0.01,\n",
    "                 perc_burnin=0.1,\n",
    "                 num_burnin_steps=0,\n",
    "                 warm_restart=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_mcmc_samples = tf.constant(num_mcmc_samples)\n",
    "        self.perc_burnin = perc_burnin\n",
    "        self.num_burnin_steps = num_burnin_steps\n",
    "        self.warm_restart = warm_restart\n",
    "        self.step_size = tf.Variable(step_size,trainable=False)\n",
    "        self.step_sizes = []\n",
    "\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.mcmc_kernel = tfp.mcmc.HamiltonianMonteCarlo(\n",
    "            target_log_prob_fn=self.model.target_log_prob_fn,\n",
    "            step_size=self.step_size, #0.15\n",
    "            num_leapfrog_steps=3\n",
    "        )\n",
    "\n",
    "        self.get_mcmc_kernel = lambda step_size: tfp.mcmc.HamiltonianMonteCarlo(\n",
    "            target_log_prob_fn=self.model.target_log_prob_fn,\n",
    "            num_leapfrog_steps=3,\n",
    "            step_size=step_size)\n",
    "\n",
    "        # self.mcmc_kernel = tfp.mcmc.DualAveragingStepSizeAdaptation(\n",
    "        #     inner_kernel=tfp.mcmc.NoUTurnSampler(\n",
    "        #         target_log_prob_fn=self.model.target_log_prob_fn,\n",
    "        #         step_size=self.step_size),\n",
    "        #     num_adaptation_steps=500,\n",
    "        #     target_accept_prob=0.651)\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch==0:\n",
    "            self.model.all_samples.extend(([[state[num] for state in self.model.current_state] for num in range(1)]))\n",
    "\n",
    "            self.model.mean_samples = [tf.reduce_mean([sample[q] for sample in self.model.all_samples[round(epoch*(self.perc_burnin)):]], axis=0) for q in\n",
    "                                       range(len(self.model.qs))]\n",
    "        if self.model.fe_pretraining:\n",
    "            if self.model.fe_converged:\n",
    "                self.run_sampling(epoch)\n",
    "            else:\n",
    "                self.model.acceptance_rates.append(-1)\n",
    "        else:\n",
    "            self.run_sampling(epoch)\n",
    "\n",
    "    def run_sampling(self,epoch):\n",
    "        self.model.fX.assign(self.model.fe_model(self.model.X, training=False))\n",
    "\n",
    "        if self.model.embed_x:\n",
    "            self.model.X_embedded.assign(self.model.X_embed_model(self.model.X, training=False))\n",
    "\n",
    "        if self.model.embed_z:\n",
    "            for q_num in range(len(self.model.qs)):\n",
    "                self.model.Z_embedded[q_num].assign(self.model.Z_embed_models[q_num](self.model.Z[:,q_num], training=False))\n",
    "\n",
    "                ## Find initial step size\n",
    "        # if self.model.previous_kernel_results.log_accept_ratio == -np.inf:\n",
    "        # if len(self.model.acceptance_rates)>0 and self.model.acceptance_rates[-1]<0.5:\n",
    "        if len(self.model.acceptance_rates)>0 and self.model.acceptance_rates[-1]<0.0001:\n",
    "            # self.mcmc_kernel.parameters[\"step_size\"] = self.mcmc_kernel.parameters[\"step_size\"]/2\n",
    "            # self.model.previous_kernel_results[\"new_step_size\"] = self.model.previous_kernel_results.step_size/2\n",
    "            # setattr(self.model.previous_kernel_results, \"new_step_size\", self.model.previous_kernel_results.step_size/2)\n",
    "            self.step_size.assign(self.step_size/2)\n",
    "            print(f\"Adapt step size to {float(self.step_size)}\")\n",
    "\n",
    "\n",
    "        if self.warm_restart!=None and epoch>0:\n",
    "            ## Warm restart\n",
    "            # if self.model.previous_kernel_results.log_accept_ratio == -np.inf:\n",
    "                # restart = True\n",
    "            # else:\n",
    "                # restart = False\n",
    "            # else:\n",
    "            restart = ((epoch + 1) % self.warm_restart) == 0 and epoch != 0\n",
    "\n",
    "            if restart:\n",
    "                print(\"\\n Warm restart to unstuck the chain\")\n",
    "                if self.model.embed_z and self.model.embed_x:\n",
    "                    self.model.current_state = self.model.data_model(self.model.fX, self.model.X_embedded, self.model.Z_embedded).sample(1, seed=self.model.RS)[:-1]\n",
    "                elif self.model.embed_z and not self.model.embed_x:\n",
    "                    self.model.current_state = self.model.data_model(self.model.fX, self.model.X, self.model.Z_embedded).sample(1, seed=self.model.RS)[:-1]\n",
    "                elif not self.model.embed_z and self.model.embed_x:\n",
    "                    self.model.current_state = self.model.data_model(self.model.fX, self.model.X_embedded, self.model.Z).sample(1, seed=self.model.RS)[:-1]\n",
    "                else:\n",
    "                    self.model.current_state = self.model.data_model(self.model.fX, self.model.X, self.model.Z).sample(1, seed=self.model.RS)[:-1]\n",
    "\n",
    "        print(\"\\n Start sampling for epoch {} of training\".format(epoch + 1))\n",
    "        start = time.time()\n",
    "        new_state, self.model.previous_kernel_results = self.get_mcmc_samples(self.model.current_state,\n",
    "                                                                              tf.constant(self.num_mcmc_samples),\n",
    "                                                                              None\n",
    "                                                                                               )\n",
    "        # self.model.divide_constants.assign(\n",
    "        #     list(1/np.mean(self.model.data_model._stddev_z,axis=1))+[1.])\n",
    "        # self.model.divide_constants.assign(\n",
    "        #     list((lambda x: 1+(x-x.mean()))(np.array(1+tf.math.softmax(1/len(self.model.qs)+0.5*tf.math.softmax(np.abs([np.mean(i) for i in self.model.previous_kernel_results.grads_target_log_prob]))))))+[1.])\n",
    "        # print(np.round(self.model.divide_constants,2))\n",
    "        try:\n",
    "            log_accept_ratio = self.model.previous_kernel_results.log_accept_ratio\n",
    "        except:\n",
    "            log_accept_ratio = self.model.previous_kernel_results.inner_results.log_accept_ratio\n",
    "        acceptance_rate = tf.math.exp(tf.minimum(log_accept_ratio, 0.))\n",
    "\n",
    "        self.step_sizes.append(float(self.step_size))\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        self.model.current_state = [tf.identity(i) for i in new_state]\n",
    "        # Todo: Append all current states\n",
    "        self.model.acceptance_rates.append(acceptance_rate)\n",
    "        # self.model.all_samples.append(\n",
    "        #     [tf.math.reduce_mean(self.model.current_state[q_num], axis=0) for q_num in range(len(self.model.qs))])\n",
    "        self.model.all_samples.extend(([[state[num] for state in self.model.current_state] for num in range(self.num_mcmc_samples)]))\n",
    "\n",
    "        self.model.mean_samples = [tf.reduce_mean([sample[q] for sample in self.model.all_samples[round(epoch*(self.perc_burnin)):]], axis=0) for q in\n",
    "                                   range(len(self.model.qs))]\n",
    "\n",
    "        self.model.e_step_times.append(round(end - start, 2))\n",
    "\n",
    "    # def on_epoch_end(self, epoch, logs=None):\n",
    "        for q_num in range(len(self.model.qs)):\n",
    "                self.model.data_model.trainable_variables[q_num].assign(\n",
    "                    tf.math.reduce_std(self.model.current_state[q_num][-1],axis=0))\n",
    "\n",
    "        self.model.stds.append([tf.identity(i) for i in self.model.data_model._stddev_z])\n",
    "\n",
    "    @tf.function(reduce_retracing=True)  # autograph=False, jit_compile=True, reduce_retracing=True)\n",
    "    def get_mcmc_samples(self, current_state, num_mcmc_samples=tf.constant(1), previous_kernel_results=None):\n",
    "        samples, _, previous_kernel_results = tfp.mcmc.sample_chain(\n",
    "            kernel=self.get_mcmc_kernel(self.step_size), num_results=num_mcmc_samples,\n",
    "            current_state=[state[-1] for state in current_state],\n",
    "            num_burnin_steps=self.num_burnin_steps,\n",
    "            trace_fn=None, previous_kernel_results=previous_kernel_results,\n",
    "            return_final_kernel_results=True, seed=self.model.RS)\n",
    "        #     current_state=[sample[-1] for sample in samples]\n",
    "\n",
    "        return samples, previous_kernel_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f4d14a",
   "metadata": {},
   "source": [
    "#### Download and save data from Pargent et al. by running \"data/download_pargent2022_datasets.py before running this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65a5c35f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training procedure for eucalyptus\n",
      "Fold no. 0\n",
      "Load results for dataset eucalyptus, iteration=0\n",
      "Fold no. 1\n",
      "Load results for dataset eucalyptus, iteration=1\n",
      "Fold no. 2\n",
      "Load results for dataset eucalyptus, iteration=2\n",
      "Fold no. 3\n",
      "Load results for dataset eucalyptus, iteration=3\n",
      "Fold no. 4\n",
      "Load results for dataset eucalyptus, iteration=4\n",
      "Start training procedure for Midwest_survey\n",
      "Fold no. 0\n",
      "Load results for dataset Midwest_survey, iteration=0\n",
      "Fold no. 1\n",
      "Load results for dataset Midwest_survey, iteration=1\n",
      "Fold no. 2\n",
      "Load results for dataset Midwest_survey, iteration=2\n",
      "Fold no. 3\n",
      "Load results for dataset Midwest_survey, iteration=3\n",
      "Fold no. 4\n",
      "Load results for dataset Midwest_survey, iteration=4\n",
      "Start training procedure for hpc-job-scheduling\n",
      "Fold no. 0\n",
      "Load results for dataset hpc-job-scheduling, iteration=0\n",
      "Fold no. 1\n",
      "Load results for dataset hpc-job-scheduling, iteration=1\n",
      "Fold no. 2\n",
      "Load results for dataset hpc-job-scheduling, iteration=2\n",
      "Fold no. 3\n",
      "Load results for dataset hpc-job-scheduling, iteration=3\n",
      "Fold no. 4\n",
      "Load results for dataset hpc-job-scheduling, iteration=4\n",
      "Start training procedure for video-game-sales\n",
      "Fold no. 0\n",
      "Load results for dataset video-game-sales, iteration=0\n",
      "Fold no. 1\n",
      "Load results for dataset video-game-sales, iteration=1\n",
      "Fold no. 2\n",
      "Load results for dataset video-game-sales, iteration=2\n",
      "Fold no. 3\n",
      "Load results for dataset video-game-sales, iteration=3\n",
      "Fold no. 4\n",
      "Load results for dataset video-game-sales, iteration=4\n",
      "Start training procedure for okcupid-stem\n",
      "Fold no. 0\n",
      "Load results for dataset okcupid-stem, iteration=0\n",
      "Fold no. 1\n",
      "Load results for dataset okcupid-stem, iteration=1\n",
      "Fold no. 2\n",
      "Load results for dataset okcupid-stem, iteration=2\n",
      "Fold no. 3\n",
      "Load results for dataset okcupid-stem, iteration=3\n",
      "Fold no. 4\n",
      "Load results for dataset okcupid-stem, iteration=4\n",
      "Start training procedure for Diabetes130US\n",
      "Fold no. 0\n",
      "Load results for dataset Diabetes130US, iteration=0\n",
      "Fold no. 1\n",
      "Load results for dataset Diabetes130US, iteration=1\n",
      "Fold no. 2\n",
      "Load results for dataset Diabetes130US, iteration=2\n",
      "Fold no. 3\n",
      "Load results for dataset Diabetes130US, iteration=3\n",
      "Fold no. 4\n",
      "Load results for dataset Diabetes130US, iteration=4\n"
     ]
    }
   ],
   "source": [
    "mode=\"cv\"\n",
    "hct=10\n",
    "test_ratio=None\n",
    "val_ratio=None\n",
    "folds=5\n",
    "results = {}\n",
    "dataset_names = [\"eucalyptus\", \"Midwest_survey\", \"hpc-job-scheduling\", \"video-game-sales\", \"okcupid-stem\", \"Diabetes130US\"]\n",
    "\n",
    "\n",
    "loss_use = lambda: tf.keras.losses.CategoricalCrossentropy\n",
    "target= \"categorical\"\n",
    "batch_size=512\n",
    "epochs = 500\n",
    "early_stopping = 20\n",
    "model_name = \"AutoGluon\"\n",
    "embed_dims_method = \"AutoGluon\"\n",
    "\n",
    "results = {}\n",
    "\n",
    "#######################################\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    print(f\"Start training procedure for {dataset_name}\")\n",
    "    data_path = f\"{mode}_RS{RS}_hct{hct}\"\n",
    "    if mode == \"cv\":\n",
    "        data_path += f\"_{folds}folds\"\n",
    "    elif mode == \"train_test\":\n",
    "        data_path += f\"_split{1-test_ratio*100}-{test_ratio*100}\"\n",
    "    elif mode == \"train_val_test\":\n",
    "        data_path += f\"_split{round(100-(test_ratio+val_ratio)*100)}-{round(test_ratio*100)}-{round(val_ratio*100)}\"\n",
    "\n",
    "    # If no data_dict exists, run preprocessing, else load data_dict\n",
    "    if not os.path.exists(f\"../data/prepared/{dataset_name}/\"+data_path+\"/data_dict.pickle\"):\n",
    "        dataset_preprocessing.process_dataset(dataset_name, target, mode, RS, hct, test_ratio, val_ratio, folds)\n",
    "    with open(f\"../data/prepared/{dataset_name}/{data_path}/data_dict.pickle\", 'rb') as handle:\n",
    "            data_dict = pickle.load(handle)\n",
    "\n",
    "    z_cols = data_dict[\"z_cols\"]\n",
    "    \n",
    "    results[dataset_name] = {}\n",
    "    for fold_num in range(folds):\n",
    "        results[dataset_name][fold_num] = {}\n",
    "\n",
    "        print(f\"Fold no. {fold_num}\")\n",
    "        save_path = f\"../results/{dataset_name}/{data_path}/fold_{fold_num}/HMC\"\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        \n",
    "        z_ohe_encoded_train = data_dict[f\"z_ohe_encoded_train_{fold_num}\"] \n",
    "        z_ohe_encoded_val = data_dict[f\"z_ohe_encoded_val_{fold_num}\"] \n",
    "        z_ohe_encoded_test = data_dict[f\"z_ohe_encoded_test_{fold_num}\"] \n",
    "\n",
    "        z_target_encoded_train = data_dict[f\"z_target_encoded_train_{fold_num}\"] \n",
    "        z_target_encoded_val = data_dict[f\"z_target_encoded_val_{fold_num}\"] \n",
    "        z_target_encoded_test = data_dict[f\"z_target_encoded_test_{fold_num}\"] \n",
    "        \n",
    "        target_encoding_time = data_dict[f\"target_encoding_time_{fold_num}\"]\n",
    "        ohe_encoding_time = data_dict[f\"ohe_encoding_time_{fold_num}\"]\n",
    "        \n",
    "        x_cols = data_dict[f\"X_train_{fold_num}\"].columns\n",
    "        X_train = data_dict[f\"X_train_{fold_num}\"]\n",
    "        Z_train = data_dict[f\"Z_train_{fold_num}\"]\n",
    "        y_train = data_dict[f\"y_train_{fold_num}\"]\n",
    "\n",
    "        X_val = data_dict[f\"X_val_{fold_num}\"]\n",
    "        Z_val = data_dict[f\"Z_val_{fold_num}\"]\n",
    "        y_val = data_dict[f\"y_val_{fold_num}\"]\n",
    "\n",
    "        X_test = data_dict[f\"X_test_{fold_num}\"]\n",
    "        Z_test = data_dict[f\"Z_test_{fold_num}\"]\n",
    "        y_test = data_dict[f\"y_test_{fold_num}\"]\n",
    "    \n",
    "        if not os.path.exists(f\"{save_path}/results_RS{RS}_{dataset_name}_iter{fold_num}.pickle\"):\n",
    "\n",
    "            tf.random.set_seed(RS+fold_num)\n",
    "            np.random.seed(RS+fold_num)\n",
    "\n",
    "            qs = np.max([tf.reduce_max(Z_train, axis=0),tf.reduce_max(Z_val, axis=0),tf.reduce_max(Z_test, axis=0)],axis=0)+1\n",
    "            \n",
    "            X_train = tf.convert_to_tensor(X_train)\n",
    "            Z_train = tf.convert_to_tensor(Z_train,dtype=tf.int32)\n",
    "            y_train = tf.convert_to_tensor(y_train)\n",
    "\n",
    "            X_val = tf.convert_to_tensor(X_val)\n",
    "            Z_val = tf.convert_to_tensor(Z_val,dtype=tf.int32)\n",
    "            y_val = tf.convert_to_tensor(y_val)\n",
    "\n",
    "            X_test = tf.convert_to_tensor(X_test)\n",
    "            Z_test = tf.convert_to_tensor(Z_test,dtype=tf.int32)\n",
    "            y_test = tf.convert_to_tensor(y_test)\n",
    "\n",
    "            if target == \"categorical\":\n",
    "                n_classes = np.unique(y_train).shape[0]\n",
    "            elif target==\"binary\":\n",
    "                n_classes = 1\n",
    "            \n",
    "            y_train = tf.one_hot(tf.cast(y_train,tf.int32),n_classes)\n",
    "            y_val = tf.one_hot(tf.cast(y_val,tf.int32),n_classes)\n",
    "            y_test = tf.one_hot(tf.cast(y_test,tf.int32),n_classes)\n",
    "            \n",
    "            ##### GMENN #####\n",
    "            d = X_train.shape[1] # columns\n",
    "            n = X_train.shape[0] # rows\n",
    "            num_outputs = n_classes\n",
    "            perc_numeric = d/(d+Z_train.shape[1])\n",
    "\n",
    "#             qs = np.max([tf.reduce_max(Z_train, axis=0),tf.reduce_max(Z_val, axis=0),tf.reduce_max(Z_test, axis=0)],axis=0)+1\n",
    "\n",
    "            set_seed(RS)\n",
    "\n",
    "            fe_model, optimizer = get_model(model_name=model_name, input_size=X_train.shape[1], \n",
    "                                              output_size=num_outputs, \n",
    "                                              target=target, \n",
    "                                              perc_numeric=perc_numeric, RS=RS)\n",
    "            \n",
    "            if dataset_name==\"eucalyptus\":\n",
    "                optimizer.learning_rate.assign(optimizer.learning_rate*10)\n",
    "        \n",
    "        \n",
    "            initial_stds = np.ones([len(qs),num_outputs]).astype(float).tolist()\n",
    "\n",
    "            me_model = MixedEffectsNetwork(X_train, Z_train, y_train, fe_model, \n",
    "                                           target=target, qs=qs,\n",
    "                                           initial_stds=initial_stds,\n",
    "                                          fe_loss_weight=1.,\n",
    "                                           mode=\"intercepts\",\n",
    "                                           early_stopping_fe=early_stopping,\n",
    "                                          )    \n",
    "\n",
    "            me_model.compile(\n",
    "                loss_class_me = loss_use()(),\n",
    "                loss_class_fe = loss_use()(),\n",
    "            #     metric_class_me = tf.keras.metrics.AUC(multi_label=True, name=\"auc_me\"),\n",
    "            #     metric_class_fe = tf.keras.metrics.AUC(multi_label=True, name=\"auc_fe\"),\n",
    "                optimizer=optimizer\n",
    "            )\n",
    "\n",
    "            mcmc = MCMCSamplingCallback(num_mcmc_samples=1,\n",
    "                                        perc_burnin=0.7,\n",
    "                                        warm_restart=None,\n",
    "                                        num_burnin_steps=1,\n",
    "                                        step_size = 0.1#initial_step_size,\n",
    "                                   )\n",
    "\n",
    "            print_metric = PrintMetrics(X_train, Z_train, y_train, X_val, Z_val, y_val)\n",
    "\n",
    "            start = time.time()\n",
    "            history = me_model.fit([X_train,Z_train], y_train,\n",
    "                         callbacks=[mcmc,\n",
    "                                    print_metric,\n",
    "                                    tf.keras.callbacks.EarlyStopping(monitor=\"me_auc_val\", patience=early_stopping, mode=\"max\")],\n",
    "                         epochs=epochs,\n",
    "                         validation_data=[[X_val,Z_val],y_val],\n",
    "                        batch_size=batch_size)\n",
    "\n",
    "            end = time.time()\n",
    "            fit_time_gmenn = round(end-start,2)\n",
    "\n",
    "            y_train_pred_gmenn, y_train_pred_gmenn_fe = me_model([X_train,Z_train])\n",
    "            y_val_pred_gmenn, y_val_pred_gmenn_fe = me_model([X_val,Z_val])\n",
    "            y_test_pred_gmenn, y_test_pred_gmenn_fe = me_model([X_test,Z_test])    \n",
    "\n",
    "            \n",
    "            ###### Prepare NN Training ######\n",
    "#             metrics_use = []\n",
    "#             if target ==\"binary\":\n",
    "#                 metrics_use.append(tf.keras.metrics.AUC(name=\"auc\"))\n",
    "#                 metrics_use.append(tf.keras.metrics.Accuracy(name=\"accuracy\"))\n",
    "#                 metrics_use.append(F1Score(num_classes=2, average=\"micro\", name=\"f1\"))\n",
    "#                 stop_mode = \"max\"\n",
    "#                 activation_layer = tf.keras.activations.sigmoid\n",
    "#             elif target ==\"categorical\":\n",
    "#                 metrics_use.append(tf.keras.metrics.AUC(multi_label=True, name=\"auc\"))\n",
    "#                 metrics_use.append(tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"))\n",
    "#                 metrics_use.append(F1Score(num_classes=num_outputs, average=\"weighted\", name=\"f1\"))\n",
    "#                 stop_mode = \"max\"\n",
    "#                 activation_layer = tf.keras.activations.softmax\n",
    "#             elif target == \"continuous\":\n",
    "#                 metrics_use.append(RSquare(name=\"r2\"))\n",
    "#                 metrics_use.append(tf.keras.metrics.MeanSquaredError(name=\"mse\"))\n",
    "#                 stop_mode = \"min\"            \n",
    "            \n",
    "#             ##### Ignore #####\n",
    "#             model_nn, optimizer = get_model(model_name=model_name, \n",
    "#                                             input_size=X_train.shape[1], \n",
    "#                                             output_size=num_outputs, \n",
    "#                                             target=target, \n",
    "#                                             perc_numeric=perc_numeric, RS=RS)\n",
    "#             if dataset_name==\"eucalyptus\":\n",
    "#                 optimizer.learning_rate.assign(optimizer.learning_rate*10)\n",
    "\n",
    "#             model_nn.build((n,d))\n",
    "#             update_layer_activation(model=model_nn, activation=activation_layer)\n",
    "\n",
    "#             model_nn.compile(loss=loss_use()(), optimizer=optimizer, metrics = metrics_use)\n",
    "\n",
    "#             callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_auc\", patience=early_stopping, mode=stop_mode)\n",
    "\n",
    "#             start = time.time()\n",
    "#             history_nn = model_nn.fit(X_train, y_train,\n",
    "#                          validation_data= [X_val, y_val],\n",
    "#                          epochs=epochs, batch_size=batch_size, callbacks=[callback])\n",
    "#             end = time.time()\n",
    "#             fit_time_nn = round(end-start,2)\n",
    "\n",
    "#             y_train_pred_nn = model_nn.predict(X_train ,batch_size=batch_size)\n",
    "#             y_val_pred_nn = model_nn.predict(X_val ,batch_size=batch_size)\n",
    "#             y_test_pred_nn = model_nn.predict(X_test ,batch_size=batch_size)\n",
    "\n",
    "#             if target == \"binary\":\n",
    "#                 eval_res_train_nn = get_metrics(y_train[:,0], y_train_pred_nn, target=target)\n",
    "#                 eval_res_val_nn = get_metrics(y_val[:,0], y_val_pred_nn, target=target)\n",
    "#                 eval_res_test_nn = get_metrics(y_test[:,0], y_test_pred_nn, target=target)\n",
    "#             elif target == \"categorical\":\n",
    "#                 eval_res_train_nn = get_metrics(y_train, y_train_pred_nn, target=target)\n",
    "#                 eval_res_val_nn = get_metrics(y_val, y_val_pred_nn, target=target)\n",
    "#                 eval_res_test_nn = get_metrics(y_test, y_test_pred_nn, target=target)\n",
    "\n",
    "#             ##### Target Encoding #####\n",
    "#             print(\"\\n Train Target Encoding Network\")\n",
    "#             model_nn_te, optimizer = get_model(model_name=model_name, \n",
    "#                                             input_size=np.append(X_train ,z_target_encoded_train, axis=1).shape[1], \n",
    "#                                             output_size=num_outputs, \n",
    "#                                             target=target, \n",
    "#                                             perc_numeric=perc_numeric, RS=RS)\n",
    "#             if dataset_name==\"eucalyptus\":\n",
    "#                 optimizer.learning_rate.assign(optimizer.learning_rate*10)\n",
    "#             model_nn_te.build((n,np.append(X_train ,z_target_encoded_train, axis=1).shape[1]))\n",
    "#             update_layer_activation(model=model_nn_te, activation=activation_layer)\n",
    "#             model_nn_te.compile(loss=loss_use()(), optimizer=optimizer, metrics = metrics_use)\n",
    "#             callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_auc\", patience=early_stopping, mode=stop_mode)\n",
    "\n",
    "#             start = time.time()\n",
    "#             history_nn_te = model_nn_te.fit(np.append(X_train ,z_target_encoded_train, axis=1), y_train,\n",
    "#                          validation_data= [np.append(X_val ,z_target_encoded_val, axis=1), y_val],\n",
    "#                          epochs=epochs, batch_size=batch_size, callbacks=[callback])\n",
    "#             end = time.time()\n",
    "#             fit_time_te = round(end-start,2)+target_encoding_time\n",
    "\n",
    "#             y_train_pred_nn_te = model_nn_te.predict(np.append(X_train ,z_target_encoded_train, axis=1) ,batch_size=batch_size)\n",
    "#             y_val_pred_nn_te = model_nn_te.predict(np.append(X_val ,z_target_encoded_val, axis=1) ,batch_size=batch_size)\n",
    "#             y_test_pred_nn_te = model_nn_te.predict(np.append(X_test ,z_target_encoded_test, axis=1) ,batch_size=batch_size)\n",
    "\n",
    "#             if target == \"binary\":\n",
    "#                 eval_res_train_nn_te = get_metrics(y_train[:,0], y_train_pred_nn_te, target=target)\n",
    "#                 eval_res_val_nn_te = get_metrics(y_val[:,0], y_val_pred_nn_te, target=target)\n",
    "#                 eval_res_test_nn_te = get_metrics(y_test[:,0], y_test_pred_nn_te, target=target)\n",
    "#             elif target == \"categorical\":\n",
    "#                 eval_res_train_nn_te = get_metrics(y_train, y_train_pred_nn_te, target=target)\n",
    "#                 eval_res_val_nn_te = get_metrics(y_val, y_val_pred_nn_te, target=target)\n",
    "#                 eval_res_test_nn_te = get_metrics(y_test, y_test_pred_nn_te, target=target)\n",
    "\n",
    "#             ##### OHE #####\n",
    "#             print(\"\\n Train OHE Network\")\n",
    "#             model_nn_ohe, optimizer = get_model(model_name=model_name, \n",
    "#                                             input_size=np.append(X_train ,z_ohe_encoded_train, axis=1).shape[1], \n",
    "#                                             output_size=num_outputs, \n",
    "#                                             target=target, \n",
    "#                                             perc_numeric=perc_numeric, RS=RS)\n",
    "#             if dataset_name==\"eucalyptus\":\n",
    "#                 optimizer.learning_rate.assign(optimizer.learning_rate*10)\n",
    "#             model_nn_ohe.build((n,np.append(X_train ,z_ohe_encoded_train, axis=1).shape[1]))\n",
    "#             update_layer_activation(model=model_nn_ohe, activation=activation_layer)\n",
    "#             model_nn_ohe.compile(loss=loss_use()(), optimizer=optimizer, metrics = metrics_use)\n",
    "#             callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_auc\", patience=early_stopping, mode=stop_mode)\n",
    "\n",
    "#             start = time.time()\n",
    "#             history_nn_ohe = model_nn_ohe.fit(np.append(X_train ,z_ohe_encoded_train, axis=1), y_train,\n",
    "#                          validation_data= [np.append(X_val ,z_ohe_encoded_val, axis=1), y_val],\n",
    "#                          epochs=epochs, batch_size=batch_size, callbacks=[callback])\n",
    "#             end = time.time()\n",
    "#             fit_time_ohe = round(end-start,2)+ohe_encoding_time\n",
    "\n",
    "#             y_train_pred_nn_ohe = model_nn_ohe.predict(np.append(X_train ,z_ohe_encoded_train, axis=1), batch_size=batch_size)\n",
    "#             y_val_pred_nn_ohe = model_nn_ohe.predict(np.append(X_val ,z_ohe_encoded_val, axis=1), batch_size=batch_size)\n",
    "#             y_test_pred_nn_ohe = model_nn_ohe.predict(np.append(X_test ,z_ohe_encoded_test, axis=1), batch_size=batch_size)\n",
    "            \n",
    "#             if target == \"binary\":\n",
    "#                 eval_res_train_nn_ohe = get_metrics(y_train[:,0], y_train_pred_nn_ohe, target=target)\n",
    "#                 eval_res_val_nn_ohe = get_metrics(y_val[:,0], y_val_pred_nn_ohe, target=target)\n",
    "#                 eval_res_test_nn_ohe = get_metrics(y_test[:,0], y_test_pred_nn_ohe, target=target)            \n",
    "#             elif target == \"categorical\":\n",
    "#                 eval_res_train_nn_ohe = get_metrics(y_train, y_train_pred_nn_ohe, target=target)\n",
    "#                 eval_res_val_nn_ohe = get_metrics(y_val, y_val_pred_nn_ohe, target=target)\n",
    "#                 eval_res_test_nn_ohe = get_metrics(y_test, y_test_pred_nn_ohe, target=target)\n",
    "                \n",
    "#             ##### Embedding #####\n",
    "#             print(\"\\n Embedding Estimate Network\")\n",
    "\n",
    "#             if embed_dims_method==\"sqrt\":\n",
    "#                 embed_dims = [int(np.sqrt(q)) for q in qs]\n",
    "#             elif embed_dims_method==\"AutoGluon\":\n",
    "#                 embed_dims = [int(np.max([100, np.round(1.6*q**0.56)])) for q in qs]\n",
    "#             else:\n",
    "#                 embed_dims = [10 for q in qs]\n",
    "\n",
    "#             input_layer = Input(shape=(d,))\n",
    "\n",
    "#             # Define embedding layers\n",
    "#             embed_inputs = []\n",
    "#             embedding_layers = []\n",
    "#             for q_num in range(len(qs)):\n",
    "#                 Z_input_layer = Input(shape=(1,))\n",
    "#                 embedding_layer = Embedding(qs[q_num], embed_dims[q_num], input_length=1)(Z_input_layer)\n",
    "#                 embedding_layer = Reshape(target_shape=(embed_dims[q_num],))(embedding_layer)\n",
    "\n",
    "#                 embed_inputs.append(Z_input_layer)\n",
    "#                 embedding_layers.append(embedding_layer)\n",
    "\n",
    "#             ### Get model layer dimensions\n",
    "#             min_numeric_embed_dim = 32\n",
    "#             max_numeric_embed_dim = 2056\n",
    "#             max_layer_width = 2056\n",
    "#             # Main dense model\n",
    "#             if target == \"continuous\":\n",
    "#                 default_layer_sizes = [256,\n",
    "#                                        128]  # overall network will have 4 layers. Input layer, 256-unit hidden layer, 128-unit hidden layer, output layer.\n",
    "#             else:\n",
    "#                 default_sizes = [256, 128]  # will be scaled adaptively\n",
    "#                 # base_size = max(1, min(num_net_outputs, 20)/2.0) # scale layer width based on number of classes\n",
    "#                 base_size = max(1, min(num_outputs,\n",
    "#                                        100) / 50)  # TODO: Updated because it improved model quality and made training far faster\n",
    "#                 default_layer_sizes = [defaultsize * base_size for defaultsize in default_sizes]\n",
    "#             layer_expansion_factor = 1  # TODO: consider scaling based on num_rows, eg: layer_expansion_factor = 2-np.exp(-max(0,train_dataset.num_examples-10000))\n",
    "#             first_layer_width = int(min(max_layer_width, layer_expansion_factor * default_layer_sizes[0]))\n",
    "\n",
    "#             # numeric embed dim\n",
    "#             vector_dim = 0  # total dimensionality of vector features (I think those should be transformed string features, which we don't have)\n",
    "#             prop_vector_features = perc_numeric  # Fraction of features that are numeric\n",
    "#             numeric_embedding_size = int(min(max_numeric_embed_dim,\n",
    "#                                              max(min_numeric_embed_dim,\n",
    "#                                                  first_layer_width * prop_vector_features * np.log10(vector_dim + 10))))\n",
    "\n",
    "\n",
    "#             numeric_embedding = Dense(numeric_embedding_size, activation=\"relu\")(input_layer)\n",
    "\n",
    "#             concat = Concatenate()([numeric_embedding] + embedding_layers)\n",
    "\n",
    "#             base_model, optimizer = get_model(model_name=model_name, \n",
    "#                                               input_size=numeric_embedding_size + sum(embed_dims), \n",
    "#                                               output_size=num_outputs, target=target,\n",
    "#                                               perc_numeric=perc_numeric, RS=RS)\n",
    "\n",
    "#             if dataset_name==\"eucalyptus\":\n",
    "#                 optimizer.learning_rate.assign(optimizer.learning_rate*10)\n",
    "#             base_model.build((n, numeric_embedding_size + sum(embed_dims)))\n",
    "#             update_layer_activation(model=base_model, activation=activation_layer)\n",
    "\n",
    "#             layers = base_model(concat)\n",
    "\n",
    "#             model_embed = Model(inputs=[input_layer] + embed_inputs, outputs=layers)\n",
    "\n",
    "\n",
    "#             model_embed.compile(loss=loss_use()(), optimizer=optimizer, metrics = metrics_use)\n",
    "#             callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_auc\", patience=early_stopping, mode=stop_mode)\n",
    "\n",
    "#             start = time.time()\n",
    "#             history_nn_embed = model_embed.fit([X_train] + [Z_train[: ,q_num] for q_num in range(len(qs))], y_train,\n",
    "#                             validation_data=[[X_val] + [Z_val[: ,q_num] for q_num in range(len(qs))], y_val],\n",
    "#                             epochs=epochs, batch_size=batch_size, callbacks=[callback])\n",
    "#             end = time.time()\n",
    "#             fit_time_embed = round(end-start,2)\n",
    "\n",
    "#             y_train_pred_embed = model_embed.predict([X_train] + [Z_train[: ,q_num] for q_num in range(len(qs))]\n",
    "#                                                      ,batch_size=batch_size)\n",
    "#             y_val_pred_embed = model_embed.predict([X_val] + [Z_val[: ,q_num] for q_num in range(len(qs))]\n",
    "#                                                     ,batch_size=batch_size)\n",
    "#             y_test_pred_embed = model_embed.predict([X_test] + [Z_test[: ,q_num] for q_num in range(len(qs))]\n",
    "#                                                     ,batch_size=batch_size)\n",
    "\n",
    "#             if target == \"binary\":\n",
    "#                 eval_res_train_embed = get_metrics(y_train[:,0], y_train_pred_embed, target=target)\n",
    "#                 eval_res_val_embed = get_metrics(y_val[:,0], y_val_pred_embed, target=target)\n",
    "#                 eval_res_test_embed = get_metrics(y_test[:,0], y_test_pred_embed, target=target)\n",
    "#             elif target == \"categorical\":\n",
    "#                 eval_res_train_embed = get_metrics(y_train, y_train_pred_embed, target=target)\n",
    "#                 eval_res_val_embed = get_metrics(y_val, y_val_pred_embed, target=target)\n",
    "#                 eval_res_test_embed = get_metrics(y_test, y_test_pred_embed, target=target)\n",
    "\n",
    "#             eval_res_train_embed, eval_res_test_embed        \n",
    "\n",
    "\n",
    "\n",
    "            ##### Document Results #####\n",
    "            \n",
    "            results[dataset_name][fold_num][\"histories\"] = {\"GMENN\": history.history,\n",
    "#                                                        \"Ignore\": history_nn.history,\n",
    "#                                                        \"TE\": history_nn_te.history,\n",
    "#                                                        \"OHE\": history_nn_ohe.history,\n",
    "#                                                        \"Embedding\": history_nn_embed.history,\n",
    "                                                      }\n",
    "            \n",
    "            results[dataset_name][fold_num][\"predictions\"] = {\"GMENN\": [y_train_pred_gmenn, y_val_pred_gmenn, y_test_pred_gmenn],\n",
    "                                                        \"GMENN (FE)\": [y_train_pred_gmenn_fe, y_val_pred_gmenn_fe, y_test_pred_gmenn_fe],\n",
    "#                                                         \"Ignore\": [y_train_pred_nn, y_val_pred_nn, y_test_pred_nn],\n",
    "#                                                         \"TE\": [y_train_pred_nn_te, y_val_pred_nn_te, y_test_pred_nn_te],\n",
    "#                                                         \"OHE\": [y_train_pred_nn_ohe, y_val_pred_nn_ohe, y_test_pred_nn_ohe],\n",
    "#                                                         \"Embedding\": [y_train_pred_embed, y_val_pred_embed, y_test_pred_embed],\n",
    "                                                     }\n",
    "            \n",
    "            results[dataset_name][fold_num][\"times\"] = {\"GMENN\": fit_time_gmenn,\n",
    "#                                                    \"Ignore\": fit_time_nn,\n",
    "#                                                    \"TE\": fit_time_te,\n",
    "#                                                    \"OHE\": fit_time_ohe,\n",
    "#                                                    \"Embedding\": fit_time_embed,\n",
    "                                                      }\n",
    "            \n",
    "            results[dataset_name][fold_num][\"other_info\"] = {\n",
    "                \"GMENN\": {\n",
    "#                     \"_stddev_z\": np.array([i.numpy() for i in me_model.data_model._stddev_z]),\n",
    "#                     \"acceptance_rates\": np.array(me_model.acceptance_rates),\n",
    "#                     \"random_effects\": me_model.mean_samples,\n",
    "#                     \"all_samples\": me_model.all_samples,\n",
    "#                     \"stds\": me_model.stds\n",
    "                },\n",
    "            }\n",
    "            \n",
    "            \n",
    "            with open(f\"{save_path}//results_RS{RS}_{dataset_name}_iter{fold_num}.pickle\", 'wb') as handle:\n",
    "                pickle.dump(results[dataset_name][fold_num], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "            \n",
    "            del X_train, X_val, X_test, y_train, y_val, y_test\n",
    "#             del z_target_encoded_train, z_target_encoded_val, z_target_encoded_test\n",
    "#             del z_ohe_encoded_train, z_ohe_encoded_val, z_ohe_encoded_test\n",
    "            \n",
    "            gc.collect()\n",
    "        else:\n",
    "            print(f\"Load results for dataset {dataset_name}, iteration={fold_num}\")\n",
    "            with open(f\"{save_path}/results_RS{RS}_{dataset_name}_iter{fold_num}.pickle\", 'rb') as handle:\n",
    "                results[dataset_name][fold_num] = pickle.load(handle)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219c02f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63d93847",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dcfb167",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set nan for eucalyptus, 0\n",
      "Set nan for eucalyptus, 0\n",
      "Set nan for eucalyptus, 0\n",
      "Set nan for eucalyptus, 0\n",
      "Set nan for eucalyptus, 1\n",
      "Set nan for eucalyptus, 1\n",
      "Set nan for eucalyptus, 1\n",
      "Set nan for eucalyptus, 1\n",
      "Set nan for eucalyptus, 2\n",
      "Set nan for eucalyptus, 2\n",
      "Set nan for eucalyptus, 2\n",
      "Set nan for eucalyptus, 2\n",
      "Set nan for eucalyptus, 3\n",
      "Set nan for eucalyptus, 3\n",
      "Set nan for eucalyptus, 3\n",
      "Set nan for eucalyptus, 3\n",
      "Set nan for eucalyptus, 4\n",
      "Set nan for eucalyptus, 4\n",
      "Set nan for eucalyptus, 4\n",
      "Set nan for eucalyptus, 4\n",
      "Set nan for Midwest_survey, 0\n",
      "Set nan for Midwest_survey, 0\n",
      "Set nan for Midwest_survey, 0\n",
      "Set nan for Midwest_survey, 0\n",
      "Set nan for Midwest_survey, 1\n",
      "Set nan for Midwest_survey, 1\n",
      "Set nan for Midwest_survey, 1\n",
      "Set nan for Midwest_survey, 1\n",
      "Set nan for Midwest_survey, 2\n",
      "Set nan for Midwest_survey, 2\n",
      "Set nan for Midwest_survey, 2\n",
      "Set nan for Midwest_survey, 2\n",
      "Set nan for Midwest_survey, 3\n",
      "Set nan for Midwest_survey, 3\n",
      "Set nan for Midwest_survey, 3\n",
      "Set nan for Midwest_survey, 3\n",
      "Set nan for Midwest_survey, 4\n",
      "Set nan for Midwest_survey, 4\n",
      "Set nan for Midwest_survey, 4\n",
      "Set nan for Midwest_survey, 4\n",
      "Set nan for hpc-job-scheduling, 0\n",
      "Set nan for hpc-job-scheduling, 0\n",
      "Set nan for hpc-job-scheduling, 0\n",
      "Set nan for hpc-job-scheduling, 0\n",
      "Set nan for hpc-job-scheduling, 1\n",
      "Set nan for hpc-job-scheduling, 1\n",
      "Set nan for hpc-job-scheduling, 1\n",
      "Set nan for hpc-job-scheduling, 1\n",
      "Set nan for hpc-job-scheduling, 2\n",
      "Set nan for hpc-job-scheduling, 2\n",
      "Set nan for hpc-job-scheduling, 2\n",
      "Set nan for hpc-job-scheduling, 2\n",
      "Set nan for hpc-job-scheduling, 3\n",
      "Set nan for hpc-job-scheduling, 3\n",
      "Set nan for hpc-job-scheduling, 3\n",
      "Set nan for hpc-job-scheduling, 3\n",
      "Set nan for hpc-job-scheduling, 4\n",
      "Set nan for hpc-job-scheduling, 4\n",
      "Set nan for hpc-job-scheduling, 4\n",
      "Set nan for hpc-job-scheduling, 4\n",
      "Set nan for video-game-sales, 0\n",
      "Set nan for video-game-sales, 0\n",
      "Set nan for video-game-sales, 0\n",
      "Set nan for video-game-sales, 0\n",
      "Set nan for video-game-sales, 1\n",
      "Set nan for video-game-sales, 1\n",
      "Set nan for video-game-sales, 1\n",
      "Set nan for video-game-sales, 1\n",
      "Set nan for video-game-sales, 2\n",
      "Set nan for video-game-sales, 2\n",
      "Set nan for video-game-sales, 2\n",
      "Set nan for video-game-sales, 2\n",
      "Set nan for video-game-sales, 3\n",
      "Set nan for video-game-sales, 3\n",
      "Set nan for video-game-sales, 3\n",
      "Set nan for video-game-sales, 3\n",
      "Set nan for video-game-sales, 4\n",
      "Set nan for video-game-sales, 4\n",
      "Set nan for video-game-sales, 4\n",
      "Set nan for video-game-sales, 4\n",
      "Set nan for okcupid-stem, 0\n",
      "Set nan for okcupid-stem, 0\n",
      "Set nan for okcupid-stem, 0\n",
      "Set nan for okcupid-stem, 0\n",
      "Set nan for okcupid-stem, 1\n",
      "Set nan for okcupid-stem, 1\n",
      "Set nan for okcupid-stem, 1\n",
      "Set nan for okcupid-stem, 1\n",
      "Set nan for okcupid-stem, 2\n",
      "Set nan for okcupid-stem, 2\n",
      "Set nan for okcupid-stem, 2\n",
      "Set nan for okcupid-stem, 2\n",
      "Set nan for okcupid-stem, 3\n",
      "Set nan for okcupid-stem, 3\n",
      "Set nan for okcupid-stem, 3\n",
      "Set nan for okcupid-stem, 3\n",
      "Set nan for okcupid-stem, 4\n",
      "Set nan for okcupid-stem, 4\n",
      "Set nan for okcupid-stem, 4\n",
      "Set nan for okcupid-stem, 4\n",
      "Set nan for Diabetes130US, 0\n",
      "Set nan for Diabetes130US, 0\n",
      "Set nan for Diabetes130US, 0\n",
      "Set nan for Diabetes130US, 0\n",
      "Set nan for Diabetes130US, 1\n",
      "Set nan for Diabetes130US, 1\n",
      "Set nan for Diabetes130US, 1\n",
      "Set nan for Diabetes130US, 1\n",
      "Set nan for Diabetes130US, 2\n",
      "Set nan for Diabetes130US, 2\n",
      "Set nan for Diabetes130US, 2\n",
      "Set nan for Diabetes130US, 2\n",
      "Set nan for Diabetes130US, 3\n",
      "Set nan for Diabetes130US, 3\n",
      "Set nan for Diabetes130US, 3\n",
      "Set nan for Diabetes130US, 3\n",
      "Set nan for Diabetes130US, 4\n",
      "Set nan for Diabetes130US, 4\n",
      "Set nan for Diabetes130US, 4\n",
      "Set nan for Diabetes130US, 4\n"
     ]
    }
   ],
   "source": [
    "models = [\"GMENN\", \"TE\", \"OHE\", \"Embedding\",\"Ignore\"]\n",
    "\n",
    "results_perf = {dataset_name: {num: {model: {}  for model in models} for num in range(folds)} for dataset_name in dataset_names}\n",
    "for dataset_name in dataset_names:\n",
    "    try:\n",
    "        with open(f\"../data/prepared/{dataset_name}/{data_path}/data_dict.pickle\", 'rb') as handle:\n",
    "            data_dict = pickle.load(handle)        \n",
    "    except:\n",
    "        print(f\"dataset {dataset_name} not found\") \n",
    "    for num in range(folds):\n",
    "        y_test = data_dict[f\"y_test_{num}\"]\n",
    "        n_classes = np.unique(y_test).shape[0]\n",
    "        y_test = tf.one_hot(data_dict[f\"y_test_{num}\"],n_classes)\n",
    "        for model in models:\n",
    "            try:\n",
    "                y_pred = results[dataset_name][num][\"predictions\"][model][2]\n",
    "\n",
    "                results_perf[dataset_name][num][model] = get_metrics(y_test,y_pred,target)\n",
    "                results_perf[dataset_name][num][model][\"Time\"] = results[dataset_name][num][\"times\"][model]\n",
    "            except:\n",
    "                print(f\"Set nan for {dataset_name}, {num}\")\n",
    "                results_perf[dataset_name][num][model] = {\"Accuracy\": np.nan,\n",
    "                                                          \"AUROC\": np.nan,\n",
    "                                                          \"F1\": np.nan,\n",
    "                                                          \"Time\": np.nan}\n",
    "#                 print(f\"Didnt work for {dataset_name}, {num}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3246b11d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GMENN</th>\n",
       "      <th>TE</th>\n",
       "      <th>OHE</th>\n",
       "      <th>Embedding</th>\n",
       "      <th>Ignore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eucalyptus</th>\n",
       "      <td>0.89 (0.027)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Midwest_survey</th>\n",
       "      <td>0.85 (0.006)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hpc-job-scheduling</th>\n",
       "      <td>0.91 (0.006)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video-game-sales</th>\n",
       "      <td>0.78 (0.006)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>okcupid-stem</th>\n",
       "      <td>0.79 (0.006)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Diabetes130US</th>\n",
       "      <td>0.64 (0.005)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           GMENN         TE        OHE  Embedding     Ignore\n",
       "eucalyptus          0.89 (0.027)  nan (nan)  nan (nan)  nan (nan)  nan (nan)\n",
       "Midwest_survey      0.85 (0.006)  nan (nan)  nan (nan)  nan (nan)  nan (nan)\n",
       "hpc-job-scheduling  0.91 (0.006)  nan (nan)  nan (nan)  nan (nan)  nan (nan)\n",
       "video-game-sales    0.78 (0.006)  nan (nan)  nan (nan)  nan (nan)  nan (nan)\n",
       "okcupid-stem        0.79 (0.006)  nan (nan)  nan (nan)  nan (nan)  nan (nan)\n",
       "Diabetes130US       0.64 (0.005)  nan (nan)  nan (nan)  nan (nan)  nan (nan)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [\"GMENN\", \"TE\", \"OHE\", \"Embedding\", \"Ignore\"]\n",
    "\n",
    "metric = \"AUROC\"\n",
    "\n",
    "#####\n",
    "dataset_res_dict = {}\n",
    "best_models = {}\n",
    "t_test_results = {}\n",
    "\n",
    "round_mean_at = 2\n",
    "round_std_at = 3\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    dataset_models = list(results_perf[dataset_name][0].keys())\n",
    "    use_df = pd.DataFrame([pd.DataFrame(results_perf[dataset_name][fold_num]).loc[metric,models] for fold_num in results_perf[dataset_name].keys()],index=results_perf[dataset_name].keys())\n",
    "    \n",
    "    df_mean = pd.DataFrame(use_df.mean(axis=0).round(round_mean_at).astype(str) + \" (\" + use_df.std(axis=0).round(round_std_at).astype(str) + \")\").transpose()\n",
    "    model_dict = {i: df_mean[i].values[0] for i in df_mean.columns}\n",
    "    dataset_res_dict[dataset_name] = model_dict\n",
    "    \n",
    "    best_models[dataset_name] = use_df.columns[use_df.mean(axis=0).argmax()]\n",
    "\n",
    "    t_test_res = np.array([stats.ttest_rel(use_df[best_models[dataset_name]].values, use_df[model].values)[1] if model in dataset_models else 0 for model in models]).round(3)\n",
    "    t_test_res[np.isnan(t_test_res)] = 1.\n",
    "    t_test_results[dataset_name] = t_test_res\n",
    "    \n",
    "res_df = pd.DataFrame(dataset_res_dict).transpose()\n",
    "    \n",
    "def negative_bold(val):\n",
    "    i = np.where(val.name==np.array(models))[0][0]\n",
    "    return [\"font-weight: bold\"  if t_test_results[dataset_name][i]>=0.05 else \"\" for dataset_name in val.keys()]\n",
    "    # Case without transpose:\n",
    "#     return [\"font-weight: bold\"  if t_test_results[val.name][i]>=0.05 else \"\" for i in range(len(val))]\n",
    "\n",
    "# res_df.style.apply(negative_bold)\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2c72e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.89, 0.85, 0.91, 0.78, 0.79, 0.64])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df[\"GMENN\"].apply(lambda x: float(x.split(\" \")[0])).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0d4d2b",
   "metadata": {},
   "source": [
    "### Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd49a259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GMENN</th>\n",
       "      <th>TE</th>\n",
       "      <th>OHE</th>\n",
       "      <th>Embedding</th>\n",
       "      <th>Ignore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eucalyptus</th>\n",
       "      <td>1.59 (0.894)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Midwest_survey</th>\n",
       "      <td>1.75 (0.842)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hpc-job-scheduling</th>\n",
       "      <td>1.24 (0.321)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video-game-sales</th>\n",
       "      <td>0.76 (0.187)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>okcupid-stem</th>\n",
       "      <td>1.86 (1.276)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Diabetes130US</th>\n",
       "      <td>6.35 (0.866)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           GMENN         TE        OHE  Embedding     Ignore\n",
       "eucalyptus          1.59 (0.894)  nan (nan)  nan (nan)  nan (nan)  nan (nan)\n",
       "Midwest_survey      1.75 (0.842)  nan (nan)  nan (nan)  nan (nan)  nan (nan)\n",
       "hpc-job-scheduling  1.24 (0.321)  nan (nan)  nan (nan)  nan (nan)  nan (nan)\n",
       "video-game-sales    0.76 (0.187)  nan (nan)  nan (nan)  nan (nan)  nan (nan)\n",
       "okcupid-stem        1.86 (1.276)  nan (nan)  nan (nan)  nan (nan)  nan (nan)\n",
       "Diabetes130US       6.35 (0.866)  nan (nan)  nan (nan)  nan (nan)  nan (nan)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [\"GMENN\", \"TE\", \"OHE\", \"Embedding\", \"Ignore\"]\n",
    "metric = \"Time\"\n",
    "\n",
    "#####\n",
    "dataset_res_dict = {}\n",
    "best_models = {}\n",
    "t_test_results = {}\n",
    "\n",
    "round_mean_at = 2\n",
    "round_std_at = 3\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    dataset_models = list(results_perf[dataset_name][0].keys())\n",
    "    use_df = pd.DataFrame([pd.DataFrame(results_perf[dataset_name][fold_num]).loc[metric,models] for fold_num in results_perf[dataset_name].keys()],index=results_perf[dataset_name].keys())/60\n",
    "    \n",
    "    df_mean = pd.DataFrame(use_df.mean(axis=0).round(round_mean_at).astype(str) + \" (\" + use_df.std(axis=0).round(round_std_at).astype(str) + \")\").transpose()\n",
    "    model_dict = {i: df_mean[i].values[0] for i in df_mean.columns}\n",
    "    dataset_res_dict[dataset_name] = model_dict\n",
    "    \n",
    "    best_models[dataset_name] = use_df.columns[use_df.mean(axis=0).argmin()]\n",
    "\n",
    "    t_test_res = np.array([stats.ttest_rel(use_df[best_models[dataset_name]].values, use_df[model].values)[1] if model in dataset_models else 0 for model in models]).round(3)\n",
    "    t_test_res[np.isnan(t_test_res)] = 1.\n",
    "    t_test_results[dataset_name] = t_test_res\n",
    "    \n",
    "res_df = pd.DataFrame(dataset_res_dict).transpose()\n",
    "    \n",
    "def negative_bold(val):\n",
    "    i = np.where(val.name==np.array(models))[0][0]\n",
    "    return [\"font-weight: bold\"  if t_test_results[dataset_name][i]>=0.05 else \"\" for dataset_name in val.keys()]\n",
    "    # Case without transpose:\n",
    "#     return [\"font-weight: bold\"  if t_test_results[val.name][i]>=0.05 else \"\" for i in range(len(val))]\n",
    "\n",
    "# res_df.style.apply(negative_bold)\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d068c049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.59, 1.75, 1.24, 0.76, 1.86, 6.35])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df[\"GMENN\"].apply(lambda x: float(x.split(\" \")[0])).values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gmenn",
   "language": "python",
   "name": "gmenn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc07112e7ae1e8e28a0232207620ff002934c05692de8df42430404c766a0a8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
