{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca4ea20d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Found GPU: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# Append root path \n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../lmmnn\")\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"]=\"true\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "if tf.test.gpu_device_name() != '/device:GPU:0':\n",
    "    print('WARNING: GPU device not found.')\n",
    "else:\n",
    "    print('SUCCESS: Found GPU: {}'.format(tf.test.gpu_device_name()))\n",
    "\n",
    "from model.mixed_effects import *\n",
    "from utils.fe_models import get_model\n",
    "from utils.evaluation import *\n",
    "from utils.utils import *\n",
    "from data.preprocessing import dataset_preprocessing\n",
    "from utils.training_functions import *\n",
    "\n",
    "# from vis.utils.utils import apply_modifications\n",
    "# # helper function\n",
    "def update_layer_activation(model, activation, index=-1):\n",
    "    model.layers[index].activation = activation\n",
    "    return model\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Reshape, Embedding, Concatenate\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "\n",
    "from sklearn.metrics import accuracy_score as acc\n",
    "from sklearn.metrics import roc_auc_score as auroc\n",
    "from sklearn.metrics import f1_score as f1\n",
    "from sklearn.model_selection import train_test_split\n",
    "from category_encoders import TargetEncoder\n",
    "from tensorflow_addons.metrics import F1Score\n",
    "\n",
    "from scipy import stats\n",
    "import pickle\n",
    "import yaml\n",
    "import time\n",
    "import gc\n",
    "\n",
    "RS = 555"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "102d8696",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCMCSamplingCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self,\n",
    "                 num_mcmc_samples=1,\n",
    "                 step_size=0.01,\n",
    "                 perc_burnin=0.1,\n",
    "                 num_burnin_steps=0,\n",
    "                 warm_restart=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_mcmc_samples = tf.constant(num_mcmc_samples)\n",
    "        self.perc_burnin = perc_burnin\n",
    "        self.num_burnin_steps = num_burnin_steps\n",
    "        self.warm_restart = warm_restart\n",
    "        self.step_size = tf.Variable(step_size,trainable=False)\n",
    "        self.step_sizes = []\n",
    "\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.mcmc_kernel = tfp.mcmc.MetropolisAdjustedLangevinAlgorithm(\n",
    "            target_log_prob_fn=self.model.target_log_prob_fn,\n",
    "            step_size=self.step_size,\n",
    "        )\n",
    "\n",
    "        self.get_mcmc_kernel = lambda step_size: tfp.mcmc.MetropolisAdjustedLangevinAlgorithm(\n",
    "            target_log_prob_fn=self.model.target_log_prob_fn,\n",
    "            step_size=step_size)\n",
    "\n",
    "        # self.mcmc_kernel = tfp.mcmc.DualAveragingStepSizeAdaptation(\n",
    "        #     inner_kernel=tfp.mcmc.NoUTurnSampler(\n",
    "        #         target_log_prob_fn=self.model.target_log_prob_fn,\n",
    "        #         step_size=self.step_size),\n",
    "        #     num_adaptation_steps=500,\n",
    "        #     target_accept_prob=0.651)\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch==0:\n",
    "            self.model.all_samples.extend(([[state[num] for state in self.model.current_state] for num in range(1)]))\n",
    "\n",
    "            self.model.mean_samples = [tf.reduce_mean([sample[q] for sample in self.model.all_samples[round(epoch*(self.perc_burnin)):]], axis=0) for q in\n",
    "                                       range(len(self.model.qs))]\n",
    "        if self.model.fe_pretraining:\n",
    "            if self.model.fe_converged:\n",
    "                self.run_sampling(epoch)\n",
    "            else:\n",
    "                self.model.acceptance_rates.append(-1)\n",
    "        else:\n",
    "            self.run_sampling(epoch)\n",
    "\n",
    "    def run_sampling(self,epoch):\n",
    "        self.model.fX.assign(self.model.fe_model(self.model.X, training=False))\n",
    "\n",
    "        if self.model.embed_x:\n",
    "            self.model.X_embedded.assign(self.model.X_embed_model(self.model.X, training=False))\n",
    "\n",
    "        if self.model.embed_z:\n",
    "            for q_num in range(len(self.model.qs)):\n",
    "                self.model.Z_embedded[q_num].assign(self.model.Z_embed_models[q_num](self.model.Z[:,q_num], training=False))\n",
    "\n",
    "                ## Find initial step size\n",
    "        # if self.model.previous_kernel_results.log_accept_ratio == -np.inf:\n",
    "        # if len(self.model.acceptance_rates)>0 and self.model.acceptance_rates[-1]<0.5:\n",
    "        if len(self.model.acceptance_rates)>0 and self.model.acceptance_rates[-1]<0.0001:\n",
    "            # self.mcmc_kernel.parameters[\"step_size\"] = self.mcmc_kernel.parameters[\"step_size\"]/2\n",
    "            # self.model.previous_kernel_results[\"new_step_size\"] = self.model.previous_kernel_results.step_size/2\n",
    "            # setattr(self.model.previous_kernel_results, \"new_step_size\", self.model.previous_kernel_results.step_size/2)\n",
    "            self.step_size.assign(self.step_size/2)\n",
    "            print(f\"Adapt step size to {float(self.step_size)}\")\n",
    "\n",
    "\n",
    "        if self.warm_restart!=None and epoch>0:\n",
    "            ## Warm restart\n",
    "            # if self.model.previous_kernel_results.log_accept_ratio == -np.inf:\n",
    "                # restart = True\n",
    "            # else:\n",
    "                # restart = False\n",
    "            # else:\n",
    "            restart = ((epoch + 1) % self.warm_restart) == 0 and epoch != 0\n",
    "\n",
    "            if restart:\n",
    "                print(\"\\n Warm restart to unstuck the chain\")\n",
    "                if self.model.embed_z and self.model.embed_x:\n",
    "                    self.model.current_state = self.model.data_model(self.model.fX, self.model.X_embedded, self.model.Z_embedded).sample(1, seed=self.model.RS)[:-1]\n",
    "                elif self.model.embed_z and not self.model.embed_x:\n",
    "                    self.model.current_state = self.model.data_model(self.model.fX, self.model.X, self.model.Z_embedded).sample(1, seed=self.model.RS)[:-1]\n",
    "                elif not self.model.embed_z and self.model.embed_x:\n",
    "                    self.model.current_state = self.model.data_model(self.model.fX, self.model.X_embedded, self.model.Z).sample(1, seed=self.model.RS)[:-1]\n",
    "                else:\n",
    "                    self.model.current_state = self.model.data_model(self.model.fX, self.model.X, self.model.Z).sample(1, seed=self.model.RS)[:-1]\n",
    "\n",
    "        print(\"\\n Start sampling for epoch {} of training\".format(epoch + 1))\n",
    "        start = time.time()\n",
    "        new_state, self.model.previous_kernel_results = self.get_mcmc_samples(self.model.current_state,\n",
    "                                                                              tf.constant(self.num_mcmc_samples),\n",
    "                                                                              None\n",
    "                                                                                               )\n",
    "        # self.model.divide_constants.assign(\n",
    "        #     list(1/np.mean(self.model.data_model._stddev_z,axis=1))+[1.])\n",
    "        # self.model.divide_constants.assign(\n",
    "        #     list((lambda x: 1+(x-x.mean()))(np.array(1+tf.math.softmax(1/len(self.model.qs)+0.5*tf.math.softmax(np.abs([np.mean(i) for i in self.model.previous_kernel_results.grads_target_log_prob]))))))+[1.])\n",
    "        # print(np.round(self.model.divide_constants,2))\n",
    "        try:\n",
    "            log_accept_ratio = self.model.previous_kernel_results.log_accept_ratio\n",
    "        except:\n",
    "            log_accept_ratio = self.model.previous_kernel_results.inner_results.log_accept_ratio\n",
    "        acceptance_rate = tf.math.exp(tf.minimum(log_accept_ratio, 0.))\n",
    "\n",
    "        self.step_sizes.append(float(self.step_size))\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        self.model.current_state = [tf.identity(i) for i in new_state]\n",
    "        # Todo: Append all current states\n",
    "        self.model.acceptance_rates.append(acceptance_rate)\n",
    "        # self.model.all_samples.append(\n",
    "        #     [tf.math.reduce_mean(self.model.current_state[q_num], axis=0) for q_num in range(len(self.model.qs))])\n",
    "        self.model.all_samples.extend(([[state[num] for state in self.model.current_state] for num in range(self.num_mcmc_samples)]))\n",
    "\n",
    "        self.model.mean_samples = [tf.reduce_mean([sample[q] for sample in self.model.all_samples[round(epoch*(self.perc_burnin)):]], axis=0) for q in\n",
    "                                   range(len(self.model.qs))]\n",
    "\n",
    "        self.model.e_step_times.append(round(end - start, 2))\n",
    "\n",
    "    # def on_epoch_end(self, epoch, logs=None):\n",
    "        for q_num in range(len(self.model.qs)):\n",
    "                self.model.data_model.trainable_variables[q_num].assign(\n",
    "                    tf.math.reduce_std(self.model.current_state[q_num][-1],axis=0))\n",
    "\n",
    "        self.model.stds.append([tf.identity(i) for i in self.model.data_model._stddev_z])\n",
    "\n",
    "    @tf.function(reduce_retracing=True)  # autograph=False, jit_compile=True, reduce_retracing=True)\n",
    "    def get_mcmc_samples(self, current_state, num_mcmc_samples=tf.constant(1), previous_kernel_results=None):\n",
    "        samples, _, previous_kernel_results = tfp.mcmc.sample_chain(\n",
    "            kernel=self.get_mcmc_kernel(self.step_size), num_results=num_mcmc_samples,\n",
    "            current_state=[state[-1] for state in current_state],\n",
    "            num_burnin_steps=self.num_burnin_steps,\n",
    "            trace_fn=None, previous_kernel_results=previous_kernel_results,\n",
    "            return_final_kernel_results=True, seed=self.model.RS)\n",
    "        #     current_state=[sample[-1] for sample in samples]\n",
    "\n",
    "        return samples, previous_kernel_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258caf10",
   "metadata": {},
   "source": [
    "#### Download and save data from Pargent et al. by running \"data/download_pargent2022_datasets.py before running this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "057f4663",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training procedure for churn\n",
      "Fold no. 0\n",
      "Load results for dataset churn, iteration=0\n",
      "Fold no. 1\n",
      "Load results for dataset churn, iteration=1\n",
      "Fold no. 2\n",
      "Load results for dataset churn, iteration=2\n",
      "Fold no. 3\n",
      "Load results for dataset churn, iteration=3\n",
      "Fold no. 4\n",
      "Load results for dataset churn, iteration=4\n",
      "Start training procedure for kdd_internet_usage\n",
      "Fold no. 0\n",
      "Load results for dataset kdd_internet_usage, iteration=0\n",
      "Fold no. 1\n",
      "Load results for dataset kdd_internet_usage, iteration=1\n",
      "Fold no. 2\n",
      "Load results for dataset kdd_internet_usage, iteration=2\n",
      "Fold no. 3\n",
      "Load results for dataset kdd_internet_usage, iteration=3\n",
      "Fold no. 4\n",
      "Load results for dataset kdd_internet_usage, iteration=4\n",
      "Start training procedure for Amazon_employee_access\n",
      "Fold no. 0\n",
      "Load results for dataset Amazon_employee_access, iteration=0\n",
      "Fold no. 1\n",
      "Load results for dataset Amazon_employee_access, iteration=1\n",
      "Fold no. 2\n",
      "Load results for dataset Amazon_employee_access, iteration=2\n",
      "Fold no. 3\n",
      "Load results for dataset Amazon_employee_access, iteration=3\n",
      "Fold no. 4\n",
      "Load results for dataset Amazon_employee_access, iteration=4\n",
      "Start training procedure for Click_prediction_small\n",
      "Fold no. 0\n",
      "Load results for dataset Click_prediction_small, iteration=0\n",
      "Fold no. 1\n",
      "Load results for dataset Click_prediction_small, iteration=1\n",
      "Fold no. 2\n",
      "Load results for dataset Click_prediction_small, iteration=2\n",
      "Fold no. 3\n",
      "Load results for dataset Click_prediction_small, iteration=3\n",
      "Fold no. 4\n",
      "Load results for dataset Click_prediction_small, iteration=4\n",
      "Start training procedure for adult\n",
      "Fold no. 0\n",
      "Load results for dataset adult, iteration=0\n",
      "Fold no. 1\n",
      "Load results for dataset adult, iteration=1\n",
      "Fold no. 2\n",
      "Load results for dataset adult, iteration=2\n",
      "Fold no. 3\n",
      "Load results for dataset adult, iteration=3\n",
      "Fold no. 4\n",
      "Load results for dataset adult, iteration=4\n",
      "Start training procedure for KDDCup09_upselling\n",
      "Fold no. 0\n",
      "Load results for dataset KDDCup09_upselling, iteration=0\n",
      "Fold no. 1\n",
      "Load results for dataset KDDCup09_upselling, iteration=1\n",
      "Fold no. 2\n",
      "Load results for dataset KDDCup09_upselling, iteration=2\n",
      "Fold no. 3\n",
      "Load results for dataset KDDCup09_upselling, iteration=3\n",
      "Fold no. 4\n",
      "Load results for dataset KDDCup09_upselling, iteration=4\n",
      "Start training procedure for kick\n",
      "Fold no. 0\n",
      "Load results for dataset kick, iteration=0\n",
      "Fold no. 1\n",
      "Load results for dataset kick, iteration=1\n",
      "Fold no. 2\n",
      "Load results for dataset kick, iteration=2\n",
      "Fold no. 3\n",
      "Load results for dataset kick, iteration=3\n",
      "Fold no. 4\n",
      "Load results for dataset kick, iteration=4\n",
      "Start training procedure for open_payments\n",
      "Fold no. 0\n",
      "Load results for dataset open_payments, iteration=0\n",
      "Fold no. 1\n",
      "Load results for dataset open_payments, iteration=1\n",
      "Fold no. 2\n",
      "Load results for dataset open_payments, iteration=2\n",
      "Fold no. 3\n",
      "Load results for dataset open_payments, iteration=3\n",
      "Fold no. 4\n",
      "Load results for dataset open_payments, iteration=4\n",
      "Start training procedure for road-safety-drivers-sex\n",
      "Fold no. 0\n",
      "Load results for dataset road-safety-drivers-sex, iteration=0\n",
      "Fold no. 1\n",
      "Load results for dataset road-safety-drivers-sex, iteration=1\n",
      "Fold no. 2\n",
      "Load results for dataset road-safety-drivers-sex, iteration=2\n",
      "Fold no. 3\n",
      "Load results for dataset road-safety-drivers-sex, iteration=3\n",
      "Fold no. 4\n",
      "Load results for dataset road-safety-drivers-sex, iteration=4\n",
      "Start training procedure for porto-seguro\n",
      "Fold no. 0\n",
      "Load results for dataset porto-seguro, iteration=0\n",
      "Fold no. 1\n",
      "Load results for dataset porto-seguro, iteration=1\n",
      "Fold no. 2\n",
      "Load results for dataset porto-seguro, iteration=2\n",
      "Fold no. 3\n",
      "Random seed set as 555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-18 13:59:19.845780: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Start sampling for epoch 1 of training\n",
      "Epoch 1/500\n",
      "698/698 [==============================] - 16s 18ms/step - me_loss: 0.1913 - me_loss_val: 0.1939 - fe_loss: 0.1664 - fe_loss_val: 0.1675 - me_auc: 0.5408 - me_auc_val: 0.5229 - fe_auc: 0.6216 - fe_auc_val: 0.6077 - stds: 0.9566 - acceptance_rate: 0.0000e+00\n",
      "Adapt step size to 0.05000000074505806\n",
      "\n",
      " Start sampling for epoch 2 of training\n",
      "Epoch 2/500\n",
      "698/698 [==============================] - 11s 16ms/step - me_loss: 0.1878 - me_loss_val: 0.1916 - fe_loss: 0.1668 - fe_loss_val: 0.1688 - me_auc: 0.5574 - me_auc_val: 0.5360 - fe_auc: 0.6328 - fe_auc_val: 0.6111 - stds: 0.9566 - acceptance_rate: 0.0000e+00\n",
      "Adapt step size to 0.02500000037252903\n",
      "\n",
      " Start sampling for epoch 3 of training\n",
      "Epoch 3/500\n",
      "698/698 [==============================] - 12s 17ms/step - me_loss: 0.1876 - me_loss_val: 0.1924 - fe_loss: 0.1642 - fe_loss_val: 0.1673 - me_auc: 0.5612 - me_auc_val: 0.5333 - fe_auc: 0.6419 - fe_auc_val: 0.6121 - stds: 0.9566 - acceptance_rate: 0.0000e+00\n",
      "Adapt step size to 0.012500000186264515\n",
      "\n",
      " Start sampling for epoch 4 of training\n",
      "Epoch 4/500\n",
      "698/698 [==============================] - 12s 17ms/step - me_loss: 0.1823 - me_loss_val: 0.1881 - fe_loss: 0.1682 - fe_loss_val: 0.1724 - me_auc: 0.5703 - me_auc_val: 0.5380 - fe_auc: 0.6505 - fe_auc_val: 0.6128 - stds: 0.9566 - acceptance_rate: 0.0000e+00\n",
      "Adapt step size to 0.0062500000931322575\n",
      "\n",
      " Start sampling for epoch 5 of training\n",
      "Epoch 5/500\n",
      "698/698 [==============================] - 12s 17ms/step - me_loss: 0.1885 - me_loss_val: 0.1957 - fe_loss: 0.1598 - fe_loss_val: 0.1654 - me_auc: 0.5784 - me_auc_val: 0.5392 - fe_auc: 0.6604 - fe_auc_val: 0.6127 - stds: 0.9566 - acceptance_rate: 0.0000e+00\n",
      "Adapt step size to 0.0031250000465661287\n",
      "\n",
      " Start sampling for epoch 6 of training\n",
      "Epoch 6/500\n",
      "698/698 [==============================] - 12s 17ms/step - me_loss: 0.1809 - me_loss_val: 0.1903 - fe_loss: 0.1624 - fe_loss_val: 0.1698 - me_auc: 0.5884 - me_auc_val: 0.5395 - fe_auc: 0.6692 - fe_auc_val: 0.6118 - stds: 0.9566 - acceptance_rate: 0.0000e+00\n",
      "Adapt step size to 0.0015625000232830644\n",
      "\n",
      " Start sampling for epoch 7 of training\n",
      "Epoch 7/500\n",
      "698/698 [==============================] - 12s 17ms/step - me_loss: 0.1775 - me_loss_val: 0.1902 - fe_loss: 0.1624 - fe_loss_val: 0.1730 - me_auc: 0.6008 - me_auc_val: 0.5384 - fe_auc: 0.6821 - fe_auc_val: 0.6049 - stds: 0.9566 - acceptance_rate: 0.0000e+00\n",
      "Adapt step size to 0.0007812500116415322\n",
      "\n",
      " Start sampling for epoch 8 of training\n",
      "Epoch 8/500\n",
      "698/698 [==============================] - 12s 17ms/step - me_loss: 0.1781 - me_loss_val: 0.1936 - fe_loss: 0.1584 - fe_loss_val: 0.1714 - me_auc: 0.6049 - me_auc_val: 0.5283 - fe_auc: 0.7008 - fe_auc_val: 0.6002 - stds: 0.9566 - acceptance_rate: 0.0000e+00\n",
      "Adapt step size to 0.0003906250058207661\n",
      "\n",
      " Start sampling for epoch 9 of training\n",
      "Epoch 9/500\n",
      "698/698 [==============================] - 12s 17ms/step - me_loss: 0.1722 - me_loss_val: 0.1906 - fe_loss: 0.1585 - fe_loss_val: 0.1743 - me_auc: 0.6231 - me_auc_val: 0.5338 - fe_auc: 0.7132 - fe_auc_val: 0.6007 - stds: 0.9566 - acceptance_rate: 0.0000e+00\n",
      "Adapt step size to 0.00019531250291038305\n",
      "\n",
      " Start sampling for epoch 10 of training\n",
      "Epoch 10/500\n",
      "698/698 [==============================] - 12s 17ms/step - me_loss: 0.1689 - me_loss_val: 0.1903 - fe_loss: 0.1530 - fe_loss_val: 0.1716 - me_auc: 0.6440 - me_auc_val: 0.5420 - fe_auc: 0.7285 - fe_auc_val: 0.6037 - stds: 0.9176 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 11 of training\n",
      "Epoch 11/500\n",
      "698/698 [==============================] - 12s 17ms/step - me_loss: 0.1621 - me_loss_val: 0.1872 - fe_loss: 0.1497 - fe_loss_val: 0.1721 - me_auc: 0.6604 - me_auc_val: 0.5379 - fe_auc: 0.7470 - fe_auc_val: 0.5940 - stds: 0.8856 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 12 of training\n",
      "Epoch 12/500\n",
      "698/698 [==============================] - 12s 17ms/step - me_loss: 0.1554 - me_loss_val: 0.1837 - fe_loss: 0.1483 - fe_loss_val: 0.1741 - me_auc: 0.6842 - me_auc_val: 0.5440 - fe_auc: 0.7590 - fe_auc_val: 0.5940 - stds: 0.8607 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 13 of training\n",
      "Epoch 13/500\n",
      "698/698 [==============================] - 12s 17ms/step - me_loss: 0.1511 - me_loss_val: 0.1847 - fe_loss: 0.1449 - fe_loss_val: 0.1756 - me_auc: 0.7085 - me_auc_val: 0.5487 - fe_auc: 0.7713 - fe_auc_val: 0.5917 - stds: 0.8411 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 14 of training\n",
      "Epoch 14/500\n",
      "698/698 [==============================] - 12s 17ms/step - me_loss: 0.1458 - me_loss_val: 0.1818 - fe_loss: 0.1475 - fe_loss_val: 0.1811 - me_auc: 0.7254 - me_auc_val: 0.5467 - fe_auc: 0.7854 - fe_auc_val: 0.5831 - stds: 0.8183 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 15 of training\n",
      "Epoch 15/500\n",
      "698/698 [==============================] - 12s 17ms/step - me_loss: 0.1403 - me_loss_val: 0.1816 - fe_loss: 0.1396 - fe_loss_val: 0.1784 - me_auc: 0.7513 - me_auc_val: 0.5588 - fe_auc: 0.7960 - fe_auc_val: 0.5877 - stds: 0.8106 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 16 of training\n",
      "Epoch 16/500\n",
      "698/698 [==============================] - 12s 17ms/step - me_loss: 0.1374 - me_loss_val: 0.1830 - fe_loss: 0.1405 - fe_loss_val: 0.1839 - me_auc: 0.7654 - me_auc_val: 0.5615 - fe_auc: 0.8014 - fe_auc_val: 0.5881 - stds: 0.7902 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 17 of training\n",
      "Epoch 17/500\n",
      "698/698 [==============================] - 12s 17ms/step - me_loss: 0.1338 - me_loss_val: 0.1836 - fe_loss: 0.1360 - fe_loss_val: 0.1834 - me_auc: 0.7802 - me_auc_val: 0.5549 - fe_auc: 0.8202 - fe_auc_val: 0.5834 - stds: 0.7798 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 18 of training\n",
      "Epoch 18/500\n",
      "698/698 [==============================] - 12s 17ms/step - me_loss: 0.1294 - me_loss_val: 0.1825 - fe_loss: 0.1331 - fe_loss_val: 0.1841 - me_auc: 0.7986 - me_auc_val: 0.5587 - fe_auc: 0.8321 - fe_auc_val: 0.5829 - stds: 0.7718 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 19 of training\n",
      "Epoch 19/500\n",
      "698/698 [==============================] - 12s 17ms/step - me_loss: 0.1258 - me_loss_val: 0.1846 - fe_loss: 0.1309 - fe_loss_val: 0.1877 - me_auc: 0.8138 - me_auc_val: 0.5563 - fe_auc: 0.8402 - fe_auc_val: 0.5781 - stds: 0.7630 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 20 of training\n",
      "Epoch 20/500\n",
      "698/698 [==============================] - 12s 17ms/step - me_loss: 0.1235 - me_loss_val: 0.1862 - fe_loss: 0.1283 - fe_loss_val: 0.1888 - me_auc: 0.8231 - me_auc_val: 0.5623 - fe_auc: 0.8457 - fe_auc_val: 0.5802 - stds: 0.7532 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 21 of training\n",
      "Epoch 21/500\n",
      "698/698 [==============================] - 12s 17ms/step - me_loss: 0.1195 - me_loss_val: 0.1867 - fe_loss: 0.1224 - fe_loss_val: 0.1871 - me_auc: 0.8369 - me_auc_val: 0.5555 - fe_auc: 0.8583 - fe_auc_val: 0.5764 - stds: 0.7472 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 22 of training\n",
      "Epoch 22/500\n",
      "698/698 [==============================] - 12s 18ms/step - me_loss: 0.1152 - me_loss_val: 0.1870 - fe_loss: 0.1198 - fe_loss_val: 0.1897 - me_auc: 0.8524 - me_auc_val: 0.5480 - fe_auc: 0.8757 - fe_auc_val: 0.5672 - stds: 0.7362 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 23 of training\n",
      "Epoch 23/500\n",
      "698/698 [==============================] - 12s 17ms/step - me_loss: 0.1140 - me_loss_val: 0.1886 - fe_loss: 0.1213 - fe_loss_val: 0.1940 - me_auc: 0.8570 - me_auc_val: 0.5527 - fe_auc: 0.8730 - fe_auc_val: 0.5700 - stds: 0.7247 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 24 of training\n",
      "Epoch 24/500\n",
      "698/698 [==============================] - ETA: 0s\n",
      " Early stopping of FE by fe_auc_val at 24 epochs\n",
      "698/698 [==============================] - 12s 17ms/step - me_loss: 0.1101 - me_loss_val: 0.1908 - fe_loss: 0.1164 - fe_loss_val: 0.1952 - me_auc: 0.8681 - me_auc_val: 0.5519 - fe_auc: 0.8800 - fe_auc_val: 0.5688 - stds: 0.7192 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 25 of training\n",
      "Epoch 25/500\n",
      "698/698 [==============================] - 4s 5ms/step - me_loss: 0.1098 - me_loss_val: 0.1904 - fe_loss: 0.1164 - fe_loss_val: 0.1952 - me_auc: 0.8694 - me_auc_val: 0.5523 - fe_auc: 0.8800 - fe_auc_val: 0.5688 - stds: 0.7093 - acceptance_rate: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Start sampling for epoch 26 of training\n",
      "Epoch 26/500\n",
      "698/698 [==============================] - 4s 5ms/step - me_loss: 0.1095 - me_loss_val: 0.1900 - fe_loss: 0.1164 - fe_loss_val: 0.1952 - me_auc: 0.8707 - me_auc_val: 0.5525 - fe_auc: 0.8800 - fe_auc_val: 0.5688 - stds: 0.7022 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 27 of training\n",
      "Epoch 27/500\n",
      "698/698 [==============================] - 4s 5ms/step - me_loss: 0.1094 - me_loss_val: 0.1899 - fe_loss: 0.1164 - fe_loss_val: 0.1952 - me_auc: 0.8712 - me_auc_val: 0.5525 - fe_auc: 0.8800 - fe_auc_val: 0.5688 - stds: 0.6959 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 28 of training\n",
      "Epoch 28/500\n",
      "698/698 [==============================] - 4s 5ms/step - me_loss: 0.1091 - me_loss_val: 0.1895 - fe_loss: 0.1164 - fe_loss_val: 0.1952 - me_auc: 0.8724 - me_auc_val: 0.5529 - fe_auc: 0.8800 - fe_auc_val: 0.5688 - stds: 0.6872 - acceptance_rate: 0.7973\n",
      "\n",
      " Start sampling for epoch 29 of training\n",
      "Epoch 29/500\n",
      "698/698 [==============================] - 4s 5ms/step - me_loss: 0.1089 - me_loss_val: 0.1892 - fe_loss: 0.1164 - fe_loss_val: 0.1952 - me_auc: 0.8735 - me_auc_val: 0.5532 - fe_auc: 0.8800 - fe_auc_val: 0.5688 - stds: 0.6789 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 30 of training\n",
      "Epoch 30/500\n",
      "698/698 [==============================] - 3s 5ms/step - me_loss: 0.1088 - me_loss_val: 0.1891 - fe_loss: 0.1164 - fe_loss_val: 0.1952 - me_auc: 0.8740 - me_auc_val: 0.5533 - fe_auc: 0.8800 - fe_auc_val: 0.5688 - stds: 0.6728 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 31 of training\n",
      "Epoch 31/500\n",
      "698/698 [==============================] - 4s 5ms/step - me_loss: 0.1085 - me_loss_val: 0.1888 - fe_loss: 0.1164 - fe_loss_val: 0.1952 - me_auc: 0.8749 - me_auc_val: 0.5537 - fe_auc: 0.8800 - fe_auc_val: 0.5688 - stds: 0.6718 - acceptance_rate: 0.3184\n",
      "\n",
      " Start sampling for epoch 32 of training\n",
      "Epoch 32/500\n",
      "698/698 [==============================] - 2s 4ms/step - me_loss: 0.1083 - me_loss_val: 0.1885 - fe_loss: 0.1164 - fe_loss_val: 0.1952 - me_auc: 0.8758 - me_auc_val: 0.5539 - fe_auc: 0.8800 - fe_auc_val: 0.5688 - stds: 0.6691 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 33 of training\n",
      "Epoch 33/500\n",
      "698/698 [==============================] - 3s 5ms/step - me_loss: 0.1083 - me_loss_val: 0.1884 - fe_loss: 0.1164 - fe_loss_val: 0.1952 - me_auc: 0.8761 - me_auc_val: 0.5537 - fe_auc: 0.8800 - fe_auc_val: 0.5688 - stds: 0.6656 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 34 of training\n",
      "Epoch 34/500\n",
      "698/698 [==============================] - 4s 5ms/step - me_loss: 0.1081 - me_loss_val: 0.1882 - fe_loss: 0.1164 - fe_loss_val: 0.1952 - me_auc: 0.8770 - me_auc_val: 0.5539 - fe_auc: 0.8800 - fe_auc_val: 0.5688 - stds: 0.6626 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 35 of training\n",
      "Epoch 35/500\n",
      "698/698 [==============================] - 3s 5ms/step - me_loss: 0.1079 - me_loss_val: 0.1880 - fe_loss: 0.1164 - fe_loss_val: 0.1952 - me_auc: 0.8777 - me_auc_val: 0.5542 - fe_auc: 0.8800 - fe_auc_val: 0.5688 - stds: 0.6543 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 36 of training\n",
      "Epoch 36/500\n",
      "698/698 [==============================] - 4s 5ms/step - me_loss: 0.1078 - me_loss_val: 0.1879 - fe_loss: 0.1164 - fe_loss_val: 0.1952 - me_auc: 0.8780 - me_auc_val: 0.5543 - fe_auc: 0.8800 - fe_auc_val: 0.5688 - stds: 0.6506 - acceptance_rate: 0.8487\n",
      "\n",
      " Start sampling for epoch 37 of training\n",
      "Epoch 37/500\n",
      "698/698 [==============================] - 3s 5ms/step - me_loss: 0.1077 - me_loss_val: 0.1877 - fe_loss: 0.1164 - fe_loss_val: 0.1952 - me_auc: 0.8786 - me_auc_val: 0.5543 - fe_auc: 0.8800 - fe_auc_val: 0.5688 - stds: 0.6478 - acceptance_rate: 0.4855\n",
      "\n",
      " Start sampling for epoch 38 of training\n",
      "Epoch 38/500\n",
      "698/698 [==============================] - 4s 5ms/step - me_loss: 0.1075 - me_loss_val: 0.1875 - fe_loss: 0.1164 - fe_loss_val: 0.1952 - me_auc: 0.8792 - me_auc_val: 0.5541 - fe_auc: 0.8800 - fe_auc_val: 0.5688 - stds: 0.6421 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 39 of training\n",
      "Epoch 39/500\n",
      "698/698 [==============================] - 4s 5ms/step - me_loss: 0.1074 - me_loss_val: 0.1874 - fe_loss: 0.1164 - fe_loss_val: 0.1952 - me_auc: 0.8799 - me_auc_val: 0.5544 - fe_auc: 0.8800 - fe_auc_val: 0.5688 - stds: 0.6357 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 40 of training\n",
      "Epoch 40/500\n",
      "698/698 [==============================] - 3s 5ms/step - me_loss: 0.1074 - me_loss_val: 0.1873 - fe_loss: 0.1164 - fe_loss_val: 0.1952 - me_auc: 0.8801 - me_auc_val: 0.5543 - fe_auc: 0.8800 - fe_auc_val: 0.5688 - stds: 0.6296 - acceptance_rate: 0.5879\n",
      "Fold no. 4\n",
      "Random seed set as 555\n",
      "\n",
      " Start sampling for epoch 1 of training\n",
      "Epoch 1/500\n",
      "698/698 [==============================] - 13s 17ms/step - me_loss: 0.1892 - me_loss_val: 0.1895 - fe_loss: 0.1686 - fe_loss_val: 0.1708 - me_auc: 0.5444 - me_auc_val: 0.5444 - fe_auc: 0.6228 - fe_auc_val: 0.6062 - stds: 0.9566 - acceptance_rate: 0.0000e+00\n",
      "Adapt step size to 0.05000000074505806\n",
      "\n",
      " Start sampling for epoch 2 of training\n",
      "Epoch 2/500\n",
      "698/698 [==============================] - 12s 17ms/step - me_loss: 0.1903 - me_loss_val: 0.1913 - fe_loss: 0.1644 - fe_loss_val: 0.1670 - me_auc: 0.5477 - me_auc_val: 0.5462 - fe_auc: 0.6325 - fe_auc_val: 0.6116 - stds: 0.9566 - acceptance_rate: 0.0000e+00\n",
      "Adapt step size to 0.02500000037252903\n",
      "\n",
      " Start sampling for epoch 3 of training\n",
      "Epoch 3/500\n",
      "698/698 [==============================] - 12s 17ms/step - me_loss: 0.1875 - me_loss_val: 0.1898 - fe_loss: 0.1645 - fe_loss_val: 0.1682 - me_auc: 0.5525 - me_auc_val: 0.5449 - fe_auc: 0.6425 - fe_auc_val: 0.6128 - stds: 0.9566 - acceptance_rate: 0.0000e+00\n",
      "Adapt step size to 0.012500000186264515\n",
      "\n",
      " Start sampling for epoch 4 of training\n",
      "Epoch 4/500\n",
      "698/698 [==============================] - 12s 18ms/step - me_loss: 0.1841 - me_loss_val: 0.1881 - fe_loss: 0.1655 - fe_loss_val: 0.1708 - me_auc: 0.5673 - me_auc_val: 0.5512 - fe_auc: 0.6505 - fe_auc_val: 0.6124 - stds: 0.9566 - acceptance_rate: 0.0000e+00\n",
      "Adapt step size to 0.0062500000931322575\n",
      "\n",
      " Start sampling for epoch 5 of training\n",
      "Epoch 5/500\n",
      "698/698 [==============================] - 12s 17ms/step - me_loss: 0.1854 - me_loss_val: 0.1916 - fe_loss: 0.1609 - fe_loss_val: 0.1680 - me_auc: 0.5753 - me_auc_val: 0.5490 - fe_auc: 0.6672 - fe_auc_val: 0.6113 - stds: 0.9566 - acceptance_rate: 0.0000e+00\n",
      "Adapt step size to 0.0031250000465661287\n",
      "\n",
      " Start sampling for epoch 6 of training\n",
      "Epoch 6/500\n",
      "698/698 [==============================] - 12s 17ms/step - me_loss: 0.1840 - me_loss_val: 0.1927 - fe_loss: 0.1591 - fe_loss_val: 0.1687 - me_auc: 0.5839 - me_auc_val: 0.5450 - fe_auc: 0.6800 - fe_auc_val: 0.6054 - stds: 0.9566 - acceptance_rate: 0.0000e+00\n",
      "Adapt step size to 0.0015625000232830644\n",
      "\n",
      " Start sampling for epoch 7 of training\n",
      "Epoch 7/500\n",
      "698/698 [==============================] - 12s 17ms/step - me_loss: 0.1783 - me_loss_val: 0.1892 - fe_loss: 0.1604 - fe_loss_val: 0.1720 - me_auc: 0.6007 - me_auc_val: 0.5529 - fe_auc: 0.6859 - fe_auc_val: 0.6037 - stds: 0.9566 - acceptance_rate: 0.0000e+00\n",
      "Adapt step size to 0.0007812500116415322\n",
      "\n",
      " Start sampling for epoch 8 of training\n",
      "Epoch 8/500\n",
      "698/698 [==============================] - 12s 17ms/step - me_loss: 0.1791 - me_loss_val: 0.1939 - fe_loss: 0.1555 - fe_loss_val: 0.1707 - me_auc: 0.6093 - me_auc_val: 0.5427 - fe_auc: 0.7107 - fe_auc_val: 0.5995 - stds: 0.9566 - acceptance_rate: 0.0000e+00\n",
      "Adapt step size to 0.0003906250058207661\n",
      "\n",
      " Start sampling for epoch 9 of training\n",
      "Epoch 9/500\n",
      "698/698 [==============================] - 12s 17ms/step - me_loss: 0.1749 - me_loss_val: 0.1912 - fe_loss: 0.1556 - fe_loss_val: 0.1724 - me_auc: 0.6287 - me_auc_val: 0.5580 - fe_auc: 0.7107 - fe_auc_val: 0.6056 - stds: 0.9566 - acceptance_rate: 0.0000e+00\n",
      "Adapt step size to 0.00019531250291038305\n",
      "\n",
      " Start sampling for epoch 10 of training\n",
      "Epoch 10/500\n",
      "698/698 [==============================] - 12s 17ms/step - me_loss: 0.1698 - me_loss_val: 0.1898 - fe_loss: 0.1509 - fe_loss_val: 0.1709 - me_auc: 0.6441 - me_auc_val: 0.5542 - fe_auc: 0.7346 - fe_auc_val: 0.6016 - stds: 0.9147 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 11 of training\n",
      "Epoch 11/500\n",
      "698/698 [==============================] - 12s 17ms/step - me_loss: 0.1619 - me_loss_val: 0.1851 - fe_loss: 0.1502 - fe_loss_val: 0.1737 - me_auc: 0.6588 - me_auc_val: 0.5487 - fe_auc: 0.7547 - fe_auc_val: 0.5917 - stds: 0.8834 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 12 of training\n",
      "Epoch 12/500\n",
      "698/698 [==============================] - 12s 17ms/step - me_loss: 0.1552 - me_loss_val: 0.1824 - fe_loss: 0.1481 - fe_loss_val: 0.1756 - me_auc: 0.6870 - me_auc_val: 0.5600 - fe_auc: 0.7581 - fe_auc_val: 0.5934 - stds: 0.8600 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 13 of training\n",
      "Epoch 13/500\n",
      "698/698 [==============================] - 12s 17ms/step - me_loss: 0.1504 - me_loss_val: 0.1817 - fe_loss: 0.1450 - fe_loss_val: 0.1767 - me_auc: 0.7080 - me_auc_val: 0.5583 - fe_auc: 0.7756 - fe_auc_val: 0.5892 - stds: 0.8406 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 14 of training\n",
      "Epoch 14/500\n",
      "698/698 [==============================] - 12s 17ms/step - me_loss: 0.1458 - me_loss_val: 0.1819 - fe_loss: 0.1461 - fe_loss_val: 0.1825 - me_auc: 0.7322 - me_auc_val: 0.5627 - fe_auc: 0.7867 - fe_auc_val: 0.5894 - stds: 0.8194 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 15 of training\n",
      "Epoch 15/500\n",
      "698/698 [==============================] - 12s 17ms/step - me_loss: 0.1409 - me_loss_val: 0.1798 - fe_loss: 0.1414 - fe_loss_val: 0.1804 - me_auc: 0.7500 - me_auc_val: 0.5622 - fe_auc: 0.8068 - fe_auc_val: 0.5878 - stds: 0.8100 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 16 of training\n",
      "Epoch 16/500\n",
      "698/698 [==============================] - 12s 17ms/step - me_loss: 0.1369 - me_loss_val: 0.1822 - fe_loss: 0.1375 - fe_loss_val: 0.1828 - me_auc: 0.7693 - me_auc_val: 0.5654 - fe_auc: 0.8093 - fe_auc_val: 0.5836 - stds: 0.7907 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 17 of training\n",
      "Epoch 17/500\n",
      "698/698 [==============================] - 12s 17ms/step - me_loss: 0.1333 - me_loss_val: 0.1803 - fe_loss: 0.1333 - fe_loss_val: 0.1803 - me_auc: 0.7823 - me_auc_val: 0.5605 - fe_auc: 0.8297 - fe_auc_val: 0.5791 - stds: 0.7764 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 18 of training\n",
      "Epoch 18/500\n",
      "698/698 [==============================] - 12s 17ms/step - me_loss: 0.1293 - me_loss_val: 0.1819 - fe_loss: 0.1317 - fe_loss_val: 0.1842 - me_auc: 0.7995 - me_auc_val: 0.5661 - fe_auc: 0.8342 - fe_auc_val: 0.5826 - stds: 0.7688 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 19 of training\n",
      "Epoch 19/500\n",
      "698/698 [==============================] - 12s 17ms/step - me_loss: 0.1264 - me_loss_val: 0.1824 - fe_loss: 0.1287 - fe_loss_val: 0.1848 - me_auc: 0.8127 - me_auc_val: 0.5582 - fe_auc: 0.8460 - fe_auc_val: 0.5743 - stds: 0.7605 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 20 of training\n",
      "Epoch 20/500\n",
      "698/698 [==============================] - 12s 17ms/step - me_loss: 0.1230 - me_loss_val: 0.1835 - fe_loss: 0.1272 - fe_loss_val: 0.1880 - me_auc: 0.8269 - me_auc_val: 0.5631 - fe_auc: 0.8543 - fe_auc_val: 0.5765 - stds: 0.7485 - acceptance_rate: 0.4284\n",
      "\n",
      " Start sampling for epoch 21 of training\n",
      "Epoch 21/500\n",
      "698/698 [==============================] - 12s 17ms/step - me_loss: 0.1191 - me_loss_val: 0.1858 - fe_loss: 0.1232 - fe_loss_val: 0.1900 - me_auc: 0.8402 - me_auc_val: 0.5579 - fe_auc: 0.8678 - fe_auc_val: 0.5715 - stds: 0.7435 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 22 of training\n",
      "Epoch 22/500\n",
      "698/698 [==============================] - 12s 17ms/step - me_loss: 0.1157 - me_loss_val: 0.1864 - fe_loss: 0.1218 - fe_loss_val: 0.1927 - me_auc: 0.8539 - me_auc_val: 0.5543 - fe_auc: 0.8792 - fe_auc_val: 0.5700 - stds: 0.7335 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 23 of training\n",
      "Epoch 23/500\n",
      "698/698 [==============================] - ETA: 0s\n",
      " Early stopping of FE by fe_auc_val at 23 epochs\n",
      "698/698 [==============================] - 12s 17ms/step - me_loss: 0.1135 - me_loss_val: 0.1886 - fe_loss: 0.1218 - fe_loss_val: 0.1971 - me_auc: 0.8610 - me_auc_val: 0.5555 - fe_auc: 0.8797 - fe_auc_val: 0.5693 - stds: 0.7232 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 24 of training\n",
      "Epoch 24/500\n",
      "698/698 [==============================] - 4s 5ms/step - me_loss: 0.1131 - me_loss_val: 0.1881 - fe_loss: 0.1218 - fe_loss_val: 0.1971 - me_auc: 0.8626 - me_auc_val: 0.5563 - fe_auc: 0.8797 - fe_auc_val: 0.5693 - stds: 0.7180 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 25 of training\n",
      "Epoch 25/500\n",
      "698/698 [==============================] - 3s 5ms/step - me_loss: 0.1127 - me_loss_val: 0.1876 - fe_loss: 0.1218 - fe_loss_val: 0.1971 - me_auc: 0.8642 - me_auc_val: 0.5565 - fe_auc: 0.8797 - fe_auc_val: 0.5693 - stds: 0.7094 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 26 of training\n",
      "Epoch 26/500\n",
      "698/698 [==============================] - 2s 3ms/step - me_loss: 0.1122 - me_loss_val: 0.1872 - fe_loss: 0.1218 - fe_loss_val: 0.1971 - me_auc: 0.8658 - me_auc_val: 0.5565 - fe_auc: 0.8797 - fe_auc_val: 0.5693 - stds: 0.7025 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 27 of training\n",
      "Epoch 27/500\n",
      "698/698 [==============================] - 4s 5ms/step - me_loss: 0.1121 - me_loss_val: 0.1870 - fe_loss: 0.1218 - fe_loss_val: 0.1971 - me_auc: 0.8664 - me_auc_val: 0.5563 - fe_auc: 0.8797 - fe_auc_val: 0.5693 - stds: 0.6966 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 28 of training\n",
      "Epoch 28/500\n",
      "698/698 [==============================] - 3s 5ms/step - me_loss: 0.1117 - me_loss_val: 0.1866 - fe_loss: 0.1218 - fe_loss_val: 0.1971 - me_auc: 0.8677 - me_auc_val: 0.5560 - fe_auc: 0.8797 - fe_auc_val: 0.5693 - stds: 0.6883 - acceptance_rate: 0.8929\n",
      "\n",
      " Start sampling for epoch 29 of training\n",
      "Epoch 29/500\n",
      "698/698 [==============================] - 4s 5ms/step - me_loss: 0.1113 - me_loss_val: 0.1862 - fe_loss: 0.1218 - fe_loss_val: 0.1971 - me_auc: 0.8689 - me_auc_val: 0.5561 - fe_auc: 0.8797 - fe_auc_val: 0.5693 - stds: 0.6805 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 30 of training\n",
      "Epoch 30/500\n",
      "698/698 [==============================] - 3s 5ms/step - me_loss: 0.1112 - me_loss_val: 0.1861 - fe_loss: 0.1218 - fe_loss_val: 0.1971 - me_auc: 0.8693 - me_auc_val: 0.5565 - fe_auc: 0.8797 - fe_auc_val: 0.5693 - stds: 0.6746 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 31 of training\n",
      "Epoch 31/500\n",
      "698/698 [==============================] - 2s 3ms/step - me_loss: 0.1109 - me_loss_val: 0.1857 - fe_loss: 0.1218 - fe_loss_val: 0.1971 - me_auc: 0.8702 - me_auc_val: 0.5573 - fe_auc: 0.8797 - fe_auc_val: 0.5693 - stds: 0.6735 - acceptance_rate: 0.2766\n",
      "\n",
      " Start sampling for epoch 32 of training\n",
      "Epoch 32/500\n",
      "698/698 [==============================] - 2s 3ms/step - me_loss: 0.1107 - me_loss_val: 0.1854 - fe_loss: 0.1218 - fe_loss_val: 0.1971 - me_auc: 0.8713 - me_auc_val: 0.5580 - fe_auc: 0.8797 - fe_auc_val: 0.5693 - stds: 0.6709 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 33 of training\n",
      "Epoch 33/500\n",
      "698/698 [==============================] - 2s 3ms/step - me_loss: 0.1106 - me_loss_val: 0.1853 - fe_loss: 0.1218 - fe_loss_val: 0.1971 - me_auc: 0.8718 - me_auc_val: 0.5583 - fe_auc: 0.8797 - fe_auc_val: 0.5693 - stds: 0.6675 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 34 of training\n",
      "Epoch 34/500\n",
      "698/698 [==============================] - 3s 4ms/step - me_loss: 0.1104 - me_loss_val: 0.1851 - fe_loss: 0.1218 - fe_loss_val: 0.1971 - me_auc: 0.8728 - me_auc_val: 0.5577 - fe_auc: 0.8797 - fe_auc_val: 0.5693 - stds: 0.6649 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 35 of training\n",
      "Epoch 35/500\n",
      "698/698 [==============================] - 4s 5ms/step - me_loss: 0.1102 - me_loss_val: 0.1849 - fe_loss: 0.1218 - fe_loss_val: 0.1971 - me_auc: 0.8736 - me_auc_val: 0.5582 - fe_auc: 0.8797 - fe_auc_val: 0.5693 - stds: 0.6569 - acceptance_rate: 1.0000\n",
      "\n",
      " Start sampling for epoch 36 of training\n",
      "Epoch 36/500\n",
      "698/698 [==============================] - 3s 5ms/step - me_loss: 0.1101 - me_loss_val: 0.1848 - fe_loss: 0.1218 - fe_loss_val: 0.1971 - me_auc: 0.8739 - me_auc_val: 0.5583 - fe_auc: 0.8797 - fe_auc_val: 0.5693 - stds: 0.6536 - acceptance_rate: 0.9105\n",
      "\n",
      " Start sampling for epoch 37 of training\n",
      "Epoch 37/500\n",
      "698/698 [==============================] - 3s 5ms/step - me_loss: 0.1099 - me_loss_val: 0.1846 - fe_loss: 0.1218 - fe_loss_val: 0.1971 - me_auc: 0.8747 - me_auc_val: 0.5587 - fe_auc: 0.8797 - fe_auc_val: 0.5693 - stds: 0.6506 - acceptance_rate: 0.4742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Start sampling for epoch 38 of training\n",
      "Epoch 38/500\n",
      "698/698 [==============================] - 3s 5ms/step - me_loss: 0.1098 - me_loss_val: 0.1844 - fe_loss: 0.1218 - fe_loss_val: 0.1971 - me_auc: 0.8753 - me_auc_val: 0.5590 - fe_auc: 0.8797 - fe_auc_val: 0.5693 - stds: 0.6448 - acceptance_rate: 1.0000\n"
     ]
    }
   ],
   "source": [
    "mode=\"cv\"\n",
    "hct=10\n",
    "test_ratio=None\n",
    "val_ratio=None\n",
    "folds=5\n",
    "results = {}\n",
    "dataset_names = [\"churn\", \"kdd_internet_usage\", \"Amazon_employee_access\", \"Click_prediction_small\", \"adult\", \"KDDCup09_upselling\", \"kick\", \"open_payments\", \"road-safety-drivers-sex\", \"porto-seguro\"]\n",
    "\n",
    "\n",
    "loss_use = lambda: tf.keras.losses.BinaryCrossentropy\n",
    "\n",
    "target= \"binary\"\n",
    "batch_size=512\n",
    "epochs = 500\n",
    "early_stopping = 20\n",
    "model_name = \"AutoGluon\"\n",
    "embed_dims_method = \"AutoGluon\"\n",
    "\n",
    "\n",
    "results = {}\n",
    "\n",
    "#######################################\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    print(f\"Start training procedure for {dataset_name}\")\n",
    "    data_path = f\"{mode}_RS{RS}_hct{hct}\"\n",
    "    if mode == \"cv\":\n",
    "        data_path += f\"_{folds}folds\"\n",
    "    elif mode == \"train_test\":\n",
    "        data_path += f\"_split{1-test_ratio*100}-{test_ratio*100}\"\n",
    "    elif mode == \"train_val_test\":\n",
    "        data_path += f\"_split{round(100-(test_ratio+val_ratio)*100)}-{round(test_ratio*100)}-{round(val_ratio*100)}\"\n",
    "\n",
    "    # If no data_dict exists, run preprocessing, else load data_dict\n",
    "    if not os.path.exists(f\"../data/prepared/{dataset_name}/\"+data_path+\"/data_dict.pickle\"):\n",
    "        dataset_preprocessing.process_dataset(dataset_name, target, mode, RS, hct, test_ratio, val_ratio, folds)\n",
    "    with open(f\"../data/prepared/{dataset_name}/{data_path}/data_dict.pickle\", 'rb') as handle:\n",
    "            data_dict = pickle.load(handle)\n",
    "\n",
    "    z_cols = data_dict[\"z_cols\"]\n",
    "    results[dataset_name] = {}\n",
    "    for fold_num in range(folds):\n",
    "        results[dataset_name][fold_num] = {}\n",
    "\n",
    "        print(f\"Fold no. {fold_num}\")\n",
    "        save_path = f\"../results/{dataset_name}/{data_path}/fold_{fold_num}/MALA\"\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        \n",
    "        z_ohe_encoded_train = data_dict[f\"z_ohe_encoded_train_{fold_num}\"] \n",
    "        z_ohe_encoded_val = data_dict[f\"z_ohe_encoded_val_{fold_num}\"] \n",
    "        z_ohe_encoded_test = data_dict[f\"z_ohe_encoded_test_{fold_num}\"] \n",
    "\n",
    "        z_target_encoded_train = data_dict[f\"z_target_encoded_train_{fold_num}\"] \n",
    "        z_target_encoded_val = data_dict[f\"z_target_encoded_val_{fold_num}\"] \n",
    "        z_target_encoded_test = data_dict[f\"z_target_encoded_test_{fold_num}\"] \n",
    "        \n",
    "        target_encoding_time = data_dict[f\"target_encoding_time_{fold_num}\"]\n",
    "        ohe_encoding_time = data_dict[f\"ohe_encoding_time_{fold_num}\"]\n",
    "        \n",
    "        x_cols = data_dict[f\"X_train_{fold_num}\"].columns\n",
    "        X_train = data_dict[f\"X_train_{fold_num}\"]\n",
    "        Z_train = data_dict[f\"Z_train_{fold_num}\"]\n",
    "        y_train = data_dict[f\"y_train_{fold_num}\"]\n",
    "\n",
    "        X_val = data_dict[f\"X_val_{fold_num}\"]\n",
    "        Z_val = data_dict[f\"Z_val_{fold_num}\"]\n",
    "        y_val = data_dict[f\"y_val_{fold_num}\"]\n",
    "\n",
    "        X_test = data_dict[f\"X_test_{fold_num}\"]\n",
    "        Z_test = data_dict[f\"Z_test_{fold_num}\"]\n",
    "        y_test = data_dict[f\"y_test_{fold_num}\"]\n",
    "    \n",
    "        if not os.path.exists(f\"{save_path}/results_RS{RS}_{dataset_name}_iter{fold_num}.pickle\"):\n",
    "\n",
    "            tf.random.set_seed(RS+fold_num)\n",
    "            np.random.seed(RS+fold_num)\n",
    "\n",
    "            qs = np.max([tf.reduce_max(Z_train, axis=0),tf.reduce_max(Z_val, axis=0),tf.reduce_max(Z_test, axis=0)],axis=0)+1\n",
    "            \n",
    "            X_train = tf.convert_to_tensor(X_train)\n",
    "            Z_train = tf.convert_to_tensor(Z_train,dtype=tf.int32)\n",
    "            y_train = tf.convert_to_tensor(y_train)\n",
    "\n",
    "            X_val = tf.convert_to_tensor(X_val)\n",
    "            Z_val = tf.convert_to_tensor(Z_val,dtype=tf.int32)\n",
    "            y_val = tf.convert_to_tensor(y_val)\n",
    "\n",
    "            X_test = tf.convert_to_tensor(X_test)\n",
    "            Z_test = tf.convert_to_tensor(Z_test,dtype=tf.int32)\n",
    "            y_test = tf.convert_to_tensor(y_test)\n",
    "\n",
    "            if target == \"categorical\":\n",
    "                n_classes = np.unique(y_train).shape[0]\n",
    "            elif target==\"binary\":\n",
    "                n_classes = 1\n",
    "            \n",
    "            y_train = tf.one_hot(tf.cast(y_train,tf.int32),n_classes)\n",
    "            y_val = tf.one_hot(tf.cast(y_val,tf.int32),n_classes)\n",
    "            y_test = tf.one_hot(tf.cast(y_test,tf.int32),n_classes)\n",
    "            \n",
    "            ##### GMENN #####\n",
    "            d = X_train.shape[1] # columns\n",
    "            n = X_train.shape[0] # rows\n",
    "            num_outputs = n_classes\n",
    "            perc_numeric = d/(d+Z_train.shape[1])\n",
    "\n",
    "#             qs = np.max([tf.reduce_max(Z_train, axis=0),tf.reduce_max(Z_val, axis=0),tf.reduce_max(Z_test, axis=0)],axis=0)+1\n",
    "\n",
    "            set_seed(RS)\n",
    "\n",
    "            fe_model, optimizer = get_model(model_name=model_name, input_size=X_train.shape[1], \n",
    "                                              output_size=num_outputs, \n",
    "                                              target=target, \n",
    "                                              perc_numeric=perc_numeric, RS=RS)\n",
    "\n",
    "            initial_stds = np.ones([len(qs),num_outputs]).astype(float).tolist()\n",
    "\n",
    "            me_model = MixedEffectsNetwork(X_train, Z_train, y_train, fe_model, \n",
    "                                           target=target, qs=qs,\n",
    "                                           initial_stds=initial_stds,\n",
    "                                          fe_loss_weight=1.,\n",
    "                                           mode=\"intercepts\",\n",
    "                                           early_stopping_fe=early_stopping,\n",
    "                                          )    \n",
    "\n",
    "            me_model.compile(\n",
    "                loss_class_me = loss_use()(),\n",
    "                loss_class_fe = loss_use()(),\n",
    "            #     metric_class_me = tf.keras.metrics.AUC(multi_label=True, name=\"auc_me\"),\n",
    "            #     metric_class_fe = tf.keras.metrics.AUC(multi_label=True, name=\"auc_fe\"),\n",
    "                optimizer=optimizer\n",
    "            )\n",
    "\n",
    "            mcmc = MCMCSamplingCallback(num_mcmc_samples=1,\n",
    "                                        perc_burnin=0.7,\n",
    "                                        warm_restart=None,\n",
    "                                        num_burnin_steps=1,\n",
    "                                        step_size = 0.1#initial_step_size,\n",
    "                                   )\n",
    "\n",
    "\n",
    "            print_metric = PrintMetrics(X_train, Z_train, y_train, X_val, Z_val, y_val)\n",
    "\n",
    "            start = time.time()\n",
    "            history = me_model.fit([X_train,Z_train], y_train,\n",
    "                         callbacks=[mcmc,\n",
    "                                    print_metric,\n",
    "                                    tf.keras.callbacks.EarlyStopping(monitor=\"me_auc_val\", patience=early_stopping, mode=\"max\")],\n",
    "                         epochs=epochs,\n",
    "                         validation_data=[[X_val,Z_val],y_val],\n",
    "                        batch_size=batch_size)\n",
    "\n",
    "            end = time.time()\n",
    "            fit_time_gmenn = round(end-start,2)\n",
    "\n",
    "            y_train_pred_gmenn, y_train_pred_gmenn_fe = me_model([X_train,Z_train])\n",
    "            y_val_pred_gmenn, y_val_pred_gmenn_fe = me_model([X_val,Z_val])\n",
    "            y_test_pred_gmenn, y_test_pred_gmenn_fe = me_model([X_test,Z_test])    \n",
    "\n",
    "            \n",
    "            ###### Prepare NN Training ######\n",
    "#             metrics_use = []\n",
    "#             if target ==\"binary\":\n",
    "#                 metrics_use.append(tf.keras.metrics.AUC(name=\"auc\"))\n",
    "#                 metrics_use.append(tf.keras.metrics.Accuracy(name=\"accuracy\"))\n",
    "#                 metrics_use.append(F1Score(num_classes=2, average=\"micro\", name=\"f1\"))\n",
    "#                 stop_mode = \"max\"\n",
    "#                 activation_layer = tf.keras.activations.sigmoid\n",
    "#             elif target ==\"categorical\":\n",
    "#                 metrics_use.append(tf.keras.metrics.AUC(multi_label=True, name=\"auc\"))\n",
    "#                 metrics_use.append(tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"))\n",
    "#                 metrics_use.append(F1Score(num_classes=num_outputs, average=\"weighted\", name=\"f1\"))\n",
    "#                 stop_mode = \"max\"\n",
    "#                 activation_layer = tf.keras.activations.softmax\n",
    "#             elif target == \"continuous\":\n",
    "#                 metrics_use.append(RSquare(name=\"r2\"))\n",
    "#                 metrics_use.append(tf.keras.metrics.MeanSquaredError(name=\"mse\"))\n",
    "#                 stop_mode = \"min\"            \n",
    "            \n",
    "#             ##### Ignore #####\n",
    "#             model_nn, optimizer = get_model(model_name=model_name, \n",
    "#                                             input_size=X_train.shape[1], \n",
    "#                                             output_size=num_outputs, \n",
    "#                                             target=target, \n",
    "#                                             perc_numeric=perc_numeric, RS=RS)\n",
    "#             model_nn.build((n,d))\n",
    "#             update_layer_activation(model=model_nn, activation=activation_layer)\n",
    "\n",
    "#             model_nn.compile(loss=loss_use()(), optimizer=optimizer, metrics = metrics_use)\n",
    "\n",
    "#             callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_auc\", patience=early_stopping, mode=stop_mode)\n",
    "\n",
    "#             start = time.time()\n",
    "#             history_nn = model_nn.fit(X_train, y_train,\n",
    "#                          validation_data= [X_val, y_val],\n",
    "#                          epochs=epochs, batch_size=batch_size, callbacks=[callback])\n",
    "#             end = time.time()\n",
    "#             fit_time_nn = round(end-start,2)\n",
    "\n",
    "#             y_train_pred_nn = model_nn.predict(X_train ,batch_size=batch_size)\n",
    "#             y_val_pred_nn = model_nn.predict(X_val ,batch_size=batch_size)\n",
    "#             y_test_pred_nn = model_nn.predict(X_test ,batch_size=batch_size)\n",
    "\n",
    "#             if target == \"binary\":\n",
    "#                 eval_res_train_nn = get_metrics(y_train[:,0], y_train_pred_nn, target=target)\n",
    "#                 eval_res_val_nn = get_metrics(y_val[:,0], y_val_pred_nn, target=target)\n",
    "#                 eval_res_test_nn = get_metrics(y_test[:,0], y_test_pred_nn, target=target)\n",
    "#             elif target == \"categorical\":\n",
    "#                 eval_res_train_nn = get_metrics(y_train, y_train_pred_nn, target=target)\n",
    "#                 eval_res_val_nn = get_metrics(y_val, y_val_pred_nn, target=target)\n",
    "#                 eval_res_test_nn = get_metrics(y_test, y_test_pred_nn, target=target)\n",
    "\n",
    "#             ##### Target Encoding #####\n",
    "#             print(\"\\n Train Target Encoding Network\")\n",
    "#             model_nn_te, optimizer = get_model(model_name=model_name, \n",
    "#                                             input_size=np.append(X_train ,z_target_encoded_train, axis=1).shape[1], \n",
    "#                                             output_size=num_outputs, \n",
    "#                                             target=target, \n",
    "#                                             perc_numeric=perc_numeric, RS=RS)\n",
    "#             model_nn_te.build((n,np.append(X_train ,z_target_encoded_train, axis=1).shape[1]))\n",
    "#             update_layer_activation(model=model_nn_te, activation=activation_layer)\n",
    "#             model_nn_te.compile(loss=loss_use()(), optimizer=optimizer, metrics = metrics_use)\n",
    "#             callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_auc\", patience=early_stopping, mode=stop_mode)\n",
    "\n",
    "#             start = time.time()\n",
    "#             history_nn_te = model_nn_te.fit(np.append(X_train ,z_target_encoded_train, axis=1), y_train,\n",
    "#                          validation_data= [np.append(X_val ,z_target_encoded_val, axis=1), y_val],\n",
    "#                          epochs=epochs, batch_size=batch_size, callbacks=[callback])\n",
    "#             end = time.time()\n",
    "#             fit_time_te = round(end-start,2)+target_encoding_time\n",
    "\n",
    "#             y_train_pred_nn_te = model_nn_te.predict(np.append(X_train ,z_target_encoded_train, axis=1) ,batch_size=batch_size)\n",
    "#             y_val_pred_nn_te = model_nn_te.predict(np.append(X_val ,z_target_encoded_val, axis=1) ,batch_size=batch_size)\n",
    "#             y_test_pred_nn_te = model_nn_te.predict(np.append(X_test ,z_target_encoded_test, axis=1) ,batch_size=batch_size)\n",
    "\n",
    "#             if target == \"binary\":\n",
    "#                 eval_res_train_nn_te = get_metrics(y_train[:,0], y_train_pred_nn_te, target=target)\n",
    "#                 eval_res_val_nn_te = get_metrics(y_val[:,0], y_val_pred_nn_te, target=target)\n",
    "#                 eval_res_test_nn_te = get_metrics(y_test[:,0], y_test_pred_nn_te, target=target)\n",
    "#             elif target == \"categorical\":\n",
    "#                 eval_res_train_nn_te = get_metrics(y_train, y_train_pred_te, target=target)\n",
    "#                 eval_res_val_nn_te = get_metrics(y_val, y_val_pred_te, target=target)\n",
    "#                 eval_res_test_nn_te = get_metrics(y_test, y_test_pred_te, target=target)\n",
    "\n",
    " \n",
    "#             gc.collect()\n",
    "#             ##### OHE #####\n",
    "#             print(\"\\n Train OHE Network\")\n",
    "#             model_nn_ohe, optimizer = get_model(model_name=model_name, \n",
    "#                                             input_size=np.append(X_train ,z_ohe_encoded_train, axis=1).shape[1], \n",
    "#                                             output_size=num_outputs, \n",
    "#                                             target=target, \n",
    "#                                             perc_numeric=perc_numeric, RS=RS)\n",
    "#             model_nn_ohe.build((n,np.append(X_train ,z_ohe_encoded_train, axis=1).shape[1]))\n",
    "#             update_layer_activation(model=model_nn_ohe, activation=activation_layer)\n",
    "#             model_nn_ohe.compile(loss=loss_use()(), optimizer=optimizer, metrics = metrics_use)\n",
    "#             callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_auc\", patience=early_stopping, mode=stop_mode)\n",
    "\n",
    "#             start = time.time()\n",
    "#             history_nn_ohe = model_nn_ohe.fit(np.append(X_train ,z_ohe_encoded_train, axis=1), y_train,\n",
    "#                          validation_data= [np.append(X_val ,z_ohe_encoded_val, axis=1), y_val],\n",
    "#                          epochs=epochs, batch_size=batch_size, callbacks=[callback])\n",
    "#             end = time.time()\n",
    "#             fit_time_ohe = round(end-start,2)+ohe_encoding_time\n",
    "\n",
    "#             y_train_pred_nn_ohe = model_nn_ohe.predict(np.append(X_train ,z_ohe_encoded_train, axis=1), batch_size=batch_size)\n",
    "#             y_val_pred_nn_ohe = model_nn_ohe.predict(np.append(X_val ,z_ohe_encoded_val, axis=1), batch_size=batch_size)\n",
    "#             y_test_pred_nn_ohe = model_nn_ohe.predict(np.append(X_test ,z_ohe_encoded_test, axis=1), batch_size=batch_size)\n",
    "            \n",
    "#             if target == \"binary\":\n",
    "#                 eval_res_train_nn_ohe = get_metrics(y_train[:,0], y_train_pred_nn_ohe, target=target)\n",
    "#                 eval_res_val_nn_ohe = get_metrics(y_val[:,0], y_val_pred_nn_ohe, target=target)\n",
    "#                 eval_res_test_nn_ohe = get_metrics(y_test[:,0], y_test_pred_nn_ohe, target=target)            \n",
    "#             elif target == \"categorical\":\n",
    "#                 eval_res_train_nn_ohe = get_metrics(y_train, y_train_pred_nn_ohe, target=target)\n",
    "#                 eval_res_val_nn_ohe = get_metrics(y_val, y_val_pred_nn_ohe, target=target)\n",
    "#                 eval_res_test_nn_ohe = get_metrics(y_test, y_test_pred_nn_ohe, target=target)\n",
    "#             gc.collect()   \n",
    "#             ##### Embedding #####\n",
    "#             print(\"\\n Embedding Estimate Network\")\n",
    "\n",
    "#             if embed_dims_method==\"sqrt\":\n",
    "#                 embed_dims = [int(np.sqrt(q)) for q in qs]\n",
    "#             elif embed_dims_method==\"AutoGluon\":\n",
    "#                 embed_dims = [int(np.max([100, np.round(1.6*q**0.56)])) for q in qs]\n",
    "#             else:\n",
    "#                 embed_dims = [10 for q in qs]\n",
    "\n",
    "#             input_layer = Input(shape=(d,))\n",
    "\n",
    "#             # Define embedding layers\n",
    "#             embed_inputs = []\n",
    "#             embedding_layers = []\n",
    "#             for q_num in range(len(qs)):\n",
    "#                 Z_input_layer = Input(shape=(1,))\n",
    "#                 embedding_layer = Embedding(qs[q_num], embed_dims[q_num], input_length=1)(Z_input_layer)\n",
    "#                 embedding_layer = Reshape(target_shape=(embed_dims[q_num],))(embedding_layer)\n",
    "\n",
    "#                 embed_inputs.append(Z_input_layer)\n",
    "#                 embedding_layers.append(embedding_layer)\n",
    "\n",
    "#             ### Get model layer dimensions\n",
    "#             min_numeric_embed_dim = 32\n",
    "#             max_numeric_embed_dim = 2056\n",
    "#             max_layer_width = 2056\n",
    "#             # Main dense model\n",
    "#             if target == \"continuous\":\n",
    "#                 default_layer_sizes = [256,\n",
    "#                                        128]  # overall network will have 4 layers. Input layer, 256-unit hidden layer, 128-unit hidden layer, output layer.\n",
    "#             else:\n",
    "#                 default_sizes = [256, 128]  # will be scaled adaptively\n",
    "#                 # base_size = max(1, min(num_net_outputs, 20)/2.0) # scale layer width based on number of classes\n",
    "#                 base_size = max(1, min(num_outputs,\n",
    "#                                        100) / 50)  # TODO: Updated because it improved model quality and made training far faster\n",
    "#                 default_layer_sizes = [defaultsize * base_size for defaultsize in default_sizes]\n",
    "#             layer_expansion_factor = 1  # TODO: consider scaling based on num_rows, eg: layer_expansion_factor = 2-np.exp(-max(0,train_dataset.num_examples-10000))\n",
    "#             first_layer_width = int(min(max_layer_width, layer_expansion_factor * default_layer_sizes[0]))\n",
    "\n",
    "#             # numeric embed dim\n",
    "#             vector_dim = 0  # total dimensionality of vector features (I think those should be transformed string features, which we don't have)\n",
    "#             prop_vector_features = perc_numeric  # Fraction of features that are numeric\n",
    "#             numeric_embedding_size = int(min(max_numeric_embed_dim,\n",
    "#                                              max(min_numeric_embed_dim,\n",
    "#                                                  first_layer_width * prop_vector_features * np.log10(vector_dim + 10))))\n",
    "\n",
    "\n",
    "#             numeric_embedding = Dense(numeric_embedding_size, activation=\"relu\")(input_layer)\n",
    "\n",
    "#             concat = Concatenate()([numeric_embedding] + embedding_layers)\n",
    "\n",
    "#             base_model, optimizer = get_model(model_name=model_name, \n",
    "#                                               input_size=numeric_embedding_size + sum(embed_dims), \n",
    "#                                               output_size=num_outputs, target=target,\n",
    "#                                               perc_numeric=perc_numeric, RS=RS)\n",
    "\n",
    "#             base_model.build((n, numeric_embedding_size + sum(embed_dims)))\n",
    "#             update_layer_activation(model=base_model, activation=activation_layer)\n",
    "\n",
    "#             layers = base_model(concat)\n",
    "\n",
    "#             model_embed = Model(inputs=[input_layer] + embed_inputs, outputs=layers)\n",
    "\n",
    "\n",
    "#             model_embed.compile(loss=loss_use()(), optimizer=optimizer, metrics = metrics_use)\n",
    "#             callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_auc\", patience=early_stopping, mode=stop_mode)\n",
    "\n",
    "#             start = time.time()\n",
    "#             history_nn_embed = model_embed.fit([X_train] + [Z_train[: ,q_num] for q_num in range(len(qs))], y_train,\n",
    "#                             validation_data=[[X_val] + [Z_val[: ,q_num] for q_num in range(len(qs))], y_val],\n",
    "#                             epochs=epochs, batch_size=batch_size, callbacks=[callback])\n",
    "#             end = time.time()\n",
    "#             fit_time_embed = round(end-start,2)\n",
    "\n",
    "#             y_train_pred_embed = model_embed.predict([X_train] + [Z_train[: ,q_num] for q_num in range(len(qs))]\n",
    "#                                                      ,batch_size=batch_size)\n",
    "#             y_val_pred_embed = model_embed.predict([X_val] + [Z_val[: ,q_num] for q_num in range(len(qs))]\n",
    "#                                                     ,batch_size=batch_size)\n",
    "#             y_test_pred_embed = model_embed.predict([X_test] + [Z_test[: ,q_num] for q_num in range(len(qs))]\n",
    "#                                                     ,batch_size=batch_size)\n",
    "\n",
    "#             if target == \"binary\":\n",
    "#                 eval_res_train_embed = get_metrics(y_train[:,0], y_train_pred_embed, target=target)\n",
    "#                 eval_res_val_embed = get_metrics(y_val[:,0], y_val_pred_embed, target=target)\n",
    "#                 eval_res_test_embed = get_metrics(y_test[:,0], y_test_pred_embed, target=target)\n",
    "#             elif target == \"categorical\":\n",
    "#                 eval_res_train_embed = get_metrics(y_train, y_train_pred_embed, target=target)\n",
    "#                 eval_res_val_embed = get_metrics(y_val, y_val_pred_embed, target=target)\n",
    "#                 eval_res_test_embed = get_metrics(y_test, y_test_pred_embed, target=target)\n",
    "\n",
    "#             eval_res_train_embed, eval_res_test_embed        \n",
    "\n",
    "\n",
    "            ##### Document Results #####\n",
    "            \n",
    "            results[dataset_name][fold_num][\"histories\"] = {\"GMENN\": history.history,\n",
    "#                                                        \"Ignore\": history_nn.history,\n",
    "#                                                        \"TE\": history_nn_te.history,\n",
    "# #                                                        \"GLMM\": history_nn_glmm.history,\n",
    "#                                                        \"OHE\": history_nn_ohe.history,\n",
    "#                                                        \"Embedding\": history_nn_embed.history,\n",
    "#                                                        \"Point\": history_nn_point.history,\n",
    "#                                                        \"LMMNN\": [],\n",
    "#                                                        \"ARMED\": history_armed.history,\n",
    "#                                                        \"ARMED (no adv.)\": history_armed_noadv.history,\n",
    "                                                      }\n",
    "            \n",
    "            results[dataset_name][fold_num][\"predictions\"] = {\"GMENN\": [y_train_pred_gmenn, y_val_pred_gmenn, y_test_pred_gmenn],\n",
    "                                                        \"GMENN (FE)\": [y_train_pred_gmenn_fe, y_val_pred_gmenn_fe, y_test_pred_gmenn_fe],\n",
    "#                                                         \"Ignore\": [y_train_pred_nn, y_val_pred_nn, y_test_pred_nn],\n",
    "#                                                         \"TE\": [y_train_pred_nn_te, y_val_pred_nn_te, y_test_pred_nn_te],\n",
    "# #                                                         \"GLMM\": [y_train_pred_nn_glmm, y_val_pred_nn_glmm, y_test_pred_nn_glmm],\n",
    "#                                                         \"OHE\": [y_train_pred_nn_ohe, y_val_pred_nn_ohe, y_test_pred_nn_ohe],\n",
    "#                                                         \"Embedding\": [y_train_pred_embed, y_val_pred_embed, y_test_pred_embed],\n",
    "#                                                         \"Point\": [y_train_pred_point, y_val_pred_point, y_test_pred_point],\n",
    "#                                                         \"Point (FE)\": [y_train_pred_point_fe, y_val_pred_point_fe, y_test_pred_point_fe],\n",
    "#                                                         \"LMMNN\": [y_train_pred_lmmnn, y_val_pred_lmmnn, y_test_pred_lmmnn],\n",
    "#                                                         \"LMMNN (FE)\": [y_train_pred_lmmnn_fe, y_val_pred_lmmnn_fe, y_test_pred_lmmnn_fe],\n",
    "#                                                         \"ARMED\": [train_pred_armed, val_pred_armed, test_pred_armed],\n",
    "#                                                         \"ARMED (FE)\": [train_pred_armed_fe, val_pred_armed_fe, test_pred_armed_fe],\n",
    "#                                                         \"ARMED (no adv.)\": [train_pred_armed_noadv, val_pred_armed_noadv, test_pred_armed_noadv],\n",
    "#                                                         \"ARMED (no adv.) (FE)\": [train_pred_armed_noadv_fe, val_pred_armed_noadv_fe, test_pred_armed_noadv_fe],\n",
    "                                                      }\n",
    "            \n",
    "            results[dataset_name][fold_num][\"times\"] = {\"GMENN\": fit_time_gmenn,\n",
    "#                                                    \"Ignore\": fit_time_nn,\n",
    "#                                                    \"TE\": fit_time_te,\n",
    "# #                                                    \"GLMM\": fit_time_glmm,\n",
    "#                                                    \"OHE\": fit_time_ohe,\n",
    "#                                                    \"Embedding\": fit_time_embed,\n",
    "#                                                    \"Point\": fit_time_point,\n",
    "#                                                    \"LMMNN\": fit_time_lmmnn,\n",
    "#                                                    \"ARMED\": fit_time_armed,\n",
    "#                                                    \"ARMED (no adv.)\": fit_time_armed_noadv,\n",
    "                                                      }\n",
    "            \n",
    "            results[dataset_name][fold_num][\"other_info\"] = {\n",
    "                \"GMENN\": {\n",
    "                    \"_stddev_z\": np.array([i.numpy() for i in me_model.data_model._stddev_z]),\n",
    "                    \"acceptance_rates\": np.array(me_model.acceptance_rates),\n",
    "                    \"random_effects\": me_model.mean_samples,\n",
    "                    \"all_samples\": me_model.all_samples,\n",
    "                    \"stds\": me_model.stds\n",
    "                },\n",
    "#                 \"LMMNN\": {\n",
    "#                     \"_stddev_z\": sigmas,\n",
    "#                     \"random_effects\": b_hats,\n",
    "#                 },\n",
    "#                 \"ARMED\": {\n",
    "#                     \"_stddev_z\": np.std(model_armed.randomeffects.re_int.weights[0].numpy()[:qs[0]]),\n",
    "#                     \"random_effects\": model_armed.randomeffects.re_int.weights[0].numpy()[:qs[0]],\n",
    "#                     \"pred_cluster\": [train_pred_cluster_armed,val_pred_cluster_armed,test_pred_cluster_armed]\n",
    "#                 },\n",
    "#                 \"ARMED (no adv.)\": {\n",
    "#                     \"_stddev_z\": np.std(model_armed_noadv.randomeffects.re_int.weights[0].numpy()[:qs[0]]),\n",
    "#                     \"random_effects\": model_armed_noadv.randomeffects.re_int.weights[0].numpy()[:qs[0]],\n",
    "#                     \"pred_cluster\": [train_pred_cluster_armed_noadv,val_pred_cluster_armed_noadv,test_pred_cluster_armed_noadv]\n",
    "#                 }\n",
    "            }\n",
    "            \n",
    "#             del model_lmmnn, model_lmmnn_fe\n",
    "            \n",
    "            with open(f\"{save_path}//results_RS{RS}_{dataset_name}_iter{fold_num}.pickle\", 'wb') as handle:\n",
    "                pickle.dump(results[dataset_name][fold_num], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "            \n",
    "            del X_train, X_val, X_test, y_train, y_val, y_test\n",
    "#             del z_target_encoded_train, z_target_encoded_val, z_target_encoded_test\n",
    "#             del z_ohe_encoded_train, z_ohe_encoded_val, z_ohe_encoded_test\n",
    "#             del z_glmm_encoded_train, z_glmm_encoded_val, z_glmm_encoded_test\n",
    "            \n",
    "            gc.collect()\n",
    "        else:\n",
    "            print(f\"Load results for dataset {dataset_name}, iteration={fold_num}\")\n",
    "            with open(f\"{save_path}/results_RS{RS}_{dataset_name}_iter{fold_num}.pickle\", 'rb') as handle:\n",
    "                results[dataset_name][fold_num] = pickle.load(handle)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95cdc157",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['churn', 'kdd_internet_usage', 'Amazon_employee_access', 'Click_prediction_small', 'adult', 'KDDCup09_upselling', 'kick', 'open_payments', 'road-safety-drivers-sex', 'porto-seguro'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7825ce97",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03ecb737",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set nan for churn, 0\n",
      "Set nan for churn, 0\n",
      "Set nan for churn, 0\n",
      "Set nan for churn, 0\n",
      "Set nan for churn, 1\n",
      "Set nan for churn, 1\n",
      "Set nan for churn, 1\n",
      "Set nan for churn, 1\n",
      "Set nan for churn, 2\n",
      "Set nan for churn, 2\n",
      "Set nan for churn, 2\n",
      "Set nan for churn, 2\n",
      "Set nan for churn, 3\n",
      "Set nan for churn, 3\n",
      "Set nan for churn, 3\n",
      "Set nan for churn, 3\n",
      "Set nan for churn, 4\n",
      "Set nan for churn, 4\n",
      "Set nan for churn, 4\n",
      "Set nan for churn, 4\n",
      "Set nan for kdd_internet_usage, 0\n",
      "Set nan for kdd_internet_usage, 0\n",
      "Set nan for kdd_internet_usage, 0\n",
      "Set nan for kdd_internet_usage, 0\n",
      "Set nan for kdd_internet_usage, 1\n",
      "Set nan for kdd_internet_usage, 1\n",
      "Set nan for kdd_internet_usage, 1\n",
      "Set nan for kdd_internet_usage, 1\n",
      "Set nan for kdd_internet_usage, 2\n",
      "Set nan for kdd_internet_usage, 2\n",
      "Set nan for kdd_internet_usage, 2\n",
      "Set nan for kdd_internet_usage, 2\n",
      "Set nan for kdd_internet_usage, 3\n",
      "Set nan for kdd_internet_usage, 3\n",
      "Set nan for kdd_internet_usage, 3\n",
      "Set nan for kdd_internet_usage, 3\n",
      "Set nan for kdd_internet_usage, 4\n",
      "Set nan for kdd_internet_usage, 4\n",
      "Set nan for kdd_internet_usage, 4\n",
      "Set nan for kdd_internet_usage, 4\n",
      "Set nan for Amazon_employee_access, 0\n",
      "Set nan for Amazon_employee_access, 0\n",
      "Set nan for Amazon_employee_access, 0\n",
      "Set nan for Amazon_employee_access, 0\n",
      "Set nan for Amazon_employee_access, 1\n",
      "Set nan for Amazon_employee_access, 1\n",
      "Set nan for Amazon_employee_access, 1\n",
      "Set nan for Amazon_employee_access, 1\n",
      "Set nan for Amazon_employee_access, 2\n",
      "Set nan for Amazon_employee_access, 2\n",
      "Set nan for Amazon_employee_access, 2\n",
      "Set nan for Amazon_employee_access, 2\n",
      "Set nan for Amazon_employee_access, 3\n",
      "Set nan for Amazon_employee_access, 3\n",
      "Set nan for Amazon_employee_access, 3\n",
      "Set nan for Amazon_employee_access, 3\n",
      "Set nan for Amazon_employee_access, 4\n",
      "Set nan for Amazon_employee_access, 4\n",
      "Set nan for Amazon_employee_access, 4\n",
      "Set nan for Amazon_employee_access, 4\n",
      "Set nan for Click_prediction_small, 0\n",
      "Set nan for Click_prediction_small, 0\n",
      "Set nan for Click_prediction_small, 0\n",
      "Set nan for Click_prediction_small, 0\n",
      "Set nan for Click_prediction_small, 1\n",
      "Set nan for Click_prediction_small, 1\n",
      "Set nan for Click_prediction_small, 1\n",
      "Set nan for Click_prediction_small, 1\n",
      "Set nan for Click_prediction_small, 2\n",
      "Set nan for Click_prediction_small, 2\n",
      "Set nan for Click_prediction_small, 2\n",
      "Set nan for Click_prediction_small, 2\n",
      "Set nan for Click_prediction_small, 3\n",
      "Set nan for Click_prediction_small, 3\n",
      "Set nan for Click_prediction_small, 3\n",
      "Set nan for Click_prediction_small, 3\n",
      "Set nan for Click_prediction_small, 4\n",
      "Set nan for Click_prediction_small, 4\n",
      "Set nan for Click_prediction_small, 4\n",
      "Set nan for Click_prediction_small, 4\n",
      "Set nan for adult, 0\n",
      "Set nan for adult, 0\n",
      "Set nan for adult, 0\n",
      "Set nan for adult, 0\n",
      "Set nan for adult, 1\n",
      "Set nan for adult, 1\n",
      "Set nan for adult, 1\n",
      "Set nan for adult, 1\n",
      "Set nan for adult, 2\n",
      "Set nan for adult, 2\n",
      "Set nan for adult, 2\n",
      "Set nan for adult, 2\n",
      "Set nan for adult, 3\n",
      "Set nan for adult, 3\n",
      "Set nan for adult, 3\n",
      "Set nan for adult, 3\n",
      "Set nan for adult, 4\n",
      "Set nan for adult, 4\n",
      "Set nan for adult, 4\n",
      "Set nan for adult, 4\n",
      "Set nan for KDDCup09_upselling, 0\n",
      "Set nan for KDDCup09_upselling, 0\n",
      "Set nan for KDDCup09_upselling, 0\n",
      "Set nan for KDDCup09_upselling, 0\n",
      "Set nan for KDDCup09_upselling, 1\n",
      "Set nan for KDDCup09_upselling, 1\n",
      "Set nan for KDDCup09_upselling, 1\n",
      "Set nan for KDDCup09_upselling, 1\n",
      "Set nan for KDDCup09_upselling, 2\n",
      "Set nan for KDDCup09_upselling, 2\n",
      "Set nan for KDDCup09_upselling, 2\n",
      "Set nan for KDDCup09_upselling, 2\n",
      "Set nan for KDDCup09_upselling, 3\n",
      "Set nan for KDDCup09_upselling, 3\n",
      "Set nan for KDDCup09_upselling, 3\n",
      "Set nan for KDDCup09_upselling, 3\n",
      "Set nan for KDDCup09_upselling, 4\n",
      "Set nan for KDDCup09_upselling, 4\n",
      "Set nan for KDDCup09_upselling, 4\n",
      "Set nan for KDDCup09_upselling, 4\n",
      "Set nan for kick, 0\n",
      "Set nan for kick, 0\n",
      "Set nan for kick, 0\n",
      "Set nan for kick, 0\n",
      "Set nan for kick, 1\n",
      "Set nan for kick, 1\n",
      "Set nan for kick, 1\n",
      "Set nan for kick, 1\n",
      "Set nan for kick, 2\n",
      "Set nan for kick, 2\n",
      "Set nan for kick, 2\n",
      "Set nan for kick, 2\n",
      "Set nan for kick, 3\n",
      "Set nan for kick, 3\n",
      "Set nan for kick, 3\n",
      "Set nan for kick, 3\n",
      "Set nan for kick, 4\n",
      "Set nan for kick, 4\n",
      "Set nan for kick, 4\n",
      "Set nan for kick, 4\n",
      "Set nan for open_payments, 0\n",
      "Set nan for open_payments, 0\n",
      "Set nan for open_payments, 0\n",
      "Set nan for open_payments, 0\n",
      "Set nan for open_payments, 1\n",
      "Set nan for open_payments, 1\n",
      "Set nan for open_payments, 1\n",
      "Set nan for open_payments, 1\n",
      "Set nan for open_payments, 2\n",
      "Set nan for open_payments, 2\n",
      "Set nan for open_payments, 2\n",
      "Set nan for open_payments, 2\n",
      "Set nan for open_payments, 3\n",
      "Set nan for open_payments, 3\n",
      "Set nan for open_payments, 3\n",
      "Set nan for open_payments, 3\n",
      "Set nan for open_payments, 4\n",
      "Set nan for open_payments, 4\n",
      "Set nan for open_payments, 4\n",
      "Set nan for open_payments, 4\n",
      "Set nan for road-safety-drivers-sex, 0\n",
      "Set nan for road-safety-drivers-sex, 0\n",
      "Set nan for road-safety-drivers-sex, 0\n",
      "Set nan for road-safety-drivers-sex, 0\n",
      "Set nan for road-safety-drivers-sex, 1\n",
      "Set nan for road-safety-drivers-sex, 1\n",
      "Set nan for road-safety-drivers-sex, 1\n",
      "Set nan for road-safety-drivers-sex, 1\n",
      "Set nan for road-safety-drivers-sex, 2\n",
      "Set nan for road-safety-drivers-sex, 2\n",
      "Set nan for road-safety-drivers-sex, 2\n",
      "Set nan for road-safety-drivers-sex, 2\n",
      "Set nan for road-safety-drivers-sex, 3\n",
      "Set nan for road-safety-drivers-sex, 3\n",
      "Set nan for road-safety-drivers-sex, 3\n",
      "Set nan for road-safety-drivers-sex, 3\n",
      "Set nan for road-safety-drivers-sex, 4\n",
      "Set nan for road-safety-drivers-sex, 4\n",
      "Set nan for road-safety-drivers-sex, 4\n",
      "Set nan for road-safety-drivers-sex, 4\n",
      "Set nan for porto-seguro, 0\n",
      "Set nan for porto-seguro, 0\n",
      "Set nan for porto-seguro, 0\n",
      "Set nan for porto-seguro, 0\n",
      "Set nan for porto-seguro, 1\n",
      "Set nan for porto-seguro, 1\n",
      "Set nan for porto-seguro, 1\n",
      "Set nan for porto-seguro, 1\n",
      "Set nan for porto-seguro, 2\n",
      "Set nan for porto-seguro, 2\n",
      "Set nan for porto-seguro, 2\n",
      "Set nan for porto-seguro, 2\n",
      "Set nan for porto-seguro, 3\n",
      "Set nan for porto-seguro, 3\n",
      "Set nan for porto-seguro, 3\n",
      "Set nan for porto-seguro, 3\n",
      "Set nan for porto-seguro, 4\n",
      "Set nan for porto-seguro, 4\n",
      "Set nan for porto-seguro, 4\n",
      "Set nan for porto-seguro, 4\n"
     ]
    }
   ],
   "source": [
    "models = [\"GMENN\", \"TE\", \"OHE\", \"Embedding\", \"Ignore\"]\n",
    "results_perf = {dataset_name: {num: {model: {}  for model in models} for num in range(folds)} for dataset_name in dataset_names}\n",
    "for dataset_name in dataset_names:\n",
    "    try:\n",
    "        with open(f\"../data/prepared/{dataset_name}/{data_path}/data_dict.pickle\", 'rb') as handle:\n",
    "            data_dict = pickle.load(handle)        \n",
    "    except:\n",
    "        print(f\"dataset {dataset_name} not found\") \n",
    "    for num in range(folds):\n",
    "#         print(num)\n",
    "        n_classes=1\n",
    "        y_test = tf.one_hot(data_dict[f\"y_test_{num}\"],n_classes)\n",
    "        for model in models:\n",
    "            try:\n",
    "                y_pred = np.array(results[dataset_name][num][\"predictions\"][model][2]).ravel()\n",
    "\n",
    "                results_perf[dataset_name][num][model] = get_metrics(y_test,y_pred,target)\n",
    "                results_perf[dataset_name][num][model][\"Time\"] = results[dataset_name][num][\"times\"][model]\n",
    "            except:\n",
    "                print(f\"Set nan for {dataset_name}, {num}\")\n",
    "                results_perf[dataset_name][num][model] = {\"Accuracy\": np.nan,\n",
    "                                                          \"AUROC\": np.nan,\n",
    "                                                          \"F1\": np.nan,\n",
    "                                                          \"Time\": np.nan}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a874334b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GMENN</th>\n",
       "      <th>TE</th>\n",
       "      <th>OHE</th>\n",
       "      <th>Embedding</th>\n",
       "      <th>Ignore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>churn</th>\n",
       "      <td>0.86 (0.017)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kdd_internet_usage</th>\n",
       "      <td>0.93 (0.005)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amazon_employee_access</th>\n",
       "      <td>0.76 (0.019)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Click_prediction_small</th>\n",
       "      <td>0.6 (0.016)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adult</th>\n",
       "      <td>0.91 (0.003)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KDDCup09_upselling</th>\n",
       "      <td>0.74 (0.021)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kick</th>\n",
       "      <td>0.72 (0.01)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>open_payments</th>\n",
       "      <td>0.88 (0.015)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>road-safety-drivers-sex</th>\n",
       "      <td>0.69 (0.011)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>porto-seguro</th>\n",
       "      <td>0.55 (0.004)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                GMENN         TE        OHE  Embedding  \\\n",
       "churn                    0.86 (0.017)  nan (nan)  nan (nan)  nan (nan)   \n",
       "kdd_internet_usage       0.93 (0.005)  nan (nan)  nan (nan)  nan (nan)   \n",
       "Amazon_employee_access   0.76 (0.019)  nan (nan)  nan (nan)  nan (nan)   \n",
       "Click_prediction_small    0.6 (0.016)  nan (nan)  nan (nan)  nan (nan)   \n",
       "adult                    0.91 (0.003)  nan (nan)  nan (nan)  nan (nan)   \n",
       "KDDCup09_upselling       0.74 (0.021)  nan (nan)  nan (nan)  nan (nan)   \n",
       "kick                      0.72 (0.01)  nan (nan)  nan (nan)  nan (nan)   \n",
       "open_payments            0.88 (0.015)  nan (nan)  nan (nan)  nan (nan)   \n",
       "road-safety-drivers-sex  0.69 (0.011)  nan (nan)  nan (nan)  nan (nan)   \n",
       "porto-seguro             0.55 (0.004)  nan (nan)  nan (nan)  nan (nan)   \n",
       "\n",
       "                            Ignore  \n",
       "churn                    nan (nan)  \n",
       "kdd_internet_usage       nan (nan)  \n",
       "Amazon_employee_access   nan (nan)  \n",
       "Click_prediction_small   nan (nan)  \n",
       "adult                    nan (nan)  \n",
       "KDDCup09_upselling       nan (nan)  \n",
       "kick                     nan (nan)  \n",
       "open_payments            nan (nan)  \n",
       "road-safety-drivers-sex  nan (nan)  \n",
       "porto-seguro             nan (nan)  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [\"GMENN\", \"TE\", \"OHE\", \"Embedding\", \"Ignore\"]\n",
    "metric = \"AUROC\"\n",
    "\n",
    "#####\n",
    "dataset_res_dict = {}\n",
    "best_models = {}\n",
    "t_test_results = {}\n",
    "\n",
    "round_mean_at = 2\n",
    "round_std_at = 3\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    dataset_models = list(results_perf[dataset_name][0].keys())\n",
    "    use_df = pd.DataFrame([pd.DataFrame(results_perf[dataset_name][fold_num]).loc[metric,models] for fold_num in results_perf[dataset_name].keys()],index=results_perf[dataset_name].keys())\n",
    "    \n",
    "    df_mean = pd.DataFrame(use_df.mean(axis=0).round(round_mean_at).astype(str) + \" (\" + use_df.std(axis=0).round(round_std_at).astype(str) + \")\").transpose()\n",
    "    model_dict = {i: df_mean[i].values[0] for i in df_mean.columns}\n",
    "    dataset_res_dict[dataset_name] = model_dict\n",
    "    \n",
    "    best_models[dataset_name] = use_df.columns[use_df.mean(axis=0).argmax()]\n",
    "\n",
    "    t_test_res = np.array([stats.ttest_rel(use_df[best_models[dataset_name]].values, use_df[model].values)[1] if model in dataset_models else 0 for model in models]).round(3)\n",
    "    t_test_res[np.isnan(t_test_res)] = 1.\n",
    "    t_test_results[dataset_name] = t_test_res\n",
    "    \n",
    "res_df = pd.DataFrame(dataset_res_dict).transpose()\n",
    "    \n",
    "def negative_bold(val):\n",
    "    i = np.where(val.name==np.array(models))[0][0]\n",
    "    return [\"font-weight: bold\"  if t_test_results[dataset_name][i]>=0.05 else \"\" for dataset_name in val.keys()]\n",
    "    # Case without transpose:\n",
    "#     return [\"font-weight: bold\"  if t_test_results[val.name][i]>=0.05 else \"\" for i in range(len(val))]\n",
    "\n",
    "# res_df.style.apply(negative_bold)\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33b145fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.86, 0.93, 0.76, 0.6 , 0.91, 0.74, 0.72, 0.88, 0.69, 0.55])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df[\"GMENN\"].apply(lambda x: float(x.split(\" \")[0])).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328d4e2d",
   "metadata": {},
   "source": [
    "### Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eea2f7b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GMENN</th>\n",
       "      <th>TE</th>\n",
       "      <th>OHE</th>\n",
       "      <th>Embedding</th>\n",
       "      <th>Ignore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>churn</th>\n",
       "      <td>0.26 (0.04)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kdd_internet_usage</th>\n",
       "      <td>0.36 (0.026)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amazon_employee_access</th>\n",
       "      <td>4.02 (0.438)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Click_prediction_small</th>\n",
       "      <td>2.38 (0.473)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adult</th>\n",
       "      <td>0.28 (0.006)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KDDCup09_upselling</th>\n",
       "      <td>3.2 (1.255)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kick</th>\n",
       "      <td>1.27 (1.051)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>open_payments</th>\n",
       "      <td>3.64 (0.072)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>road-safety-drivers-sex</th>\n",
       "      <td>8.11 (3.252)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>porto-seguro</th>\n",
       "      <td>3.37 (2.171)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                GMENN         TE        OHE  Embedding  \\\n",
       "churn                     0.26 (0.04)  nan (nan)  nan (nan)  nan (nan)   \n",
       "kdd_internet_usage       0.36 (0.026)  nan (nan)  nan (nan)  nan (nan)   \n",
       "Amazon_employee_access   4.02 (0.438)  nan (nan)  nan (nan)  nan (nan)   \n",
       "Click_prediction_small   2.38 (0.473)  nan (nan)  nan (nan)  nan (nan)   \n",
       "adult                    0.28 (0.006)  nan (nan)  nan (nan)  nan (nan)   \n",
       "KDDCup09_upselling        3.2 (1.255)  nan (nan)  nan (nan)  nan (nan)   \n",
       "kick                     1.27 (1.051)  nan (nan)  nan (nan)  nan (nan)   \n",
       "open_payments            3.64 (0.072)  nan (nan)  nan (nan)  nan (nan)   \n",
       "road-safety-drivers-sex  8.11 (3.252)  nan (nan)  nan (nan)  nan (nan)   \n",
       "porto-seguro             3.37 (2.171)  nan (nan)  nan (nan)  nan (nan)   \n",
       "\n",
       "                            Ignore  \n",
       "churn                    nan (nan)  \n",
       "kdd_internet_usage       nan (nan)  \n",
       "Amazon_employee_access   nan (nan)  \n",
       "Click_prediction_small   nan (nan)  \n",
       "adult                    nan (nan)  \n",
       "KDDCup09_upselling       nan (nan)  \n",
       "kick                     nan (nan)  \n",
       "open_payments            nan (nan)  \n",
       "road-safety-drivers-sex  nan (nan)  \n",
       "porto-seguro             nan (nan)  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [\"GMENN\", \"TE\", \"OHE\", \"Embedding\", \"Ignore\"]\n",
    "metric = \"Time\"\n",
    "\n",
    "#####\n",
    "dataset_res_dict = {}\n",
    "best_models = {}\n",
    "t_test_results = {}\n",
    "\n",
    "round_mean_at = 2\n",
    "round_std_at = 3\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    dataset_models = list(results_perf[dataset_name][0].keys())\n",
    "    use_df = pd.DataFrame([pd.DataFrame(results_perf[dataset_name][fold_num]).loc[metric,models] for fold_num in results_perf[dataset_name].keys()],index=results_perf[dataset_name].keys())/60\n",
    "    \n",
    "    df_mean = pd.DataFrame(use_df.mean(axis=0).round(round_mean_at).astype(str) + \" (\" + use_df.std(axis=0).round(round_std_at).astype(str) + \")\").transpose()\n",
    "    model_dict = {i: df_mean[i].values[0] for i in df_mean.columns}\n",
    "    dataset_res_dict[dataset_name] = model_dict\n",
    "    \n",
    "    best_models[dataset_name] = use_df.columns[use_df.mean(axis=0).argmin()]\n",
    "\n",
    "    t_test_res = np.array([stats.ttest_rel(use_df[best_models[dataset_name]].values, use_df[model].values)[1] if model in dataset_models else 0 for model in models]).round(3)\n",
    "    t_test_res[np.isnan(t_test_res)] = 1.\n",
    "    t_test_results[dataset_name] = t_test_res\n",
    "    \n",
    "res_df = pd.DataFrame(dataset_res_dict).transpose()\n",
    "    \n",
    "def negative_bold(val):\n",
    "    i = np.where(val.name==np.array(models))[0][0]\n",
    "    return [\"font-weight: bold\"  if t_test_results[dataset_name][i]>=0.05 else \"\" for dataset_name in val.keys()]\n",
    "    # Case without transpose:\n",
    "#     return [\"font-weight: bold\"  if t_test_results[val.name][i]>=0.05 else \"\" for i in range(len(val))]\n",
    "\n",
    "# res_df.style.apply(negative_bold)\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08386c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.26, 0.36, 4.02, 2.38, 0.28, 3.2 , 1.27, 3.64, 8.11, 3.37])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df[\"GMENN\"].apply(lambda x: float(x.split(\" \")[0])).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28914dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cf47f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gmenn",
   "language": "python",
   "name": "gmenn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc07112e7ae1e8e28a0232207620ff002934c05692de8df42430404c766a0a8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
