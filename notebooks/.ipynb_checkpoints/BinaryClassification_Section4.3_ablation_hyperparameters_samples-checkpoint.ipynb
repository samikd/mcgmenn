{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca4ea20d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Found GPU: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# Append root path \n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../lmmnn\")\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"]=\"true\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "\n",
    "if tf.test.gpu_device_name() != '/device:GPU:0':\n",
    "    print('WARNING: GPU device not found.')\n",
    "else:\n",
    "    print('SUCCESS: Found GPU: {}'.format(tf.test.gpu_device_name()))\n",
    "\n",
    "from model.mixed_effects import *\n",
    "from utils.fe_models import get_model\n",
    "from utils.evaluation import *\n",
    "from utils.utils import *\n",
    "from data.preprocessing import dataset_preprocessing\n",
    "from utils.training_functions import *\n",
    "\n",
    "# from vis.utils.utils import apply_modifications\n",
    "# # helper function\n",
    "def update_layer_activation(model, activation, index=-1):\n",
    "    model.layers[index].activation = activation\n",
    "    return model\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Reshape, Embedding, Concatenate\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "\n",
    "from sklearn.metrics import accuracy_score as acc\n",
    "from sklearn.metrics import roc_auc_score as auroc\n",
    "from sklearn.metrics import f1_score as f1\n",
    "from sklearn.model_selection import train_test_split\n",
    "from category_encoders import TargetEncoder\n",
    "from tensorflow_addons.metrics import F1Score\n",
    "\n",
    "from scipy import stats\n",
    "import pickle\n",
    "import yaml\n",
    "import time\n",
    "import gc\n",
    "\n",
    "RS = 555"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258caf10",
   "metadata": {},
   "source": [
    "#### Download and save data from Pargent et al. by running \"data/download_pargent2022_datasets.py before running this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "057f4663",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training procedure for churn\n",
      "Fold no. 0\n",
      "Load results for dataset churn, iteration=0\n",
      "Load results for dataset churn, iteration=0\n",
      "Load results for dataset churn, iteration=0\n",
      "Load results for dataset churn, iteration=0\n",
      "Fold no. 1\n",
      "Load results for dataset churn, iteration=1\n",
      "Load results for dataset churn, iteration=1\n",
      "Load results for dataset churn, iteration=1\n",
      "Load results for dataset churn, iteration=1\n",
      "Fold no. 2\n",
      "Load results for dataset churn, iteration=2\n",
      "Load results for dataset churn, iteration=2\n",
      "Load results for dataset churn, iteration=2\n",
      "Load results for dataset churn, iteration=2\n",
      "Fold no. 3\n",
      "Load results for dataset churn, iteration=3\n",
      "Load results for dataset churn, iteration=3\n",
      "Load results for dataset churn, iteration=3\n",
      "Load results for dataset churn, iteration=3\n",
      "Fold no. 4\n",
      "Load results for dataset churn, iteration=4\n",
      "Load results for dataset churn, iteration=4\n",
      "Load results for dataset churn, iteration=4\n",
      "Load results for dataset churn, iteration=4\n",
      "Start training procedure for kdd_internet_usage\n",
      "Fold no. 0\n",
      "Load results for dataset kdd_internet_usage, iteration=0\n",
      "Load results for dataset kdd_internet_usage, iteration=0\n",
      "Load results for dataset kdd_internet_usage, iteration=0\n",
      "Load results for dataset kdd_internet_usage, iteration=0\n",
      "Fold no. 1\n",
      "Load results for dataset kdd_internet_usage, iteration=1\n",
      "Load results for dataset kdd_internet_usage, iteration=1\n",
      "Load results for dataset kdd_internet_usage, iteration=1\n",
      "Load results for dataset kdd_internet_usage, iteration=1\n",
      "Fold no. 2\n",
      "Load results for dataset kdd_internet_usage, iteration=2\n",
      "Load results for dataset kdd_internet_usage, iteration=2\n",
      "Load results for dataset kdd_internet_usage, iteration=2\n",
      "Load results for dataset kdd_internet_usage, iteration=2\n",
      "Fold no. 3\n",
      "Load results for dataset kdd_internet_usage, iteration=3\n",
      "Load results for dataset kdd_internet_usage, iteration=3\n",
      "Load results for dataset kdd_internet_usage, iteration=3\n",
      "Load results for dataset kdd_internet_usage, iteration=3\n",
      "Fold no. 4\n",
      "Load results for dataset kdd_internet_usage, iteration=4\n",
      "Load results for dataset kdd_internet_usage, iteration=4\n",
      "Load results for dataset kdd_internet_usage, iteration=4\n",
      "Load results for dataset kdd_internet_usage, iteration=4\n",
      "Start training procedure for Amazon_employee_access\n",
      "Fold no. 0\n",
      "Load results for dataset Amazon_employee_access, iteration=0\n",
      "Load results for dataset Amazon_employee_access, iteration=0\n",
      "Load results for dataset Amazon_employee_access, iteration=0\n",
      "Load results for dataset Amazon_employee_access, iteration=0\n",
      "Fold no. 1\n",
      "Load results for dataset Amazon_employee_access, iteration=1\n",
      "Load results for dataset Amazon_employee_access, iteration=1\n",
      "Load results for dataset Amazon_employee_access, iteration=1\n",
      "Load results for dataset Amazon_employee_access, iteration=1\n",
      "Fold no. 2\n",
      "Load results for dataset Amazon_employee_access, iteration=2\n",
      "Load results for dataset Amazon_employee_access, iteration=2\n",
      "Load results for dataset Amazon_employee_access, iteration=2\n",
      "Load results for dataset Amazon_employee_access, iteration=2\n",
      "Fold no. 3\n",
      "Load results for dataset Amazon_employee_access, iteration=3\n",
      "Load results for dataset Amazon_employee_access, iteration=3\n",
      "Load results for dataset Amazon_employee_access, iteration=3\n",
      "Load results for dataset Amazon_employee_access, iteration=3\n",
      "Fold no. 4\n",
      "Load results for dataset Amazon_employee_access, iteration=4\n",
      "Load results for dataset Amazon_employee_access, iteration=4\n",
      "Load results for dataset Amazon_employee_access, iteration=4\n",
      "Load results for dataset Amazon_employee_access, iteration=4\n",
      "Start training procedure for Click_prediction_small\n",
      "Fold no. 0\n",
      "Load results for dataset Click_prediction_small, iteration=0\n",
      "Load results for dataset Click_prediction_small, iteration=0\n",
      "Load results for dataset Click_prediction_small, iteration=0\n",
      "Load results for dataset Click_prediction_small, iteration=0\n",
      "Fold no. 1\n",
      "Load results for dataset Click_prediction_small, iteration=1\n",
      "Load results for dataset Click_prediction_small, iteration=1\n",
      "Load results for dataset Click_prediction_small, iteration=1\n",
      "Load results for dataset Click_prediction_small, iteration=1\n",
      "Fold no. 2\n",
      "Load results for dataset Click_prediction_small, iteration=2\n",
      "Load results for dataset Click_prediction_small, iteration=2\n",
      "Load results for dataset Click_prediction_small, iteration=2\n",
      "Load results for dataset Click_prediction_small, iteration=2\n",
      "Fold no. 3\n",
      "Load results for dataset Click_prediction_small, iteration=3\n",
      "Load results for dataset Click_prediction_small, iteration=3\n",
      "Load results for dataset Click_prediction_small, iteration=3\n",
      "Load results for dataset Click_prediction_small, iteration=3\n",
      "Fold no. 4\n",
      "Load results for dataset Click_prediction_small, iteration=4\n",
      "Load results for dataset Click_prediction_small, iteration=4\n",
      "Load results for dataset Click_prediction_small, iteration=4\n",
      "Load results for dataset Click_prediction_small, iteration=4\n",
      "Start training procedure for adult\n",
      "Fold no. 0\n",
      "Load results for dataset adult, iteration=0\n",
      "Load results for dataset adult, iteration=0\n",
      "Load results for dataset adult, iteration=0\n",
      "Load results for dataset adult, iteration=0\n",
      "Fold no. 1\n",
      "Load results for dataset adult, iteration=1\n",
      "Load results for dataset adult, iteration=1\n",
      "Load results for dataset adult, iteration=1\n",
      "Load results for dataset adult, iteration=1\n",
      "Fold no. 2\n",
      "Load results for dataset adult, iteration=2\n",
      "Load results for dataset adult, iteration=2\n",
      "Load results for dataset adult, iteration=2\n",
      "Load results for dataset adult, iteration=2\n",
      "Fold no. 3\n",
      "Load results for dataset adult, iteration=3\n",
      "Load results for dataset adult, iteration=3\n",
      "Load results for dataset adult, iteration=3\n",
      "Load results for dataset adult, iteration=3\n",
      "Fold no. 4\n",
      "Load results for dataset adult, iteration=4\n",
      "Load results for dataset adult, iteration=4\n",
      "Load results for dataset adult, iteration=4\n",
      "Load results for dataset adult, iteration=4\n",
      "Start training procedure for KDDCup09_upselling\n",
      "Fold no. 0\n",
      "Load results for dataset KDDCup09_upselling, iteration=0\n",
      "Load results for dataset KDDCup09_upselling, iteration=0\n",
      "Load results for dataset KDDCup09_upselling, iteration=0\n",
      "Load results for dataset KDDCup09_upselling, iteration=0\n",
      "Fold no. 1\n",
      "Load results for dataset KDDCup09_upselling, iteration=1\n",
      "Load results for dataset KDDCup09_upselling, iteration=1\n",
      "Load results for dataset KDDCup09_upselling, iteration=1\n",
      "Load results for dataset KDDCup09_upselling, iteration=1\n",
      "Fold no. 2\n",
      "Load results for dataset KDDCup09_upselling, iteration=2\n",
      "Load results for dataset KDDCup09_upselling, iteration=2\n",
      "Load results for dataset KDDCup09_upselling, iteration=2\n",
      "Load results for dataset KDDCup09_upselling, iteration=2\n",
      "Fold no. 3\n",
      "Load results for dataset KDDCup09_upselling, iteration=3\n",
      "Load results for dataset KDDCup09_upselling, iteration=3\n",
      "Load results for dataset KDDCup09_upselling, iteration=3\n",
      "Load results for dataset KDDCup09_upselling, iteration=3\n",
      "Fold no. 4\n",
      "Load results for dataset KDDCup09_upselling, iteration=4\n",
      "Load results for dataset KDDCup09_upselling, iteration=4\n",
      "Load results for dataset KDDCup09_upselling, iteration=4\n",
      "Load results for dataset KDDCup09_upselling, iteration=4\n",
      "Start training procedure for kick\n",
      "Fold no. 0\n",
      "Load results for dataset kick, iteration=0\n",
      "Load results for dataset kick, iteration=0\n",
      "Load results for dataset kick, iteration=0\n",
      "Load results for dataset kick, iteration=0\n",
      "Fold no. 1\n",
      "Load results for dataset kick, iteration=1\n",
      "Load results for dataset kick, iteration=1\n",
      "Load results for dataset kick, iteration=1\n",
      "Load results for dataset kick, iteration=1\n",
      "Fold no. 2\n",
      "Load results for dataset kick, iteration=2\n",
      "Load results for dataset kick, iteration=2\n",
      "Load results for dataset kick, iteration=2\n",
      "Load results for dataset kick, iteration=2\n",
      "Fold no. 3\n",
      "Load results for dataset kick, iteration=3\n",
      "Load results for dataset kick, iteration=3\n",
      "Load results for dataset kick, iteration=3\n",
      "Load results for dataset kick, iteration=3\n",
      "Fold no. 4\n",
      "Load results for dataset kick, iteration=4\n",
      "Load results for dataset kick, iteration=4\n",
      "Load results for dataset kick, iteration=4\n",
      "Load results for dataset kick, iteration=4\n",
      "Start training procedure for open_payments\n",
      "Fold no. 0\n",
      "Load results for dataset open_payments, iteration=0\n",
      "Load results for dataset open_payments, iteration=0\n",
      "Load results for dataset open_payments, iteration=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load results for dataset open_payments, iteration=0\n",
      "Fold no. 1\n",
      "Load results for dataset open_payments, iteration=1\n",
      "Load results for dataset open_payments, iteration=1\n",
      "Load results for dataset open_payments, iteration=1\n",
      "Load results for dataset open_payments, iteration=1\n",
      "Fold no. 2\n",
      "Load results for dataset open_payments, iteration=2\n",
      "Load results for dataset open_payments, iteration=2\n",
      "Load results for dataset open_payments, iteration=2\n",
      "Load results for dataset open_payments, iteration=2\n",
      "Fold no. 3\n",
      "Load results for dataset open_payments, iteration=3\n",
      "Load results for dataset open_payments, iteration=3\n",
      "Load results for dataset open_payments, iteration=3\n",
      "Load results for dataset open_payments, iteration=3\n",
      "Fold no. 4\n",
      "Load results for dataset open_payments, iteration=4\n",
      "Load results for dataset open_payments, iteration=4\n",
      "Load results for dataset open_payments, iteration=4\n",
      "Load results for dataset open_payments, iteration=4\n",
      "Start training procedure for road-safety-drivers-sex\n",
      "Fold no. 0\n",
      "Load results for dataset road-safety-drivers-sex, iteration=0\n",
      "Load results for dataset road-safety-drivers-sex, iteration=0\n",
      "Load results for dataset road-safety-drivers-sex, iteration=0\n",
      "Load results for dataset road-safety-drivers-sex, iteration=0\n",
      "Fold no. 1\n",
      "Load results for dataset road-safety-drivers-sex, iteration=1\n",
      "Load results for dataset road-safety-drivers-sex, iteration=1\n",
      "Load results for dataset road-safety-drivers-sex, iteration=1\n",
      "Load results for dataset road-safety-drivers-sex, iteration=1\n",
      "Fold no. 2\n",
      "Load results for dataset road-safety-drivers-sex, iteration=2\n",
      "Load results for dataset road-safety-drivers-sex, iteration=2\n",
      "Load results for dataset road-safety-drivers-sex, iteration=2\n",
      "Load results for dataset road-safety-drivers-sex, iteration=2\n",
      "Fold no. 3\n",
      "Load results for dataset road-safety-drivers-sex, iteration=3\n",
      "Load results for dataset road-safety-drivers-sex, iteration=3\n",
      "Load results for dataset road-safety-drivers-sex, iteration=3\n",
      "Load results for dataset road-safety-drivers-sex, iteration=3\n",
      "Fold no. 4\n",
      "Load results for dataset road-safety-drivers-sex, iteration=4\n",
      "Load results for dataset road-safety-drivers-sex, iteration=4\n",
      "Load results for dataset road-safety-drivers-sex, iteration=4\n",
      "Load results for dataset road-safety-drivers-sex, iteration=4\n",
      "Start training procedure for porto-seguro\n",
      "Fold no. 0\n",
      "Load results for dataset porto-seguro, iteration=0\n",
      "Load results for dataset porto-seguro, iteration=0\n",
      "Load results for dataset porto-seguro, iteration=0\n",
      "Load results for dataset porto-seguro, iteration=0\n",
      "Fold no. 1\n",
      "Load results for dataset porto-seguro, iteration=1\n",
      "Load results for dataset porto-seguro, iteration=1\n",
      "Load results for dataset porto-seguro, iteration=1\n",
      "Load results for dataset porto-seguro, iteration=1\n",
      "Fold no. 2\n",
      "Load results for dataset porto-seguro, iteration=2\n",
      "Load results for dataset porto-seguro, iteration=2\n",
      "Load results for dataset porto-seguro, iteration=2\n",
      "Load results for dataset porto-seguro, iteration=2\n",
      "Fold no. 3\n",
      "Load results for dataset porto-seguro, iteration=3\n",
      "Load results for dataset porto-seguro, iteration=3\n",
      "Load results for dataset porto-seguro, iteration=3\n",
      "Load results for dataset porto-seguro, iteration=3\n",
      "Fold no. 4\n",
      "Load results for dataset porto-seguro, iteration=4\n",
      "Load results for dataset porto-seguro, iteration=4\n",
      "Load results for dataset porto-seguro, iteration=4\n",
      "Load results for dataset porto-seguro, iteration=4\n"
     ]
    }
   ],
   "source": [
    "mode=\"cv\"\n",
    "hct=10\n",
    "test_ratio=None\n",
    "val_ratio=None\n",
    "folds=5\n",
    "results = {}\n",
    "dataset_names = [\"churn\", \"kdd_internet_usage\", \"Amazon_employee_access\", \"Click_prediction_small\", \"adult\", \"KDDCup09_upselling\", \"kick\", \"open_payments\", \"road-safety-drivers-sex\", \"porto-seguro\"]\n",
    "\n",
    "\n",
    "loss_use = lambda: tf.keras.losses.BinaryCrossentropy\n",
    "\n",
    "target= \"binary\"\n",
    "batch_size=512\n",
    "epochs = 500\n",
    "early_stopping = 20\n",
    "model_name = \"AutoGluon\"\n",
    "embed_dims_method = \"AutoGluon\"\n",
    "\n",
    "\n",
    "results = {}\n",
    "\n",
    "#######################################\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    print(f\"Start training procedure for {dataset_name}\")\n",
    "    data_path = f\"{mode}_RS{RS}_hct{hct}\"\n",
    "    if mode == \"cv\":\n",
    "        data_path += f\"_{folds}folds\"\n",
    "    elif mode == \"train_test\":\n",
    "        data_path += f\"_split{1-test_ratio*100}-{test_ratio*100}\"\n",
    "    elif mode == \"train_val_test\":\n",
    "        data_path += f\"_split{round(100-(test_ratio+val_ratio)*100)}-{round(test_ratio*100)}-{round(val_ratio*100)}\"\n",
    "\n",
    "    # If no data_dict exists, run preprocessing, else load data_dict\n",
    "    if not os.path.exists(f\"../data/prepared/{dataset_name}/\"+data_path+\"/data_dict.pickle\"):\n",
    "        dataset_preprocessing.process_dataset(dataset_name, target, mode, RS, hct, test_ratio, val_ratio, folds)\n",
    "    with open(f\"../data/prepared/{dataset_name}/{data_path}/data_dict.pickle\", 'rb') as handle:\n",
    "            data_dict = pickle.load(handle)\n",
    "\n",
    "    z_cols = data_dict[\"z_cols\"]\n",
    "    results[dataset_name] = {}\n",
    "    for fold_num in range(folds):\n",
    "        results[dataset_name][fold_num] = {}\n",
    "\n",
    "        print(f\"Fold no. {fold_num}\")\n",
    "        results[dataset_name][fold_num][\"histories\"] = {}\n",
    "        results[dataset_name][fold_num][\"predictions\"] = {}\n",
    "        results[dataset_name][fold_num][\"times\"] = {}\n",
    "        results[dataset_name][fold_num][\"other_info\"] = {}\n",
    "        for K,R in zip([1,2,2,5],[0,0,1,1]):\n",
    "            save_path = f\"../results/{dataset_name}/{data_path}/fold_{fold_num}/K\"+str(K)+\"_R\"+str(R)\n",
    "            if not os.path.exists(save_path):\n",
    "                os.makedirs(save_path)\n",
    "\n",
    "        \n",
    "            z_ohe_encoded_train = data_dict[f\"z_ohe_encoded_train_{fold_num}\"] \n",
    "            z_ohe_encoded_val = data_dict[f\"z_ohe_encoded_val_{fold_num}\"] \n",
    "            z_ohe_encoded_test = data_dict[f\"z_ohe_encoded_test_{fold_num}\"] \n",
    "\n",
    "            z_target_encoded_train = data_dict[f\"z_target_encoded_train_{fold_num}\"] \n",
    "            z_target_encoded_val = data_dict[f\"z_target_encoded_val_{fold_num}\"] \n",
    "            z_target_encoded_test = data_dict[f\"z_target_encoded_test_{fold_num}\"] \n",
    "\n",
    "            target_encoding_time = data_dict[f\"target_encoding_time_{fold_num}\"]\n",
    "            ohe_encoding_time = data_dict[f\"ohe_encoding_time_{fold_num}\"]\n",
    "\n",
    "            x_cols = data_dict[f\"X_train_{fold_num}\"].columns\n",
    "            X_train = data_dict[f\"X_train_{fold_num}\"]\n",
    "            Z_train = data_dict[f\"Z_train_{fold_num}\"]\n",
    "            y_train = data_dict[f\"y_train_{fold_num}\"]\n",
    "\n",
    "            X_val = data_dict[f\"X_val_{fold_num}\"]\n",
    "            Z_val = data_dict[f\"Z_val_{fold_num}\"]\n",
    "            y_val = data_dict[f\"y_val_{fold_num}\"]\n",
    "\n",
    "            X_test = data_dict[f\"X_test_{fold_num}\"]\n",
    "            Z_test = data_dict[f\"Z_test_{fold_num}\"]\n",
    "            y_test = data_dict[f\"y_test_{fold_num}\"]\n",
    "\n",
    "            if not os.path.exists(f\"{save_path}/results_RS{RS}_{dataset_name}_iter{fold_num}.pickle\"):\n",
    "\n",
    "                tf.random.set_seed(RS+fold_num)\n",
    "                np.random.seed(RS+fold_num)\n",
    "\n",
    "                qs = np.max([tf.reduce_max(Z_train, axis=0),tf.reduce_max(Z_val, axis=0),tf.reduce_max(Z_test, axis=0)],axis=0)+1\n",
    "\n",
    "                X_train = tf.convert_to_tensor(X_train)\n",
    "                Z_train = tf.convert_to_tensor(Z_train,dtype=tf.int32)\n",
    "                y_train = tf.convert_to_tensor(y_train)\n",
    "\n",
    "                X_val = tf.convert_to_tensor(X_val)\n",
    "                Z_val = tf.convert_to_tensor(Z_val,dtype=tf.int32)\n",
    "                y_val = tf.convert_to_tensor(y_val)\n",
    "\n",
    "                X_test = tf.convert_to_tensor(X_test)\n",
    "                Z_test = tf.convert_to_tensor(Z_test,dtype=tf.int32)\n",
    "                y_test = tf.convert_to_tensor(y_test)\n",
    "\n",
    "                if target == \"categorical\":\n",
    "                    n_classes = np.unique(y_train).shape[0]\n",
    "                elif target==\"binary\":\n",
    "                    n_classes = 1\n",
    "\n",
    "                y_train = tf.one_hot(tf.cast(y_train,tf.int32),n_classes)\n",
    "                y_val = tf.one_hot(tf.cast(y_val,tf.int32),n_classes)\n",
    "                y_test = tf.one_hot(tf.cast(y_test,tf.int32),n_classes)\n",
    "\n",
    "                ##### GMENN #####\n",
    "                d = X_train.shape[1] # columns\n",
    "                n = X_train.shape[0] # rows\n",
    "                num_outputs = n_classes\n",
    "                perc_numeric = d/(d+Z_train.shape[1])\n",
    "\n",
    "    #             qs = np.max([tf.reduce_max(Z_train, axis=0),tf.reduce_max(Z_val, axis=0),tf.reduce_max(Z_test, axis=0)],axis=0)+1\n",
    "\n",
    "                set_seed(RS)\n",
    "\n",
    "                fe_model, optimizer = get_model(model_name=model_name, input_size=X_train.shape[1], \n",
    "                                                  output_size=num_outputs, \n",
    "                                                  target=target, \n",
    "                                                  perc_numeric=perc_numeric, RS=RS)\n",
    "\n",
    "                initial_stds = np.ones([len(qs),num_outputs]).astype(float).tolist()\n",
    "\n",
    "                me_model = MixedEffectsNetwork(X_train, Z_train, y_train, fe_model, \n",
    "                                               target=target, qs=qs,\n",
    "                                               initial_stds=initial_stds,\n",
    "                                              fe_loss_weight=1.,\n",
    "                                               mode=\"intercepts\",\n",
    "                                               early_stopping_fe=early_stopping,\n",
    "                                              )    \n",
    "\n",
    "                me_model.compile(\n",
    "                    loss_class_me = loss_use()(),\n",
    "                    loss_class_fe = loss_use()(),\n",
    "                #     metric_class_me = tf.keras.metrics.AUC(multi_label=True, name=\"auc_me\"),\n",
    "                #     metric_class_fe = tf.keras.metrics.AUC(multi_label=True, name=\"auc_fe\"),\n",
    "                    optimizer=optimizer\n",
    "                )\n",
    "\n",
    "                mcmc = MCMCSamplingCallback(num_mcmc_samples=K,\n",
    "                                            perc_burnin=0.7,\n",
    "                                            warm_restart=None,\n",
    "                                            num_burnin_steps=R,\n",
    "                                            step_size = 0.1#initial_step_size,\n",
    "                                       )\n",
    "\n",
    "\n",
    "                print_metric = PrintMetrics(X_train, Z_train, y_train, X_val, Z_val, y_val)\n",
    "\n",
    "                start = time.time()\n",
    "                history = me_model.fit([X_train,Z_train], y_train,\n",
    "                             callbacks=[mcmc,\n",
    "                                        print_metric,\n",
    "                                        tf.keras.callbacks.EarlyStopping(monitor=\"me_auc_val\", patience=early_stopping, mode=\"max\")],\n",
    "                             epochs=epochs,\n",
    "                             validation_data=[[X_val,Z_val],y_val],\n",
    "                            batch_size=batch_size)\n",
    "\n",
    "                end = time.time()\n",
    "                fit_time_gmenn = round(end-start,2)\n",
    "\n",
    "                y_train_pred_gmenn, y_train_pred_gmenn_fe = me_model([X_train,Z_train])\n",
    "                y_val_pred_gmenn, y_val_pred_gmenn_fe = me_model([X_val,Z_val])\n",
    "                y_test_pred_gmenn, y_test_pred_gmenn_fe = me_model([X_test,Z_test])    \n",
    "\n",
    "\n",
    "                ###### Prepare NN Training ######\n",
    "\n",
    "\n",
    "\n",
    "                ##### Document Results #####\n",
    "\n",
    "                results[dataset_name][fold_num][\"histories\"][\"GMENN_K\"+str(K)+\"_R\"+str(R)] = history.history\n",
    "\n",
    "                results[dataset_name][fold_num][\"predictions\"][\"GMENN_K\"+str(K)+\"_R\"+str(R)] = [y_train_pred_gmenn, y_val_pred_gmenn, y_test_pred_gmenn]\n",
    "                \n",
    "                results[dataset_name][fold_num][\"times\"][\"GMENN_K\"+str(K)+\"_R\"+str(R)] = fit_time_gmenn\n",
    "\n",
    "                results[dataset_name][fold_num][\"other_info\"][\"GMENN_K\"+str(K)+\"_R\"+str(R)] = {\n",
    "                        \"_stddev_z\": np.array([i.numpy() for i in me_model.data_model._stddev_z]),\n",
    "                        \"acceptance_rates\": np.array(me_model.acceptance_rates),\n",
    "                        \"random_effects\": me_model.mean_samples,\n",
    "                        \"all_samples\": me_model.all_samples,\n",
    "                        \"stds\": me_model.stds\n",
    "                    }\n",
    "\n",
    "                with open(f\"{save_path}//results_RS{RS}_{dataset_name}_iter{fold_num}.pickle\", 'wb') as handle:\n",
    "                    pickle.dump(results[dataset_name][fold_num], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "                del X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "                gc.collect()\n",
    "            else:\n",
    "                print(f\"Load results for dataset {dataset_name}, iteration={fold_num}\")\n",
    "                with open(f\"{save_path}/results_RS{RS}_{dataset_name}_iter{fold_num}.pickle\", 'rb') as handle:\n",
    "                    res = pickle.load(handle)\n",
    "                results[dataset_name][fold_num][\"histories\"][\"GMENN_K\"+str(K)+\"_R\"+str(R)] = res[\"histories\"][\"GMENN_K\"+str(K)+\"_R\"+str(R)]\n",
    "                results[dataset_name][fold_num][\"predictions\"][\"GMENN_K\"+str(K)+\"_R\"+str(R)] = res[\"predictions\"][\"GMENN_K\"+str(K)+\"_R\"+str(R)]\n",
    "                results[dataset_name][fold_num][\"times\"][\"GMENN_K\"+str(K)+\"_R\"+str(R)] = res[\"times\"][\"GMENN_K\"+str(K)+\"_R\"+str(R)]\n",
    "                results[dataset_name][fold_num][\"other_info\"][\"GMENN_K\"+str(K)+\"_R\"+str(R)] = res[\"other_info\"][\"GMENN_K\"+str(K)+\"_R\"+str(R)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7825ce97",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03ecb737",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = [\"GMENN_K\"+str(K)+\"_R\"+str(R) for K,R in zip([1,2,2,5],[0,0,1,1])]\n",
    "results_perf = {dataset_name: {num: {model: {}  for model in models} for num in range(folds)} for dataset_name in dataset_names}\n",
    "for dataset_name in dataset_names:\n",
    "    try:\n",
    "        with open(f\"../data/prepared/{dataset_name}/{data_path}/data_dict.pickle\", 'rb') as handle:\n",
    "            data_dict = pickle.load(handle)        \n",
    "    except:\n",
    "        print(f\"dataset {dataset_name} not found\") \n",
    "    for num in range(folds):\n",
    "#         print(num)\n",
    "        n_classes=1\n",
    "        y_test = tf.one_hot(data_dict[f\"y_test_{num}\"],n_classes)\n",
    "        for model in models:\n",
    "            try:\n",
    "                y_pred = np.array(results[dataset_name][num][\"predictions\"][model][2]).ravel()\n",
    "\n",
    "                results_perf[dataset_name][num][model] = get_metrics(y_test,y_pred,target)\n",
    "                results_perf[dataset_name][num][model][\"Time\"] = results[dataset_name][num][\"times\"][model]\n",
    "            except:\n",
    "                print(f\"Set nan for {dataset_name}, {num}\")\n",
    "                results_perf[dataset_name][num][model] = {\"Accuracy\": np.nan,\n",
    "                                                          \"AUROC\": np.nan,\n",
    "                                                          \"F1\": np.nan,\n",
    "                                                          \"Time\": np.nan}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a874334b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GMENN_K1_R0</th>\n",
       "      <th>GMENN_K2_R0</th>\n",
       "      <th>GMENN_K2_R1</th>\n",
       "      <th>GMENN_K5_R1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>churn</th>\n",
       "      <td>0.88 (0.019)</td>\n",
       "      <td>0.88 (0.018)</td>\n",
       "      <td>0.88 (0.019)</td>\n",
       "      <td>0.88 (0.018)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kdd_internet_usage</th>\n",
       "      <td>0.94 (0.003)</td>\n",
       "      <td>0.94 (0.003)</td>\n",
       "      <td>0.94 (0.003)</td>\n",
       "      <td>0.94 (0.003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amazon_employee_access</th>\n",
       "      <td>0.84 (0.007)</td>\n",
       "      <td>0.85 (0.007)</td>\n",
       "      <td>0.85 (0.008)</td>\n",
       "      <td>0.85 (0.007)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Click_prediction_small</th>\n",
       "      <td>0.67 (0.013)</td>\n",
       "      <td>0.67 (0.014)</td>\n",
       "      <td>0.67 (0.013)</td>\n",
       "      <td>0.67 (0.013)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adult</th>\n",
       "      <td>0.91 (0.003)</td>\n",
       "      <td>0.91 (0.003)</td>\n",
       "      <td>0.91 (0.002)</td>\n",
       "      <td>0.91 (0.003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KDDCup09_upselling</th>\n",
       "      <td>0.8 (0.016)</td>\n",
       "      <td>0.8 (0.014)</td>\n",
       "      <td>0.8 (0.014)</td>\n",
       "      <td>0.8 (0.013)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kick</th>\n",
       "      <td>0.74 (0.014)</td>\n",
       "      <td>0.74 (0.009)</td>\n",
       "      <td>0.74 (0.013)</td>\n",
       "      <td>0.74 (0.013)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>open_payments</th>\n",
       "      <td>0.92 (0.009)</td>\n",
       "      <td>0.93 (0.008)</td>\n",
       "      <td>0.92 (0.008)</td>\n",
       "      <td>0.93 (0.006)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>road-safety-drivers-sex</th>\n",
       "      <td>0.73 (0.004)</td>\n",
       "      <td>0.73 (0.004)</td>\n",
       "      <td>0.73 (0.003)</td>\n",
       "      <td>0.73 (0.004)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>porto-seguro</th>\n",
       "      <td>0.56 (0.005)</td>\n",
       "      <td>0.56 (0.005)</td>\n",
       "      <td>0.56 (0.006)</td>\n",
       "      <td>0.56 (0.005)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          GMENN_K1_R0   GMENN_K2_R0   GMENN_K2_R1  \\\n",
       "churn                    0.88 (0.019)  0.88 (0.018)  0.88 (0.019)   \n",
       "kdd_internet_usage       0.94 (0.003)  0.94 (0.003)  0.94 (0.003)   \n",
       "Amazon_employee_access   0.84 (0.007)  0.85 (0.007)  0.85 (0.008)   \n",
       "Click_prediction_small   0.67 (0.013)  0.67 (0.014)  0.67 (0.013)   \n",
       "adult                    0.91 (0.003)  0.91 (0.003)  0.91 (0.002)   \n",
       "KDDCup09_upselling        0.8 (0.016)   0.8 (0.014)   0.8 (0.014)   \n",
       "kick                     0.74 (0.014)  0.74 (0.009)  0.74 (0.013)   \n",
       "open_payments            0.92 (0.009)  0.93 (0.008)  0.92 (0.008)   \n",
       "road-safety-drivers-sex  0.73 (0.004)  0.73 (0.004)  0.73 (0.003)   \n",
       "porto-seguro             0.56 (0.005)  0.56 (0.005)  0.56 (0.006)   \n",
       "\n",
       "                          GMENN_K5_R1  \n",
       "churn                    0.88 (0.018)  \n",
       "kdd_internet_usage       0.94 (0.003)  \n",
       "Amazon_employee_access   0.85 (0.007)  \n",
       "Click_prediction_small   0.67 (0.013)  \n",
       "adult                    0.91 (0.003)  \n",
       "KDDCup09_upselling        0.8 (0.013)  \n",
       "kick                     0.74 (0.013)  \n",
       "open_payments            0.93 (0.006)  \n",
       "road-safety-drivers-sex  0.73 (0.004)  \n",
       "porto-seguro             0.56 (0.005)  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [\"GMENN_K\"+str(K)+\"_R\"+str(R) for K,R in zip([1,2,2,5],[0,0,1,1])]\n",
    "metric = \"AUROC\"\n",
    "\n",
    "#####\n",
    "dataset_res_dict = {}\n",
    "best_models = {}\n",
    "t_test_results = {}\n",
    "\n",
    "round_mean_at = 2\n",
    "round_std_at = 3\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    dataset_models = list(results_perf[dataset_name][0].keys())\n",
    "    use_df = pd.DataFrame([pd.DataFrame(results_perf[dataset_name][fold_num]).loc[metric,models] for fold_num in results_perf[dataset_name].keys()],index=results_perf[dataset_name].keys())\n",
    "    \n",
    "    df_mean = pd.DataFrame(use_df.mean(axis=0).round(round_mean_at).astype(str) + \" (\" + use_df.std(axis=0).round(round_std_at).astype(str) + \")\").transpose()\n",
    "    model_dict = {i: df_mean[i].values[0] for i in df_mean.columns}\n",
    "    dataset_res_dict[dataset_name] = model_dict\n",
    "    \n",
    "    best_models[dataset_name] = use_df.columns[use_df.mean(axis=0).argmax()]\n",
    "\n",
    "    t_test_res = np.array([stats.ttest_rel(use_df[best_models[dataset_name]].values, use_df[model].values)[1] if model in dataset_models else 0 for model in models]).round(3)\n",
    "    t_test_res[np.isnan(t_test_res)] = 1.\n",
    "    t_test_results[dataset_name] = t_test_res\n",
    "    \n",
    "res_df = pd.DataFrame(dataset_res_dict).transpose()\n",
    "    \n",
    "def negative_bold(val):\n",
    "    i = np.where(val.name==np.array(models))[0][0]\n",
    "    return [\"font-weight: bold\"  if t_test_results[dataset_name][i]>=0.05 else \"\" for dataset_name in val.keys()]\n",
    "    # Case without transpose:\n",
    "#     return [\"font-weight: bold\"  if t_test_results[val.name][i]>=0.05 else \"\" for i in range(len(val))]\n",
    "\n",
    "# res_df.style.apply(negative_bold)\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfa98ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.88, 0.94, 0.84, 0.67, 0.91, 0.8 , 0.74, 0.92, 0.73, 0.56]),\n",
       " array([0.88, 0.94, 0.85, 0.67, 0.91, 0.8 , 0.74, 0.93, 0.73, 0.56]),\n",
       " array([0.88, 0.94, 0.85, 0.67, 0.91, 0.8 , 0.74, 0.92, 0.73, 0.56]),\n",
       " array([0.88, 0.94, 0.85, 0.67, 0.91, 0.8 , 0.74, 0.93, 0.73, 0.56]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df[\"GMENN_K1_R0\"].apply(lambda x: float(x.split(\" \")[0])).values,res_df[\"GMENN_K2_R0\"].apply(lambda x: float(x.split(\" \")[0])).values,res_df[\"GMENN_K2_R1\"].apply(lambda x: float(x.split(\" \")[0])).values,res_df[\"GMENN_K5_R1\"].apply(lambda x: float(x.split(\" \")[0])).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328d4e2d",
   "metadata": {},
   "source": [
    "### Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eea2f7b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GMENN_K1_R0</th>\n",
       "      <th>GMENN_K2_R0</th>\n",
       "      <th>GMENN_K2_R1</th>\n",
       "      <th>GMENN_K5_R1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>churn</th>\n",
       "      <td>1.94 (0.203)</td>\n",
       "      <td>2.43 (0.293)</td>\n",
       "      <td>1.17 (0.175)</td>\n",
       "      <td>3.45 (0.662)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kdd_internet_usage</th>\n",
       "      <td>1.82 (0.556)</td>\n",
       "      <td>3.18 (1.074)</td>\n",
       "      <td>3.18 (2.391)</td>\n",
       "      <td>8.05 (1.293)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amazon_employee_access</th>\n",
       "      <td>3.17 (1.517)</td>\n",
       "      <td>4.69 (1.806)</td>\n",
       "      <td>3.87 (2.649)</td>\n",
       "      <td>8.21 (4.259)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Click_prediction_small</th>\n",
       "      <td>3.06 (0.727)</td>\n",
       "      <td>3.92 (0.647)</td>\n",
       "      <td>2.51 (0.386)</td>\n",
       "      <td>11.62 (4.363)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adult</th>\n",
       "      <td>0.76 (0.081)</td>\n",
       "      <td>0.85 (0.11)</td>\n",
       "      <td>0.6 (0.043)</td>\n",
       "      <td>1.42 (0.117)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KDDCup09_upselling</th>\n",
       "      <td>3.96 (1.514)</td>\n",
       "      <td>5.74 (1.095)</td>\n",
       "      <td>6.34 (1.889)</td>\n",
       "      <td>15.06 (3.345)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kick</th>\n",
       "      <td>1.09 (0.087)</td>\n",
       "      <td>1.34 (0.108)</td>\n",
       "      <td>1.35 (0.155)</td>\n",
       "      <td>2.42 (0.3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>open_payments</th>\n",
       "      <td>2.32 (0.969)</td>\n",
       "      <td>3.64 (1.663)</td>\n",
       "      <td>2.26 (0.297)</td>\n",
       "      <td>10.78 (9.721)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>road-safety-drivers-sex</th>\n",
       "      <td>3.62 (0.518)</td>\n",
       "      <td>6.4 (2.075)</td>\n",
       "      <td>4.09 (0.975)</td>\n",
       "      <td>9.39 (1.155)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>porto-seguro</th>\n",
       "      <td>3.23 (0.168)</td>\n",
       "      <td>3.38 (0.113)</td>\n",
       "      <td>2.3 (0.102)</td>\n",
       "      <td>4.23 (0.565)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          GMENN_K1_R0   GMENN_K2_R0   GMENN_K2_R1  \\\n",
       "churn                    1.94 (0.203)  2.43 (0.293)  1.17 (0.175)   \n",
       "kdd_internet_usage       1.82 (0.556)  3.18 (1.074)  3.18 (2.391)   \n",
       "Amazon_employee_access   3.17 (1.517)  4.69 (1.806)  3.87 (2.649)   \n",
       "Click_prediction_small   3.06 (0.727)  3.92 (0.647)  2.51 (0.386)   \n",
       "adult                    0.76 (0.081)   0.85 (0.11)   0.6 (0.043)   \n",
       "KDDCup09_upselling       3.96 (1.514)  5.74 (1.095)  6.34 (1.889)   \n",
       "kick                     1.09 (0.087)  1.34 (0.108)  1.35 (0.155)   \n",
       "open_payments            2.32 (0.969)  3.64 (1.663)  2.26 (0.297)   \n",
       "road-safety-drivers-sex  3.62 (0.518)   6.4 (2.075)  4.09 (0.975)   \n",
       "porto-seguro             3.23 (0.168)  3.38 (0.113)   2.3 (0.102)   \n",
       "\n",
       "                           GMENN_K5_R1  \n",
       "churn                     3.45 (0.662)  \n",
       "kdd_internet_usage        8.05 (1.293)  \n",
       "Amazon_employee_access    8.21 (4.259)  \n",
       "Click_prediction_small   11.62 (4.363)  \n",
       "adult                     1.42 (0.117)  \n",
       "KDDCup09_upselling       15.06 (3.345)  \n",
       "kick                        2.42 (0.3)  \n",
       "open_payments            10.78 (9.721)  \n",
       "road-safety-drivers-sex   9.39 (1.155)  \n",
       "porto-seguro              4.23 (0.565)  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = \"Time\"\n",
    "\n",
    "#####\n",
    "dataset_res_dict = {}\n",
    "best_models = {}\n",
    "t_test_results = {}\n",
    "\n",
    "round_mean_at = 2\n",
    "round_std_at = 3\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    dataset_models = list(results_perf[dataset_name][0].keys())\n",
    "    use_df = pd.DataFrame([pd.DataFrame(results_perf[dataset_name][fold_num]).loc[metric,models] for fold_num in results_perf[dataset_name].keys()],index=results_perf[dataset_name].keys())/60\n",
    "    \n",
    "    df_mean = pd.DataFrame(use_df.mean(axis=0).round(round_mean_at).astype(str) + \" (\" + use_df.std(axis=0).round(round_std_at).astype(str) + \")\").transpose()\n",
    "    model_dict = {i: df_mean[i].values[0] for i in df_mean.columns}\n",
    "    dataset_res_dict[dataset_name] = model_dict\n",
    "    \n",
    "    best_models[dataset_name] = use_df.columns[use_df.mean(axis=0).argmin()]\n",
    "\n",
    "    t_test_res = np.array([stats.ttest_rel(use_df[best_models[dataset_name]].values, use_df[model].values)[1] if model in dataset_models else 0 for model in models]).round(3)\n",
    "    t_test_res[np.isnan(t_test_res)] = 1.\n",
    "    t_test_results[dataset_name] = t_test_res\n",
    "    \n",
    "res_df = pd.DataFrame(dataset_res_dict).transpose()\n",
    "    \n",
    "def negative_bold(val):\n",
    "    i = np.where(val.name==np.array(models))[0][0]\n",
    "    return [\"font-weight: bold\"  if t_test_results[dataset_name][i]>=0.05 else \"\" for dataset_name in val.keys()]\n",
    "    # Case without transpose:\n",
    "#     return [\"font-weight: bold\"  if t_test_results[val.name][i]>=0.05 else \"\" for i in range(len(val))]\n",
    "\n",
    "# res_df.style.apply(negative_bold)\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fb51381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.94, 1.82, 3.17, 3.06, 0.76, 3.96, 1.09, 2.32, 3.62, 3.23]),\n",
       " array([2.43, 3.18, 4.69, 3.92, 0.85, 5.74, 1.34, 3.64, 6.4 , 3.38]),\n",
       " array([1.17, 3.18, 3.87, 2.51, 0.6 , 6.34, 1.35, 2.26, 4.09, 2.3 ]),\n",
       " array([ 3.45,  8.05,  8.21, 11.62,  1.42, 15.06,  2.42, 10.78,  9.39,\n",
       "         4.23]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df[\"GMENN_K1_R0\"].apply(lambda x: float(x.split(\" \")[0])).values,res_df[\"GMENN_K2_R0\"].apply(lambda x: float(x.split(\" \")[0])).values,res_df[\"GMENN_K2_R1\"].apply(lambda x: float(x.split(\" \")[0])).values,res_df[\"GMENN_K5_R1\"].apply(lambda x: float(x.split(\" \")[0])).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28914dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cf47f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gmenn",
   "language": "python",
   "name": "gmenn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc07112e7ae1e8e28a0232207620ff002934c05692de8df42430404c766a0a8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
