{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca4ea20d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Found GPU: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# Append root path \n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../lmmnn\")\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"]=\"true\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "if tf.test.gpu_device_name() != '/device:GPU:0':\n",
    "    print('WARNING: GPU device not found.')\n",
    "else:\n",
    "    print('SUCCESS: Found GPU: {}'.format(tf.test.gpu_device_name()))\n",
    "\n",
    "from model.mixed_effects import *\n",
    "from utils.fe_models import get_model\n",
    "from utils.evaluation import *\n",
    "from utils.utils import *\n",
    "from data.preprocessing import dataset_preprocessing\n",
    "from utils.training_functions import *\n",
    "\n",
    "# from vis.utils.utils import apply_modifications\n",
    "# # helper function\n",
    "def update_layer_activation(model, activation, index=-1):\n",
    "    model.layers[index].activation = activation\n",
    "    return model\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Reshape, Embedding, Concatenate\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "\n",
    "from sklearn.metrics import accuracy_score as acc\n",
    "from sklearn.metrics import roc_auc_score as auroc\n",
    "from sklearn.metrics import f1_score as f1\n",
    "from sklearn.model_selection import train_test_split\n",
    "from category_encoders import TargetEncoder\n",
    "from tensorflow_addons.metrics import F1Score\n",
    "\n",
    "from scipy import stats\n",
    "import pickle\n",
    "import yaml\n",
    "import time\n",
    "import gc\n",
    "\n",
    "RS = 555"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "102d8696",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCMCSamplingCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self,\n",
    "                 num_mcmc_samples=1,\n",
    "                 step_size=0.01,\n",
    "                 perc_burnin=0.1,\n",
    "                 num_burnin_steps=0,\n",
    "                 warm_restart=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_mcmc_samples = tf.constant(num_mcmc_samples)\n",
    "        self.perc_burnin = perc_burnin\n",
    "        self.num_burnin_steps = num_burnin_steps\n",
    "        self.warm_restart = warm_restart\n",
    "        self.step_size = tf.Variable(step_size,trainable=False)\n",
    "        self.step_sizes = []\n",
    "\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.mcmc_kernel = tfp.mcmc.HamiltonianMonteCarlo(\n",
    "            target_log_prob_fn=self.model.target_log_prob_fn,\n",
    "            step_size=self.step_size, #0.15\n",
    "            num_leapfrog_steps=3\n",
    "        )\n",
    "\n",
    "        self.get_mcmc_kernel = lambda step_size: tfp.mcmc.HamiltonianMonteCarlo(\n",
    "            target_log_prob_fn=self.model.target_log_prob_fn,\n",
    "            num_leapfrog_steps=3,\n",
    "            step_size=step_size)\n",
    "\n",
    "        # self.mcmc_kernel = tfp.mcmc.DualAveragingStepSizeAdaptation(\n",
    "        #     inner_kernel=tfp.mcmc.NoUTurnSampler(\n",
    "        #         target_log_prob_fn=self.model.target_log_prob_fn,\n",
    "        #         step_size=self.step_size),\n",
    "        #     num_adaptation_steps=500,\n",
    "        #     target_accept_prob=0.651)\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch==0:\n",
    "            self.model.all_samples.extend(([[state[num] for state in self.model.current_state] for num in range(1)]))\n",
    "\n",
    "            self.model.mean_samples = [tf.reduce_mean([sample[q] for sample in self.model.all_samples[round(epoch*(self.perc_burnin)):]], axis=0) for q in\n",
    "                                       range(len(self.model.qs))]\n",
    "        if self.model.fe_pretraining:\n",
    "            if self.model.fe_converged:\n",
    "                self.run_sampling(epoch)\n",
    "            else:\n",
    "                self.model.acceptance_rates.append(-1)\n",
    "        else:\n",
    "            self.run_sampling(epoch)\n",
    "\n",
    "    def run_sampling(self,epoch):\n",
    "        self.model.fX.assign(self.model.fe_model(self.model.X, training=False))\n",
    "\n",
    "        if self.model.embed_x:\n",
    "            self.model.X_embedded.assign(self.model.X_embed_model(self.model.X, training=False))\n",
    "\n",
    "        if self.model.embed_z:\n",
    "            for q_num in range(len(self.model.qs)):\n",
    "                self.model.Z_embedded[q_num].assign(self.model.Z_embed_models[q_num](self.model.Z[:,q_num], training=False))\n",
    "\n",
    "                ## Find initial step size\n",
    "        # if self.model.previous_kernel_results.log_accept_ratio == -np.inf:\n",
    "        # if len(self.model.acceptance_rates)>0 and self.model.acceptance_rates[-1]<0.5:\n",
    "        if len(self.model.acceptance_rates)>0 and self.model.acceptance_rates[-1]<0.0001:\n",
    "            # self.mcmc_kernel.parameters[\"step_size\"] = self.mcmc_kernel.parameters[\"step_size\"]/2\n",
    "            # self.model.previous_kernel_results[\"new_step_size\"] = self.model.previous_kernel_results.step_size/2\n",
    "            # setattr(self.model.previous_kernel_results, \"new_step_size\", self.model.previous_kernel_results.step_size/2)\n",
    "            self.step_size.assign(self.step_size/2)\n",
    "            print(f\"Adapt step size to {float(self.step_size)}\")\n",
    "\n",
    "\n",
    "        if self.warm_restart!=None and epoch>0:\n",
    "            ## Warm restart\n",
    "            # if self.model.previous_kernel_results.log_accept_ratio == -np.inf:\n",
    "                # restart = True\n",
    "            # else:\n",
    "                # restart = False\n",
    "            # else:\n",
    "            restart = ((epoch + 1) % self.warm_restart) == 0 and epoch != 0\n",
    "\n",
    "            if restart:\n",
    "                print(\"\\n Warm restart to unstuck the chain\")\n",
    "                if self.model.embed_z and self.model.embed_x:\n",
    "                    self.model.current_state = self.model.data_model(self.model.fX, self.model.X_embedded, self.model.Z_embedded).sample(1, seed=self.model.RS)[:-1]\n",
    "                elif self.model.embed_z and not self.model.embed_x:\n",
    "                    self.model.current_state = self.model.data_model(self.model.fX, self.model.X, self.model.Z_embedded).sample(1, seed=self.model.RS)[:-1]\n",
    "                elif not self.model.embed_z and self.model.embed_x:\n",
    "                    self.model.current_state = self.model.data_model(self.model.fX, self.model.X_embedded, self.model.Z).sample(1, seed=self.model.RS)[:-1]\n",
    "                else:\n",
    "                    self.model.current_state = self.model.data_model(self.model.fX, self.model.X, self.model.Z).sample(1, seed=self.model.RS)[:-1]\n",
    "\n",
    "        print(\"\\n Start sampling for epoch {} of training\".format(epoch + 1))\n",
    "        start = time.time()\n",
    "        new_state, self.model.previous_kernel_results = self.get_mcmc_samples(self.model.current_state,\n",
    "                                                                              tf.constant(self.num_mcmc_samples),\n",
    "                                                                              None\n",
    "                                                                                               )\n",
    "        # self.model.divide_constants.assign(\n",
    "        #     list(1/np.mean(self.model.data_model._stddev_z,axis=1))+[1.])\n",
    "        # self.model.divide_constants.assign(\n",
    "        #     list((lambda x: 1+(x-x.mean()))(np.array(1+tf.math.softmax(1/len(self.model.qs)+0.5*tf.math.softmax(np.abs([np.mean(i) for i in self.model.previous_kernel_results.grads_target_log_prob]))))))+[1.])\n",
    "        # print(np.round(self.model.divide_constants,2))\n",
    "        try:\n",
    "            log_accept_ratio = self.model.previous_kernel_results.log_accept_ratio\n",
    "        except:\n",
    "            log_accept_ratio = self.model.previous_kernel_results.inner_results.log_accept_ratio\n",
    "        acceptance_rate = tf.math.exp(tf.minimum(log_accept_ratio, 0.))\n",
    "\n",
    "        self.step_sizes.append(float(self.step_size))\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        self.model.current_state = [tf.identity(i) for i in new_state]\n",
    "        # Todo: Append all current states\n",
    "        self.model.acceptance_rates.append(acceptance_rate)\n",
    "        # self.model.all_samples.append(\n",
    "        #     [tf.math.reduce_mean(self.model.current_state[q_num], axis=0) for q_num in range(len(self.model.qs))])\n",
    "        self.model.all_samples.extend(([[state[num] for state in self.model.current_state] for num in range(self.num_mcmc_samples)]))\n",
    "\n",
    "        self.model.mean_samples = [tf.reduce_mean([sample[q] for sample in self.model.all_samples[round(epoch*(self.perc_burnin)):]], axis=0) for q in\n",
    "                                   range(len(self.model.qs))]\n",
    "\n",
    "        self.model.e_step_times.append(round(end - start, 2))\n",
    "\n",
    "    # def on_epoch_end(self, epoch, logs=None):\n",
    "        for q_num in range(len(self.model.qs)):\n",
    "                self.model.data_model.trainable_variables[q_num].assign(\n",
    "                    tf.math.reduce_std(self.model.current_state[q_num][-1],axis=0))\n",
    "\n",
    "        self.model.stds.append([tf.identity(i) for i in self.model.data_model._stddev_z])\n",
    "\n",
    "    @tf.function(reduce_retracing=True)  # autograph=False, jit_compile=True, reduce_retracing=True)\n",
    "    def get_mcmc_samples(self, current_state, num_mcmc_samples=tf.constant(1), previous_kernel_results=None):\n",
    "        samples, _, previous_kernel_results = tfp.mcmc.sample_chain(\n",
    "            kernel=self.get_mcmc_kernel(self.step_size), num_results=num_mcmc_samples,\n",
    "            current_state=[state[-1] for state in current_state],\n",
    "            num_burnin_steps=self.num_burnin_steps,\n",
    "            trace_fn=None, previous_kernel_results=previous_kernel_results,\n",
    "            return_final_kernel_results=True, seed=self.model.RS)\n",
    "        #     current_state=[sample[-1] for sample in samples]\n",
    "\n",
    "        return samples, previous_kernel_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258caf10",
   "metadata": {},
   "source": [
    "#### Download and save data from Pargent et al. by running \"data/download_pargent2022_datasets.py before running this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "057f4663",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training procedure for churn\n",
      "Fold no. 0\n",
      "Load results for dataset churn, iteration=0\n",
      "Fold no. 1\n",
      "Load results for dataset churn, iteration=1\n",
      "Fold no. 2\n",
      "Load results for dataset churn, iteration=2\n",
      "Fold no. 3\n",
      "Load results for dataset churn, iteration=3\n",
      "Fold no. 4\n",
      "Load results for dataset churn, iteration=4\n",
      "Start training procedure for kdd_internet_usage\n",
      "Fold no. 0\n",
      "Load results for dataset kdd_internet_usage, iteration=0\n",
      "Fold no. 1\n",
      "Load results for dataset kdd_internet_usage, iteration=1\n",
      "Fold no. 2\n",
      "Load results for dataset kdd_internet_usage, iteration=2\n",
      "Fold no. 3\n",
      "Load results for dataset kdd_internet_usage, iteration=3\n",
      "Fold no. 4\n",
      "Load results for dataset kdd_internet_usage, iteration=4\n",
      "Start training procedure for Amazon_employee_access\n",
      "Fold no. 0\n",
      "Load results for dataset Amazon_employee_access, iteration=0\n",
      "Fold no. 1\n",
      "Load results for dataset Amazon_employee_access, iteration=1\n",
      "Fold no. 2\n",
      "Load results for dataset Amazon_employee_access, iteration=2\n",
      "Fold no. 3\n",
      "Load results for dataset Amazon_employee_access, iteration=3\n",
      "Fold no. 4\n",
      "Load results for dataset Amazon_employee_access, iteration=4\n",
      "Start training procedure for Click_prediction_small\n",
      "Fold no. 0\n",
      "Load results for dataset Click_prediction_small, iteration=0\n",
      "Fold no. 1\n",
      "Load results for dataset Click_prediction_small, iteration=1\n",
      "Fold no. 2\n",
      "Load results for dataset Click_prediction_small, iteration=2\n",
      "Fold no. 3\n",
      "Load results for dataset Click_prediction_small, iteration=3\n",
      "Fold no. 4\n",
      "Load results for dataset Click_prediction_small, iteration=4\n",
      "Start training procedure for adult\n",
      "Fold no. 0\n",
      "Load results for dataset adult, iteration=0\n",
      "Fold no. 1\n",
      "Load results for dataset adult, iteration=1\n",
      "Fold no. 2\n",
      "Load results for dataset adult, iteration=2\n",
      "Fold no. 3\n",
      "Load results for dataset adult, iteration=3\n",
      "Fold no. 4\n",
      "Load results for dataset adult, iteration=4\n",
      "Start training procedure for KDDCup09_upselling\n",
      "Fold no. 0\n",
      "Load results for dataset KDDCup09_upselling, iteration=0\n",
      "Fold no. 1\n",
      "Load results for dataset KDDCup09_upselling, iteration=1\n",
      "Fold no. 2\n",
      "Load results for dataset KDDCup09_upselling, iteration=2\n",
      "Fold no. 3\n",
      "Load results for dataset KDDCup09_upselling, iteration=3\n",
      "Fold no. 4\n",
      "Load results for dataset KDDCup09_upselling, iteration=4\n",
      "Start training procedure for kick\n",
      "Fold no. 0\n",
      "Load results for dataset kick, iteration=0\n",
      "Fold no. 1\n",
      "Load results for dataset kick, iteration=1\n",
      "Fold no. 2\n",
      "Load results for dataset kick, iteration=2\n",
      "Fold no. 3\n",
      "Load results for dataset kick, iteration=3\n",
      "Fold no. 4\n",
      "Load results for dataset kick, iteration=4\n",
      "Start training procedure for open_payments\n",
      "Fold no. 0\n",
      "Load results for dataset open_payments, iteration=0\n",
      "Fold no. 1\n",
      "Load results for dataset open_payments, iteration=1\n",
      "Fold no. 2\n",
      "Load results for dataset open_payments, iteration=2\n",
      "Fold no. 3\n",
      "Load results for dataset open_payments, iteration=3\n",
      "Fold no. 4\n",
      "Load results for dataset open_payments, iteration=4\n",
      "Start training procedure for road-safety-drivers-sex\n",
      "Fold no. 0\n",
      "Load results for dataset road-safety-drivers-sex, iteration=0\n",
      "Fold no. 1\n",
      "Load results for dataset road-safety-drivers-sex, iteration=1\n",
      "Fold no. 2\n",
      "Load results for dataset road-safety-drivers-sex, iteration=2\n",
      "Fold no. 3\n",
      "Load results for dataset road-safety-drivers-sex, iteration=3\n",
      "Fold no. 4\n",
      "Load results for dataset road-safety-drivers-sex, iteration=4\n",
      "Start training procedure for porto-seguro\n",
      "Fold no. 0\n",
      "Load results for dataset porto-seguro, iteration=0\n",
      "Fold no. 1\n",
      "Load results for dataset porto-seguro, iteration=1\n",
      "Fold no. 2\n",
      "Load results for dataset porto-seguro, iteration=2\n",
      "Fold no. 3\n",
      "Load results for dataset porto-seguro, iteration=3\n",
      "Fold no. 4\n",
      "Load results for dataset porto-seguro, iteration=4\n"
     ]
    }
   ],
   "source": [
    "mode=\"cv\"\n",
    "hct=10\n",
    "test_ratio=None\n",
    "val_ratio=None\n",
    "folds=5\n",
    "results = {}\n",
    "dataset_names = [\"churn\", \"kdd_internet_usage\", \"Amazon_employee_access\", \"Click_prediction_small\", \"adult\", \"KDDCup09_upselling\", \"kick\", \"open_payments\", \"road-safety-drivers-sex\", \"porto-seguro\"]\n",
    "\n",
    "\n",
    "loss_use = lambda: tf.keras.losses.BinaryCrossentropy\n",
    "\n",
    "target= \"binary\"\n",
    "batch_size=512\n",
    "epochs = 500\n",
    "early_stopping = 20\n",
    "model_name = \"AutoGluon\"\n",
    "embed_dims_method = \"AutoGluon\"\n",
    "\n",
    "\n",
    "results = {}\n",
    "\n",
    "#######################################\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    print(f\"Start training procedure for {dataset_name}\")\n",
    "    data_path = f\"{mode}_RS{RS}_hct{hct}\"\n",
    "    if mode == \"cv\":\n",
    "        data_path += f\"_{folds}folds\"\n",
    "    elif mode == \"train_test\":\n",
    "        data_path += f\"_split{1-test_ratio*100}-{test_ratio*100}\"\n",
    "    elif mode == \"train_val_test\":\n",
    "        data_path += f\"_split{round(100-(test_ratio+val_ratio)*100)}-{round(test_ratio*100)}-{round(val_ratio*100)}\"\n",
    "\n",
    "    # If no data_dict exists, run preprocessing, else load data_dict\n",
    "    if not os.path.exists(f\"../data/prepared/{dataset_name}/\"+data_path+\"/data_dict.pickle\"):\n",
    "        dataset_preprocessing.process_dataset(dataset_name, target, mode, RS, hct, test_ratio, val_ratio, folds)\n",
    "    with open(f\"../data/prepared/{dataset_name}/{data_path}/data_dict.pickle\", 'rb') as handle:\n",
    "            data_dict = pickle.load(handle)\n",
    "\n",
    "    z_cols = data_dict[\"z_cols\"]\n",
    "    results[dataset_name] = {}\n",
    "    for fold_num in range(folds):\n",
    "        results[dataset_name][fold_num] = {}\n",
    "\n",
    "        print(f\"Fold no. {fold_num}\")\n",
    "        save_path = f\"../results/{dataset_name}/{data_path}/fold_{fold_num}/HMC\"\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        \n",
    "        z_ohe_encoded_train = data_dict[f\"z_ohe_encoded_train_{fold_num}\"] \n",
    "        z_ohe_encoded_val = data_dict[f\"z_ohe_encoded_val_{fold_num}\"] \n",
    "        z_ohe_encoded_test = data_dict[f\"z_ohe_encoded_test_{fold_num}\"] \n",
    "\n",
    "        z_target_encoded_train = data_dict[f\"z_target_encoded_train_{fold_num}\"] \n",
    "        z_target_encoded_val = data_dict[f\"z_target_encoded_val_{fold_num}\"] \n",
    "        z_target_encoded_test = data_dict[f\"z_target_encoded_test_{fold_num}\"] \n",
    "        \n",
    "        target_encoding_time = data_dict[f\"target_encoding_time_{fold_num}\"]\n",
    "        ohe_encoding_time = data_dict[f\"ohe_encoding_time_{fold_num}\"]\n",
    "        \n",
    "        x_cols = data_dict[f\"X_train_{fold_num}\"].columns\n",
    "        X_train = data_dict[f\"X_train_{fold_num}\"]\n",
    "        Z_train = data_dict[f\"Z_train_{fold_num}\"]\n",
    "        y_train = data_dict[f\"y_train_{fold_num}\"]\n",
    "\n",
    "        X_val = data_dict[f\"X_val_{fold_num}\"]\n",
    "        Z_val = data_dict[f\"Z_val_{fold_num}\"]\n",
    "        y_val = data_dict[f\"y_val_{fold_num}\"]\n",
    "\n",
    "        X_test = data_dict[f\"X_test_{fold_num}\"]\n",
    "        Z_test = data_dict[f\"Z_test_{fold_num}\"]\n",
    "        y_test = data_dict[f\"y_test_{fold_num}\"]\n",
    "    \n",
    "        if not os.path.exists(f\"{save_path}/results_RS{RS}_{dataset_name}_iter{fold_num}.pickle\"):\n",
    "\n",
    "            tf.random.set_seed(RS+fold_num)\n",
    "            np.random.seed(RS+fold_num)\n",
    "\n",
    "            qs = np.max([tf.reduce_max(Z_train, axis=0),tf.reduce_max(Z_val, axis=0),tf.reduce_max(Z_test, axis=0)],axis=0)+1\n",
    "            \n",
    "            X_train = tf.convert_to_tensor(X_train)\n",
    "            Z_train = tf.convert_to_tensor(Z_train,dtype=tf.int32)\n",
    "            y_train = tf.convert_to_tensor(y_train)\n",
    "\n",
    "            X_val = tf.convert_to_tensor(X_val)\n",
    "            Z_val = tf.convert_to_tensor(Z_val,dtype=tf.int32)\n",
    "            y_val = tf.convert_to_tensor(y_val)\n",
    "\n",
    "            X_test = tf.convert_to_tensor(X_test)\n",
    "            Z_test = tf.convert_to_tensor(Z_test,dtype=tf.int32)\n",
    "            y_test = tf.convert_to_tensor(y_test)\n",
    "\n",
    "            if target == \"categorical\":\n",
    "                n_classes = np.unique(y_train).shape[0]\n",
    "            elif target==\"binary\":\n",
    "                n_classes = 1\n",
    "            \n",
    "            y_train = tf.one_hot(tf.cast(y_train,tf.int32),n_classes)\n",
    "            y_val = tf.one_hot(tf.cast(y_val,tf.int32),n_classes)\n",
    "            y_test = tf.one_hot(tf.cast(y_test,tf.int32),n_classes)\n",
    "            \n",
    "            ##### GMENN #####\n",
    "            d = X_train.shape[1] # columns\n",
    "            n = X_train.shape[0] # rows\n",
    "            num_outputs = n_classes\n",
    "            perc_numeric = d/(d+Z_train.shape[1])\n",
    "\n",
    "#             qs = np.max([tf.reduce_max(Z_train, axis=0),tf.reduce_max(Z_val, axis=0),tf.reduce_max(Z_test, axis=0)],axis=0)+1\n",
    "\n",
    "            set_seed(RS)\n",
    "\n",
    "            fe_model, optimizer = get_model(model_name=model_name, input_size=X_train.shape[1], \n",
    "                                              output_size=num_outputs, \n",
    "                                              target=target, \n",
    "                                              perc_numeric=perc_numeric, RS=RS)\n",
    "\n",
    "            initial_stds = np.ones([len(qs),num_outputs]).astype(float).tolist()\n",
    "\n",
    "            me_model = MixedEffectsNetwork(X_train, Z_train, y_train, fe_model, \n",
    "                                           target=target, qs=qs,\n",
    "                                           initial_stds=initial_stds,\n",
    "                                          fe_loss_weight=1.,\n",
    "                                           mode=\"intercepts\",\n",
    "                                           early_stopping_fe=early_stopping,\n",
    "                                          )    \n",
    "\n",
    "            me_model.compile(\n",
    "                loss_class_me = loss_use()(),\n",
    "                loss_class_fe = loss_use()(),\n",
    "            #     metric_class_me = tf.keras.metrics.AUC(multi_label=True, name=\"auc_me\"),\n",
    "            #     metric_class_fe = tf.keras.metrics.AUC(multi_label=True, name=\"auc_fe\"),\n",
    "                optimizer=optimizer\n",
    "            )\n",
    "\n",
    "            mcmc = MCMCSamplingCallback(num_mcmc_samples=1,\n",
    "                                        perc_burnin=0.7,\n",
    "                                        warm_restart=None,\n",
    "                                        num_burnin_steps=1,\n",
    "                                        step_size = 0.1#initial_step_size,\n",
    "                                   )\n",
    "\n",
    "\n",
    "            print_metric = PrintMetrics(X_train, Z_train, y_train, X_val, Z_val, y_val)\n",
    "\n",
    "            start = time.time()\n",
    "            history = me_model.fit([X_train,Z_train], y_train,\n",
    "                         callbacks=[mcmc,\n",
    "                                    print_metric,\n",
    "                                    tf.keras.callbacks.EarlyStopping(monitor=\"me_auc_val\", patience=early_stopping, mode=\"max\")],\n",
    "                         epochs=epochs,\n",
    "                         validation_data=[[X_val,Z_val],y_val],\n",
    "                        batch_size=batch_size)\n",
    "\n",
    "            end = time.time()\n",
    "            fit_time_gmenn = round(end-start,2)\n",
    "\n",
    "            y_train_pred_gmenn, y_train_pred_gmenn_fe = me_model([X_train,Z_train])\n",
    "            y_val_pred_gmenn, y_val_pred_gmenn_fe = me_model([X_val,Z_val])\n",
    "            y_test_pred_gmenn, y_test_pred_gmenn_fe = me_model([X_test,Z_test])    \n",
    "\n",
    "            \n",
    "            ###### Prepare NN Training ######\n",
    "#             metrics_use = []\n",
    "#             if target ==\"binary\":\n",
    "#                 metrics_use.append(tf.keras.metrics.AUC(name=\"auc\"))\n",
    "#                 metrics_use.append(tf.keras.metrics.Accuracy(name=\"accuracy\"))\n",
    "#                 metrics_use.append(F1Score(num_classes=2, average=\"micro\", name=\"f1\"))\n",
    "#                 stop_mode = \"max\"\n",
    "#                 activation_layer = tf.keras.activations.sigmoid\n",
    "#             elif target ==\"categorical\":\n",
    "#                 metrics_use.append(tf.keras.metrics.AUC(multi_label=True, name=\"auc\"))\n",
    "#                 metrics_use.append(tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"))\n",
    "#                 metrics_use.append(F1Score(num_classes=num_outputs, average=\"weighted\", name=\"f1\"))\n",
    "#                 stop_mode = \"max\"\n",
    "#                 activation_layer = tf.keras.activations.softmax\n",
    "#             elif target == \"continuous\":\n",
    "#                 metrics_use.append(RSquare(name=\"r2\"))\n",
    "#                 metrics_use.append(tf.keras.metrics.MeanSquaredError(name=\"mse\"))\n",
    "#                 stop_mode = \"min\"            \n",
    "            \n",
    "#             ##### Ignore #####\n",
    "#             model_nn, optimizer = get_model(model_name=model_name, \n",
    "#                                             input_size=X_train.shape[1], \n",
    "#                                             output_size=num_outputs, \n",
    "#                                             target=target, \n",
    "#                                             perc_numeric=perc_numeric, RS=RS)\n",
    "#             model_nn.build((n,d))\n",
    "#             update_layer_activation(model=model_nn, activation=activation_layer)\n",
    "\n",
    "#             model_nn.compile(loss=loss_use()(), optimizer=optimizer, metrics = metrics_use)\n",
    "\n",
    "#             callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_auc\", patience=early_stopping, mode=stop_mode)\n",
    "\n",
    "#             start = time.time()\n",
    "#             history_nn = model_nn.fit(X_train, y_train,\n",
    "#                          validation_data= [X_val, y_val],\n",
    "#                          epochs=epochs, batch_size=batch_size, callbacks=[callback])\n",
    "#             end = time.time()\n",
    "#             fit_time_nn = round(end-start,2)\n",
    "\n",
    "#             y_train_pred_nn = model_nn.predict(X_train ,batch_size=batch_size)\n",
    "#             y_val_pred_nn = model_nn.predict(X_val ,batch_size=batch_size)\n",
    "#             y_test_pred_nn = model_nn.predict(X_test ,batch_size=batch_size)\n",
    "\n",
    "#             if target == \"binary\":\n",
    "#                 eval_res_train_nn = get_metrics(y_train[:,0], y_train_pred_nn, target=target)\n",
    "#                 eval_res_val_nn = get_metrics(y_val[:,0], y_val_pred_nn, target=target)\n",
    "#                 eval_res_test_nn = get_metrics(y_test[:,0], y_test_pred_nn, target=target)\n",
    "#             elif target == \"categorical\":\n",
    "#                 eval_res_train_nn = get_metrics(y_train, y_train_pred_nn, target=target)\n",
    "#                 eval_res_val_nn = get_metrics(y_val, y_val_pred_nn, target=target)\n",
    "#                 eval_res_test_nn = get_metrics(y_test, y_test_pred_nn, target=target)\n",
    "\n",
    "#             ##### Target Encoding #####\n",
    "#             print(\"\\n Train Target Encoding Network\")\n",
    "#             model_nn_te, optimizer = get_model(model_name=model_name, \n",
    "#                                             input_size=np.append(X_train ,z_target_encoded_train, axis=1).shape[1], \n",
    "#                                             output_size=num_outputs, \n",
    "#                                             target=target, \n",
    "#                                             perc_numeric=perc_numeric, RS=RS)\n",
    "#             model_nn_te.build((n,np.append(X_train ,z_target_encoded_train, axis=1).shape[1]))\n",
    "#             update_layer_activation(model=model_nn_te, activation=activation_layer)\n",
    "#             model_nn_te.compile(loss=loss_use()(), optimizer=optimizer, metrics = metrics_use)\n",
    "#             callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_auc\", patience=early_stopping, mode=stop_mode)\n",
    "\n",
    "#             start = time.time()\n",
    "#             history_nn_te = model_nn_te.fit(np.append(X_train ,z_target_encoded_train, axis=1), y_train,\n",
    "#                          validation_data= [np.append(X_val ,z_target_encoded_val, axis=1), y_val],\n",
    "#                          epochs=epochs, batch_size=batch_size, callbacks=[callback])\n",
    "#             end = time.time()\n",
    "#             fit_time_te = round(end-start,2)+target_encoding_time\n",
    "\n",
    "#             y_train_pred_nn_te = model_nn_te.predict(np.append(X_train ,z_target_encoded_train, axis=1) ,batch_size=batch_size)\n",
    "#             y_val_pred_nn_te = model_nn_te.predict(np.append(X_val ,z_target_encoded_val, axis=1) ,batch_size=batch_size)\n",
    "#             y_test_pred_nn_te = model_nn_te.predict(np.append(X_test ,z_target_encoded_test, axis=1) ,batch_size=batch_size)\n",
    "\n",
    "#             if target == \"binary\":\n",
    "#                 eval_res_train_nn_te = get_metrics(y_train[:,0], y_train_pred_nn_te, target=target)\n",
    "#                 eval_res_val_nn_te = get_metrics(y_val[:,0], y_val_pred_nn_te, target=target)\n",
    "#                 eval_res_test_nn_te = get_metrics(y_test[:,0], y_test_pred_nn_te, target=target)\n",
    "#             elif target == \"categorical\":\n",
    "#                 eval_res_train_nn_te = get_metrics(y_train, y_train_pred_te, target=target)\n",
    "#                 eval_res_val_nn_te = get_metrics(y_val, y_val_pred_te, target=target)\n",
    "#                 eval_res_test_nn_te = get_metrics(y_test, y_test_pred_te, target=target)\n",
    "\n",
    " \n",
    "#             gc.collect()\n",
    "#             ##### OHE #####\n",
    "#             print(\"\\n Train OHE Network\")\n",
    "#             model_nn_ohe, optimizer = get_model(model_name=model_name, \n",
    "#                                             input_size=np.append(X_train ,z_ohe_encoded_train, axis=1).shape[1], \n",
    "#                                             output_size=num_outputs, \n",
    "#                                             target=target, \n",
    "#                                             perc_numeric=perc_numeric, RS=RS)\n",
    "#             model_nn_ohe.build((n,np.append(X_train ,z_ohe_encoded_train, axis=1).shape[1]))\n",
    "#             update_layer_activation(model=model_nn_ohe, activation=activation_layer)\n",
    "#             model_nn_ohe.compile(loss=loss_use()(), optimizer=optimizer, metrics = metrics_use)\n",
    "#             callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_auc\", patience=early_stopping, mode=stop_mode)\n",
    "\n",
    "#             start = time.time()\n",
    "#             history_nn_ohe = model_nn_ohe.fit(np.append(X_train ,z_ohe_encoded_train, axis=1), y_train,\n",
    "#                          validation_data= [np.append(X_val ,z_ohe_encoded_val, axis=1), y_val],\n",
    "#                          epochs=epochs, batch_size=batch_size, callbacks=[callback])\n",
    "#             end = time.time()\n",
    "#             fit_time_ohe = round(end-start,2)+ohe_encoding_time\n",
    "\n",
    "#             y_train_pred_nn_ohe = model_nn_ohe.predict(np.append(X_train ,z_ohe_encoded_train, axis=1), batch_size=batch_size)\n",
    "#             y_val_pred_nn_ohe = model_nn_ohe.predict(np.append(X_val ,z_ohe_encoded_val, axis=1), batch_size=batch_size)\n",
    "#             y_test_pred_nn_ohe = model_nn_ohe.predict(np.append(X_test ,z_ohe_encoded_test, axis=1), batch_size=batch_size)\n",
    "            \n",
    "#             if target == \"binary\":\n",
    "#                 eval_res_train_nn_ohe = get_metrics(y_train[:,0], y_train_pred_nn_ohe, target=target)\n",
    "#                 eval_res_val_nn_ohe = get_metrics(y_val[:,0], y_val_pred_nn_ohe, target=target)\n",
    "#                 eval_res_test_nn_ohe = get_metrics(y_test[:,0], y_test_pred_nn_ohe, target=target)            \n",
    "#             elif target == \"categorical\":\n",
    "#                 eval_res_train_nn_ohe = get_metrics(y_train, y_train_pred_nn_ohe, target=target)\n",
    "#                 eval_res_val_nn_ohe = get_metrics(y_val, y_val_pred_nn_ohe, target=target)\n",
    "#                 eval_res_test_nn_ohe = get_metrics(y_test, y_test_pred_nn_ohe, target=target)\n",
    "#             gc.collect()   \n",
    "#             ##### Embedding #####\n",
    "#             print(\"\\n Embedding Estimate Network\")\n",
    "\n",
    "#             if embed_dims_method==\"sqrt\":\n",
    "#                 embed_dims = [int(np.sqrt(q)) for q in qs]\n",
    "#             elif embed_dims_method==\"AutoGluon\":\n",
    "#                 embed_dims = [int(np.max([100, np.round(1.6*q**0.56)])) for q in qs]\n",
    "#             else:\n",
    "#                 embed_dims = [10 for q in qs]\n",
    "\n",
    "#             input_layer = Input(shape=(d,))\n",
    "\n",
    "#             # Define embedding layers\n",
    "#             embed_inputs = []\n",
    "#             embedding_layers = []\n",
    "#             for q_num in range(len(qs)):\n",
    "#                 Z_input_layer = Input(shape=(1,))\n",
    "#                 embedding_layer = Embedding(qs[q_num], embed_dims[q_num], input_length=1)(Z_input_layer)\n",
    "#                 embedding_layer = Reshape(target_shape=(embed_dims[q_num],))(embedding_layer)\n",
    "\n",
    "#                 embed_inputs.append(Z_input_layer)\n",
    "#                 embedding_layers.append(embedding_layer)\n",
    "\n",
    "#             ### Get model layer dimensions\n",
    "#             min_numeric_embed_dim = 32\n",
    "#             max_numeric_embed_dim = 2056\n",
    "#             max_layer_width = 2056\n",
    "#             # Main dense model\n",
    "#             if target == \"continuous\":\n",
    "#                 default_layer_sizes = [256,\n",
    "#                                        128]  # overall network will have 4 layers. Input layer, 256-unit hidden layer, 128-unit hidden layer, output layer.\n",
    "#             else:\n",
    "#                 default_sizes = [256, 128]  # will be scaled adaptively\n",
    "#                 # base_size = max(1, min(num_net_outputs, 20)/2.0) # scale layer width based on number of classes\n",
    "#                 base_size = max(1, min(num_outputs,\n",
    "#                                        100) / 50)  # TODO: Updated because it improved model quality and made training far faster\n",
    "#                 default_layer_sizes = [defaultsize * base_size for defaultsize in default_sizes]\n",
    "#             layer_expansion_factor = 1  # TODO: consider scaling based on num_rows, eg: layer_expansion_factor = 2-np.exp(-max(0,train_dataset.num_examples-10000))\n",
    "#             first_layer_width = int(min(max_layer_width, layer_expansion_factor * default_layer_sizes[0]))\n",
    "\n",
    "#             # numeric embed dim\n",
    "#             vector_dim = 0  # total dimensionality of vector features (I think those should be transformed string features, which we don't have)\n",
    "#             prop_vector_features = perc_numeric  # Fraction of features that are numeric\n",
    "#             numeric_embedding_size = int(min(max_numeric_embed_dim,\n",
    "#                                              max(min_numeric_embed_dim,\n",
    "#                                                  first_layer_width * prop_vector_features * np.log10(vector_dim + 10))))\n",
    "\n",
    "\n",
    "#             numeric_embedding = Dense(numeric_embedding_size, activation=\"relu\")(input_layer)\n",
    "\n",
    "#             concat = Concatenate()([numeric_embedding] + embedding_layers)\n",
    "\n",
    "#             base_model, optimizer = get_model(model_name=model_name, \n",
    "#                                               input_size=numeric_embedding_size + sum(embed_dims), \n",
    "#                                               output_size=num_outputs, target=target,\n",
    "#                                               perc_numeric=perc_numeric, RS=RS)\n",
    "\n",
    "#             base_model.build((n, numeric_embedding_size + sum(embed_dims)))\n",
    "#             update_layer_activation(model=base_model, activation=activation_layer)\n",
    "\n",
    "#             layers = base_model(concat)\n",
    "\n",
    "#             model_embed = Model(inputs=[input_layer] + embed_inputs, outputs=layers)\n",
    "\n",
    "\n",
    "#             model_embed.compile(loss=loss_use()(), optimizer=optimizer, metrics = metrics_use)\n",
    "#             callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_auc\", patience=early_stopping, mode=stop_mode)\n",
    "\n",
    "#             start = time.time()\n",
    "#             history_nn_embed = model_embed.fit([X_train] + [Z_train[: ,q_num] for q_num in range(len(qs))], y_train,\n",
    "#                             validation_data=[[X_val] + [Z_val[: ,q_num] for q_num in range(len(qs))], y_val],\n",
    "#                             epochs=epochs, batch_size=batch_size, callbacks=[callback])\n",
    "#             end = time.time()\n",
    "#             fit_time_embed = round(end-start,2)\n",
    "\n",
    "#             y_train_pred_embed = model_embed.predict([X_train] + [Z_train[: ,q_num] for q_num in range(len(qs))]\n",
    "#                                                      ,batch_size=batch_size)\n",
    "#             y_val_pred_embed = model_embed.predict([X_val] + [Z_val[: ,q_num] for q_num in range(len(qs))]\n",
    "#                                                     ,batch_size=batch_size)\n",
    "#             y_test_pred_embed = model_embed.predict([X_test] + [Z_test[: ,q_num] for q_num in range(len(qs))]\n",
    "#                                                     ,batch_size=batch_size)\n",
    "\n",
    "#             if target == \"binary\":\n",
    "#                 eval_res_train_embed = get_metrics(y_train[:,0], y_train_pred_embed, target=target)\n",
    "#                 eval_res_val_embed = get_metrics(y_val[:,0], y_val_pred_embed, target=target)\n",
    "#                 eval_res_test_embed = get_metrics(y_test[:,0], y_test_pred_embed, target=target)\n",
    "#             elif target == \"categorical\":\n",
    "#                 eval_res_train_embed = get_metrics(y_train, y_train_pred_embed, target=target)\n",
    "#                 eval_res_val_embed = get_metrics(y_val, y_val_pred_embed, target=target)\n",
    "#                 eval_res_test_embed = get_metrics(y_test, y_test_pred_embed, target=target)\n",
    "\n",
    "#             eval_res_train_embed, eval_res_test_embed        \n",
    "\n",
    "\n",
    "            ##### Document Results #####\n",
    "            \n",
    "            results[dataset_name][fold_num][\"histories\"] = {\"GMENN\": history.history,\n",
    "#                                                        \"Ignore\": history_nn.history,\n",
    "#                                                        \"TE\": history_nn_te.history,\n",
    "# #                                                        \"GLMM\": history_nn_glmm.history,\n",
    "#                                                        \"OHE\": history_nn_ohe.history,\n",
    "#                                                        \"Embedding\": history_nn_embed.history,\n",
    "#                                                        \"Point\": history_nn_point.history,\n",
    "#                                                        \"LMMNN\": [],\n",
    "#                                                        \"ARMED\": history_armed.history,\n",
    "#                                                        \"ARMED (no adv.)\": history_armed_noadv.history,\n",
    "                                                      }\n",
    "            \n",
    "            results[dataset_name][fold_num][\"predictions\"] = {\"GMENN\": [y_train_pred_gmenn, y_val_pred_gmenn, y_test_pred_gmenn],\n",
    "                                                        \"GMENN (FE)\": [y_train_pred_gmenn_fe, y_val_pred_gmenn_fe, y_test_pred_gmenn_fe],\n",
    "#                                                         \"Ignore\": [y_train_pred_nn, y_val_pred_nn, y_test_pred_nn],\n",
    "#                                                         \"TE\": [y_train_pred_nn_te, y_val_pred_nn_te, y_test_pred_nn_te],\n",
    "# #                                                         \"GLMM\": [y_train_pred_nn_glmm, y_val_pred_nn_glmm, y_test_pred_nn_glmm],\n",
    "#                                                         \"OHE\": [y_train_pred_nn_ohe, y_val_pred_nn_ohe, y_test_pred_nn_ohe],\n",
    "#                                                         \"Embedding\": [y_train_pred_embed, y_val_pred_embed, y_test_pred_embed],\n",
    "#                                                         \"Point\": [y_train_pred_point, y_val_pred_point, y_test_pred_point],\n",
    "#                                                         \"Point (FE)\": [y_train_pred_point_fe, y_val_pred_point_fe, y_test_pred_point_fe],\n",
    "#                                                         \"LMMNN\": [y_train_pred_lmmnn, y_val_pred_lmmnn, y_test_pred_lmmnn],\n",
    "#                                                         \"LMMNN (FE)\": [y_train_pred_lmmnn_fe, y_val_pred_lmmnn_fe, y_test_pred_lmmnn_fe],\n",
    "#                                                         \"ARMED\": [train_pred_armed, val_pred_armed, test_pred_armed],\n",
    "#                                                         \"ARMED (FE)\": [train_pred_armed_fe, val_pred_armed_fe, test_pred_armed_fe],\n",
    "#                                                         \"ARMED (no adv.)\": [train_pred_armed_noadv, val_pred_armed_noadv, test_pred_armed_noadv],\n",
    "#                                                         \"ARMED (no adv.) (FE)\": [train_pred_armed_noadv_fe, val_pred_armed_noadv_fe, test_pred_armed_noadv_fe],\n",
    "                                                      }\n",
    "            \n",
    "            results[dataset_name][fold_num][\"times\"] = {\"GMENN\": fit_time_gmenn,\n",
    "#                                                    \"Ignore\": fit_time_nn,\n",
    "#                                                    \"TE\": fit_time_te,\n",
    "# #                                                    \"GLMM\": fit_time_glmm,\n",
    "#                                                    \"OHE\": fit_time_ohe,\n",
    "#                                                    \"Embedding\": fit_time_embed,\n",
    "#                                                    \"Point\": fit_time_point,\n",
    "#                                                    \"LMMNN\": fit_time_lmmnn,\n",
    "#                                                    \"ARMED\": fit_time_armed,\n",
    "#                                                    \"ARMED (no adv.)\": fit_time_armed_noadv,\n",
    "                                                      }\n",
    "            \n",
    "            results[dataset_name][fold_num][\"other_info\"] = {\n",
    "                \"GMENN\": {\n",
    "                    \"_stddev_z\": np.array([i.numpy() for i in me_model.data_model._stddev_z]),\n",
    "                    \"acceptance_rates\": np.array(me_model.acceptance_rates),\n",
    "                    \"random_effects\": me_model.mean_samples,\n",
    "                    \"all_samples\": me_model.all_samples,\n",
    "                    \"stds\": me_model.stds\n",
    "                },\n",
    "#                 \"LMMNN\": {\n",
    "#                     \"_stddev_z\": sigmas,\n",
    "#                     \"random_effects\": b_hats,\n",
    "#                 },\n",
    "#                 \"ARMED\": {\n",
    "#                     \"_stddev_z\": np.std(model_armed.randomeffects.re_int.weights[0].numpy()[:qs[0]]),\n",
    "#                     \"random_effects\": model_armed.randomeffects.re_int.weights[0].numpy()[:qs[0]],\n",
    "#                     \"pred_cluster\": [train_pred_cluster_armed,val_pred_cluster_armed,test_pred_cluster_armed]\n",
    "#                 },\n",
    "#                 \"ARMED (no adv.)\": {\n",
    "#                     \"_stddev_z\": np.std(model_armed_noadv.randomeffects.re_int.weights[0].numpy()[:qs[0]]),\n",
    "#                     \"random_effects\": model_armed_noadv.randomeffects.re_int.weights[0].numpy()[:qs[0]],\n",
    "#                     \"pred_cluster\": [train_pred_cluster_armed_noadv,val_pred_cluster_armed_noadv,test_pred_cluster_armed_noadv]\n",
    "#                 }\n",
    "            }\n",
    "            \n",
    "#             del model_lmmnn, model_lmmnn_fe\n",
    "            \n",
    "            with open(f\"{save_path}//results_RS{RS}_{dataset_name}_iter{fold_num}.pickle\", 'wb') as handle:\n",
    "                pickle.dump(results[dataset_name][fold_num], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "            \n",
    "            del X_train, X_val, X_test, y_train, y_val, y_test\n",
    "#             del z_target_encoded_train, z_target_encoded_val, z_target_encoded_test\n",
    "#             del z_ohe_encoded_train, z_ohe_encoded_val, z_ohe_encoded_test\n",
    "#             del z_glmm_encoded_train, z_glmm_encoded_val, z_glmm_encoded_test\n",
    "            \n",
    "            gc.collect()\n",
    "        else:\n",
    "            print(f\"Load results for dataset {dataset_name}, iteration={fold_num}\")\n",
    "            with open(f\"{save_path}/results_RS{RS}_{dataset_name}_iter{fold_num}.pickle\", 'rb') as handle:\n",
    "                results[dataset_name][fold_num] = pickle.load(handle)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95cdc157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['churn', 'kdd_internet_usage', 'Amazon_employee_access', 'Click_prediction_small', 'adult', 'KDDCup09_upselling', 'kick', 'open_payments', 'road-safety-drivers-sex', 'porto-seguro'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7825ce97",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03ecb737",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set nan for churn, 0\n",
      "Set nan for churn, 0\n",
      "Set nan for churn, 0\n",
      "Set nan for churn, 0\n",
      "Set nan for churn, 1\n",
      "Set nan for churn, 1\n",
      "Set nan for churn, 1\n",
      "Set nan for churn, 1\n",
      "Set nan for churn, 2\n",
      "Set nan for churn, 2\n",
      "Set nan for churn, 2\n",
      "Set nan for churn, 2\n",
      "Set nan for churn, 3\n",
      "Set nan for churn, 3\n",
      "Set nan for churn, 3\n",
      "Set nan for churn, 3\n",
      "Set nan for churn, 4\n",
      "Set nan for churn, 4\n",
      "Set nan for churn, 4\n",
      "Set nan for churn, 4\n",
      "Set nan for kdd_internet_usage, 0\n",
      "Set nan for kdd_internet_usage, 0\n",
      "Set nan for kdd_internet_usage, 0\n",
      "Set nan for kdd_internet_usage, 0\n",
      "Set nan for kdd_internet_usage, 1\n",
      "Set nan for kdd_internet_usage, 1\n",
      "Set nan for kdd_internet_usage, 1\n",
      "Set nan for kdd_internet_usage, 1\n",
      "Set nan for kdd_internet_usage, 2\n",
      "Set nan for kdd_internet_usage, 2\n",
      "Set nan for kdd_internet_usage, 2\n",
      "Set nan for kdd_internet_usage, 2\n",
      "Set nan for kdd_internet_usage, 3\n",
      "Set nan for kdd_internet_usage, 3\n",
      "Set nan for kdd_internet_usage, 3\n",
      "Set nan for kdd_internet_usage, 3\n",
      "Set nan for kdd_internet_usage, 4\n",
      "Set nan for kdd_internet_usage, 4\n",
      "Set nan for kdd_internet_usage, 4\n",
      "Set nan for kdd_internet_usage, 4\n",
      "Set nan for Amazon_employee_access, 0\n",
      "Set nan for Amazon_employee_access, 0\n",
      "Set nan for Amazon_employee_access, 0\n",
      "Set nan for Amazon_employee_access, 0\n",
      "Set nan for Amazon_employee_access, 1\n",
      "Set nan for Amazon_employee_access, 1\n",
      "Set nan for Amazon_employee_access, 1\n",
      "Set nan for Amazon_employee_access, 1\n",
      "Set nan for Amazon_employee_access, 2\n",
      "Set nan for Amazon_employee_access, 2\n",
      "Set nan for Amazon_employee_access, 2\n",
      "Set nan for Amazon_employee_access, 2\n",
      "Set nan for Amazon_employee_access, 3\n",
      "Set nan for Amazon_employee_access, 3\n",
      "Set nan for Amazon_employee_access, 3\n",
      "Set nan for Amazon_employee_access, 3\n",
      "Set nan for Amazon_employee_access, 4\n",
      "Set nan for Amazon_employee_access, 4\n",
      "Set nan for Amazon_employee_access, 4\n",
      "Set nan for Amazon_employee_access, 4\n",
      "Set nan for Click_prediction_small, 0\n",
      "Set nan for Click_prediction_small, 0\n",
      "Set nan for Click_prediction_small, 0\n",
      "Set nan for Click_prediction_small, 0\n",
      "Set nan for Click_prediction_small, 1\n",
      "Set nan for Click_prediction_small, 1\n",
      "Set nan for Click_prediction_small, 1\n",
      "Set nan for Click_prediction_small, 1\n",
      "Set nan for Click_prediction_small, 2\n",
      "Set nan for Click_prediction_small, 2\n",
      "Set nan for Click_prediction_small, 2\n",
      "Set nan for Click_prediction_small, 2\n",
      "Set nan for Click_prediction_small, 3\n",
      "Set nan for Click_prediction_small, 3\n",
      "Set nan for Click_prediction_small, 3\n",
      "Set nan for Click_prediction_small, 3\n",
      "Set nan for Click_prediction_small, 4\n",
      "Set nan for Click_prediction_small, 4\n",
      "Set nan for Click_prediction_small, 4\n",
      "Set nan for Click_prediction_small, 4\n",
      "Set nan for adult, 0\n",
      "Set nan for adult, 0\n",
      "Set nan for adult, 0\n",
      "Set nan for adult, 0\n",
      "Set nan for adult, 1\n",
      "Set nan for adult, 1\n",
      "Set nan for adult, 1\n",
      "Set nan for adult, 1\n",
      "Set nan for adult, 2\n",
      "Set nan for adult, 2\n",
      "Set nan for adult, 2\n",
      "Set nan for adult, 2\n",
      "Set nan for adult, 3\n",
      "Set nan for adult, 3\n",
      "Set nan for adult, 3\n",
      "Set nan for adult, 3\n",
      "Set nan for adult, 4\n",
      "Set nan for adult, 4\n",
      "Set nan for adult, 4\n",
      "Set nan for adult, 4\n",
      "Set nan for KDDCup09_upselling, 0\n",
      "Set nan for KDDCup09_upselling, 0\n",
      "Set nan for KDDCup09_upselling, 0\n",
      "Set nan for KDDCup09_upselling, 0\n",
      "Set nan for KDDCup09_upselling, 1\n",
      "Set nan for KDDCup09_upselling, 1\n",
      "Set nan for KDDCup09_upselling, 1\n",
      "Set nan for KDDCup09_upselling, 1\n",
      "Set nan for KDDCup09_upselling, 2\n",
      "Set nan for KDDCup09_upselling, 2\n",
      "Set nan for KDDCup09_upselling, 2\n",
      "Set nan for KDDCup09_upselling, 2\n",
      "Set nan for KDDCup09_upselling, 3\n",
      "Set nan for KDDCup09_upselling, 3\n",
      "Set nan for KDDCup09_upselling, 3\n",
      "Set nan for KDDCup09_upselling, 3\n",
      "Set nan for KDDCup09_upselling, 4\n",
      "Set nan for KDDCup09_upselling, 4\n",
      "Set nan for KDDCup09_upselling, 4\n",
      "Set nan for KDDCup09_upselling, 4\n",
      "Set nan for kick, 0\n",
      "Set nan for kick, 0\n",
      "Set nan for kick, 0\n",
      "Set nan for kick, 0\n",
      "Set nan for kick, 1\n",
      "Set nan for kick, 1\n",
      "Set nan for kick, 1\n",
      "Set nan for kick, 1\n",
      "Set nan for kick, 2\n",
      "Set nan for kick, 2\n",
      "Set nan for kick, 2\n",
      "Set nan for kick, 2\n",
      "Set nan for kick, 3\n",
      "Set nan for kick, 3\n",
      "Set nan for kick, 3\n",
      "Set nan for kick, 3\n",
      "Set nan for kick, 4\n",
      "Set nan for kick, 4\n",
      "Set nan for kick, 4\n",
      "Set nan for kick, 4\n",
      "Set nan for open_payments, 0\n",
      "Set nan for open_payments, 0\n",
      "Set nan for open_payments, 0\n",
      "Set nan for open_payments, 0\n",
      "Set nan for open_payments, 1\n",
      "Set nan for open_payments, 1\n",
      "Set nan for open_payments, 1\n",
      "Set nan for open_payments, 1\n",
      "Set nan for open_payments, 2\n",
      "Set nan for open_payments, 2\n",
      "Set nan for open_payments, 2\n",
      "Set nan for open_payments, 2\n",
      "Set nan for open_payments, 3\n",
      "Set nan for open_payments, 3\n",
      "Set nan for open_payments, 3\n",
      "Set nan for open_payments, 3\n",
      "Set nan for open_payments, 4\n",
      "Set nan for open_payments, 4\n",
      "Set nan for open_payments, 4\n",
      "Set nan for open_payments, 4\n",
      "Set nan for road-safety-drivers-sex, 0\n",
      "Set nan for road-safety-drivers-sex, 0\n",
      "Set nan for road-safety-drivers-sex, 0\n",
      "Set nan for road-safety-drivers-sex, 0\n",
      "Set nan for road-safety-drivers-sex, 1\n",
      "Set nan for road-safety-drivers-sex, 1\n",
      "Set nan for road-safety-drivers-sex, 1\n",
      "Set nan for road-safety-drivers-sex, 1\n",
      "Set nan for road-safety-drivers-sex, 2\n",
      "Set nan for road-safety-drivers-sex, 2\n",
      "Set nan for road-safety-drivers-sex, 2\n",
      "Set nan for road-safety-drivers-sex, 2\n",
      "Set nan for road-safety-drivers-sex, 3\n",
      "Set nan for road-safety-drivers-sex, 3\n",
      "Set nan for road-safety-drivers-sex, 3\n",
      "Set nan for road-safety-drivers-sex, 3\n",
      "Set nan for road-safety-drivers-sex, 4\n",
      "Set nan for road-safety-drivers-sex, 4\n",
      "Set nan for road-safety-drivers-sex, 4\n",
      "Set nan for road-safety-drivers-sex, 4\n",
      "Set nan for porto-seguro, 0\n",
      "Set nan for porto-seguro, 0\n",
      "Set nan for porto-seguro, 0\n",
      "Set nan for porto-seguro, 0\n",
      "Set nan for porto-seguro, 1\n",
      "Set nan for porto-seguro, 1\n",
      "Set nan for porto-seguro, 1\n",
      "Set nan for porto-seguro, 1\n",
      "Set nan for porto-seguro, 2\n",
      "Set nan for porto-seguro, 2\n",
      "Set nan for porto-seguro, 2\n",
      "Set nan for porto-seguro, 2\n",
      "Set nan for porto-seguro, 3\n",
      "Set nan for porto-seguro, 3\n",
      "Set nan for porto-seguro, 3\n",
      "Set nan for porto-seguro, 3\n",
      "Set nan for porto-seguro, 4\n",
      "Set nan for porto-seguro, 4\n",
      "Set nan for porto-seguro, 4\n",
      "Set nan for porto-seguro, 4\n"
     ]
    }
   ],
   "source": [
    "models = [\"GMENN\", \"TE\", \"OHE\", \"Embedding\", \"Ignore\"]\n",
    "results_perf = {dataset_name: {num: {model: {}  for model in models} for num in range(folds)} for dataset_name in dataset_names}\n",
    "for dataset_name in dataset_names:\n",
    "    try:\n",
    "        with open(f\"../data/prepared/{dataset_name}/{data_path}/data_dict.pickle\", 'rb') as handle:\n",
    "            data_dict = pickle.load(handle)        \n",
    "    except:\n",
    "        print(f\"dataset {dataset_name} not found\") \n",
    "    for num in range(folds):\n",
    "#         print(num)\n",
    "        n_classes=1\n",
    "        y_test = tf.one_hot(data_dict[f\"y_test_{num}\"],n_classes)\n",
    "        for model in models:\n",
    "            try:\n",
    "                y_pred = np.array(results[dataset_name][num][\"predictions\"][model][2]).ravel()\n",
    "\n",
    "                results_perf[dataset_name][num][model] = get_metrics(y_test,y_pred,target)\n",
    "                results_perf[dataset_name][num][model][\"Time\"] = results[dataset_name][num][\"times\"][model]\n",
    "            except:\n",
    "                print(f\"Set nan for {dataset_name}, {num}\")\n",
    "                results_perf[dataset_name][num][model] = {\"Accuracy\": np.nan,\n",
    "                                                          \"AUROC\": np.nan,\n",
    "                                                          \"F1\": np.nan,\n",
    "                                                          \"Time\": np.nan}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a874334b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GMENN</th>\n",
       "      <th>TE</th>\n",
       "      <th>OHE</th>\n",
       "      <th>Embedding</th>\n",
       "      <th>Ignore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>churn</th>\n",
       "      <td>0.88 (0.018)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kdd_internet_usage</th>\n",
       "      <td>0.93 (0.005)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amazon_employee_access</th>\n",
       "      <td>0.77 (0.02)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Click_prediction_small</th>\n",
       "      <td>0.61 (0.017)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adult</th>\n",
       "      <td>0.91 (0.003)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KDDCup09_upselling</th>\n",
       "      <td>0.75 (0.009)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kick</th>\n",
       "      <td>0.73 (0.008)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>open_payments</th>\n",
       "      <td>0.89 (0.03)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>road-safety-drivers-sex</th>\n",
       "      <td>0.7 (0.019)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>porto-seguro</th>\n",
       "      <td>0.56 (0.006)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                GMENN         TE        OHE  Embedding  \\\n",
       "churn                    0.88 (0.018)  nan (nan)  nan (nan)  nan (nan)   \n",
       "kdd_internet_usage       0.93 (0.005)  nan (nan)  nan (nan)  nan (nan)   \n",
       "Amazon_employee_access    0.77 (0.02)  nan (nan)  nan (nan)  nan (nan)   \n",
       "Click_prediction_small   0.61 (0.017)  nan (nan)  nan (nan)  nan (nan)   \n",
       "adult                    0.91 (0.003)  nan (nan)  nan (nan)  nan (nan)   \n",
       "KDDCup09_upselling       0.75 (0.009)  nan (nan)  nan (nan)  nan (nan)   \n",
       "kick                     0.73 (0.008)  nan (nan)  nan (nan)  nan (nan)   \n",
       "open_payments             0.89 (0.03)  nan (nan)  nan (nan)  nan (nan)   \n",
       "road-safety-drivers-sex   0.7 (0.019)  nan (nan)  nan (nan)  nan (nan)   \n",
       "porto-seguro             0.56 (0.006)  nan (nan)  nan (nan)  nan (nan)   \n",
       "\n",
       "                            Ignore  \n",
       "churn                    nan (nan)  \n",
       "kdd_internet_usage       nan (nan)  \n",
       "Amazon_employee_access   nan (nan)  \n",
       "Click_prediction_small   nan (nan)  \n",
       "adult                    nan (nan)  \n",
       "KDDCup09_upselling       nan (nan)  \n",
       "kick                     nan (nan)  \n",
       "open_payments            nan (nan)  \n",
       "road-safety-drivers-sex  nan (nan)  \n",
       "porto-seguro             nan (nan)  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [\"GMENN\", \"TE\", \"OHE\", \"Embedding\", \"Ignore\"]\n",
    "metric = \"AUROC\"\n",
    "\n",
    "#####\n",
    "dataset_res_dict = {}\n",
    "best_models = {}\n",
    "t_test_results = {}\n",
    "\n",
    "round_mean_at = 2\n",
    "round_std_at = 3\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    dataset_models = list(results_perf[dataset_name][0].keys())\n",
    "    use_df = pd.DataFrame([pd.DataFrame(results_perf[dataset_name][fold_num]).loc[metric,models] for fold_num in results_perf[dataset_name].keys()],index=results_perf[dataset_name].keys())\n",
    "    \n",
    "    df_mean = pd.DataFrame(use_df.mean(axis=0).round(round_mean_at).astype(str) + \" (\" + use_df.std(axis=0).round(round_std_at).astype(str) + \")\").transpose()\n",
    "    model_dict = {i: df_mean[i].values[0] for i in df_mean.columns}\n",
    "    dataset_res_dict[dataset_name] = model_dict\n",
    "    \n",
    "    best_models[dataset_name] = use_df.columns[use_df.mean(axis=0).argmax()]\n",
    "\n",
    "    t_test_res = np.array([stats.ttest_rel(use_df[best_models[dataset_name]].values, use_df[model].values)[1] if model in dataset_models else 0 for model in models]).round(3)\n",
    "    t_test_res[np.isnan(t_test_res)] = 1.\n",
    "    t_test_results[dataset_name] = t_test_res\n",
    "    \n",
    "res_df = pd.DataFrame(dataset_res_dict).transpose()\n",
    "    \n",
    "def negative_bold(val):\n",
    "    i = np.where(val.name==np.array(models))[0][0]\n",
    "    return [\"font-weight: bold\"  if t_test_results[dataset_name][i]>=0.05 else \"\" for dataset_name in val.keys()]\n",
    "    # Case without transpose:\n",
    "#     return [\"font-weight: bold\"  if t_test_results[val.name][i]>=0.05 else \"\" for i in range(len(val))]\n",
    "\n",
    "# res_df.style.apply(negative_bold)\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfa98ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.88, 0.93, 0.77, 0.61, 0.91, 0.75, 0.73, 0.89, 0.7 , 0.56])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df[\"GMENN\"].apply(lambda x: float(x.split(\" \")[0])).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328d4e2d",
   "metadata": {},
   "source": [
    "### Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eea2f7b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GMENN</th>\n",
       "      <th>TE</th>\n",
       "      <th>OHE</th>\n",
       "      <th>Embedding</th>\n",
       "      <th>Ignore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>churn</th>\n",
       "      <td>2.23 (0.289)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kdd_internet_usage</th>\n",
       "      <td>2.16 (0.651)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amazon_employee_access</th>\n",
       "      <td>4.3 (1.386)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Click_prediction_small</th>\n",
       "      <td>1.65 (0.315)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adult</th>\n",
       "      <td>0.97 (0.09)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KDDCup09_upselling</th>\n",
       "      <td>2.52 (0.402)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kick</th>\n",
       "      <td>1.89 (0.814)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>open_payments</th>\n",
       "      <td>8.75 (4.128)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>road-safety-drivers-sex</th>\n",
       "      <td>12.71 (4.436)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>porto-seguro</th>\n",
       "      <td>5.82 (0.277)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "      <td>nan (nan)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 GMENN         TE        OHE  Embedding  \\\n",
       "churn                     2.23 (0.289)  nan (nan)  nan (nan)  nan (nan)   \n",
       "kdd_internet_usage        2.16 (0.651)  nan (nan)  nan (nan)  nan (nan)   \n",
       "Amazon_employee_access     4.3 (1.386)  nan (nan)  nan (nan)  nan (nan)   \n",
       "Click_prediction_small    1.65 (0.315)  nan (nan)  nan (nan)  nan (nan)   \n",
       "adult                      0.97 (0.09)  nan (nan)  nan (nan)  nan (nan)   \n",
       "KDDCup09_upselling        2.52 (0.402)  nan (nan)  nan (nan)  nan (nan)   \n",
       "kick                      1.89 (0.814)  nan (nan)  nan (nan)  nan (nan)   \n",
       "open_payments             8.75 (4.128)  nan (nan)  nan (nan)  nan (nan)   \n",
       "road-safety-drivers-sex  12.71 (4.436)  nan (nan)  nan (nan)  nan (nan)   \n",
       "porto-seguro              5.82 (0.277)  nan (nan)  nan (nan)  nan (nan)   \n",
       "\n",
       "                            Ignore  \n",
       "churn                    nan (nan)  \n",
       "kdd_internet_usage       nan (nan)  \n",
       "Amazon_employee_access   nan (nan)  \n",
       "Click_prediction_small   nan (nan)  \n",
       "adult                    nan (nan)  \n",
       "KDDCup09_upselling       nan (nan)  \n",
       "kick                     nan (nan)  \n",
       "open_payments            nan (nan)  \n",
       "road-safety-drivers-sex  nan (nan)  \n",
       "porto-seguro             nan (nan)  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [\"GMENN\", \"TE\", \"OHE\", \"Embedding\", \"Ignore\"]\n",
    "metric = \"Time\"\n",
    "\n",
    "#####\n",
    "dataset_res_dict = {}\n",
    "best_models = {}\n",
    "t_test_results = {}\n",
    "\n",
    "round_mean_at = 2\n",
    "round_std_at = 3\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    dataset_models = list(results_perf[dataset_name][0].keys())\n",
    "    use_df = pd.DataFrame([pd.DataFrame(results_perf[dataset_name][fold_num]).loc[metric,models] for fold_num in results_perf[dataset_name].keys()],index=results_perf[dataset_name].keys())/60\n",
    "    \n",
    "    df_mean = pd.DataFrame(use_df.mean(axis=0).round(round_mean_at).astype(str) + \" (\" + use_df.std(axis=0).round(round_std_at).astype(str) + \")\").transpose()\n",
    "    model_dict = {i: df_mean[i].values[0] for i in df_mean.columns}\n",
    "    dataset_res_dict[dataset_name] = model_dict\n",
    "    \n",
    "    best_models[dataset_name] = use_df.columns[use_df.mean(axis=0).argmin()]\n",
    "\n",
    "    t_test_res = np.array([stats.ttest_rel(use_df[best_models[dataset_name]].values, use_df[model].values)[1] if model in dataset_models else 0 for model in models]).round(3)\n",
    "    t_test_res[np.isnan(t_test_res)] = 1.\n",
    "    t_test_results[dataset_name] = t_test_res\n",
    "    \n",
    "res_df = pd.DataFrame(dataset_res_dict).transpose()\n",
    "    \n",
    "def negative_bold(val):\n",
    "    i = np.where(val.name==np.array(models))[0][0]\n",
    "    return [\"font-weight: bold\"  if t_test_results[dataset_name][i]>=0.05 else \"\" for dataset_name in val.keys()]\n",
    "    # Case without transpose:\n",
    "#     return [\"font-weight: bold\"  if t_test_results[val.name][i]>=0.05 else \"\" for i in range(len(val))]\n",
    "\n",
    "# res_df.style.apply(negative_bold)\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fb51381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.23,  2.16,  4.3 ,  1.65,  0.97,  2.52,  1.89,  8.75, 12.71,\n",
       "        5.82])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df[\"GMENN\"].apply(lambda x: float(x.split(\" \")[0])).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28914dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cf47f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gmenn",
   "language": "python",
   "name": "gmenn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc07112e7ae1e8e28a0232207620ff002934c05692de8df42430404c766a0a8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
